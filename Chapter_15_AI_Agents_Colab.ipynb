{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chapter 15: Building AI Agents for Network Operations\n\nThis notebook demonstrates AI agent patterns applied to **network engineering tasks**.\nWe'll build working examples from simple tool-calling loops to multi-step network\ntroubleshooting workflows.\n\n### What is an AI Agent?\n\nAn AI agent is a program where an LLM (like Claude) acts as the \"brain\" in a loop:\n1. **Perceive**: Read input (alerts, device output, user questions)\n2. **Reason**: Decide what to do next\n3. **Act**: Call tools (query devices, run commands, check logs)\n4. **Repeat**: Keep going until the task is done\n\n**Networking analogy**: Think of an agent like an event-driven automation system\n(StackStorm, Ansible AWX) -- but instead of hardcoded playbooks, an LLM decides\nwhich \"playbook\" to run based on context."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 1: Setup and Imports\n\nInstall packages and configure the API client. You'll need an Anthropic API key\nfrom [console.anthropic.com](https://console.anthropic.com/)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install anthropic pydantic networkx -q\n\nimport os\nimport json\nimport time\nimport asyncio\nfrom datetime import datetime\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, List, Optional, Any\nfrom enum import Enum\nfrom collections import Counter\nfrom getpass import getpass\n\nimport anthropic\n\n# Set up API key\nif 'ANTHROPIC_API_KEY' not in os.environ:\n    os.environ['ANTHROPIC_API_KEY'] = getpass('Enter your Anthropic API key: ')\n\nclient = anthropic.Anthropic()\nMODEL = \"claude-sonnet-4-20250514\"\n\nprint(f\"Using model: {MODEL}\")\nprint(\"Setup complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 2: Simple Network Agent with Tool Calling\n\nThe most basic agent pattern: the LLM receives a question, decides which\ntool(s) to call, processes the results, and responds.\n\nThis is **the core pattern** behind all AI agents. Everything else in this\nnotebook builds on this loop:\n\n```\nUser asks question\n  → LLM decides which tool to call\n    → Tool returns result\n      → LLM reads result and decides: call another tool, or respond?\n        → Repeat until done\n```\n\n**Networking parallel**: This is like a recursive DNS resolver -- it keeps\nquerying until it has the answer, then responds to the original client."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class NetworkAgent:\n    \"\"\"Basic network operations agent that uses a tool-calling loop.\n\n    This agent has access to simulated network tools:\n    - get_interface_status: like 'show interface' on a router\n    - get_bgp_neighbors: like 'show bgp summary'\n    - ping_device: like running a ping test\n\n    In production, these tools would SSH into real devices via Netmiko/NAPALM.\n    Here we simulate the responses for demonstration.\n    \"\"\"\n\n    def __init__(self):\n        self.conversation = []\n        self.tools = self._define_tools()\n\n    def _define_tools(self):\n        \"\"\"Define the tools available to the agent.\n\n        Each tool has a name, description (Claude reads this to decide\n        when to use it), and an input schema (what parameters it needs).\n        \"\"\"\n        return [\n            {\n                \"name\": \"get_interface_status\",\n                \"description\": \"Get the status of a network interface on a device. \"\n                               \"Returns interface state, IP, speed, errors, and traffic counters.\",\n                \"input_schema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"device\": {\"type\": \"string\", \"description\": \"Device hostname (e.g., router-core-01)\"},\n                        \"interface\": {\"type\": \"string\", \"description\": \"Interface name (e.g., GigabitEthernet0/1)\"}\n                    },\n                    \"required\": [\"device\", \"interface\"]\n                }\n            },\n            {\n                \"name\": \"get_bgp_neighbors\",\n                \"description\": \"Get BGP neighbor summary for a device. \"\n                               \"Returns neighbor IPs, AS numbers, state, and prefixes received.\",\n                \"input_schema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"device\": {\"type\": \"string\", \"description\": \"Device hostname\"}\n                    },\n                    \"required\": [\"device\"]\n                }\n            },\n            {\n                \"name\": \"ping_device\",\n                \"description\": \"Ping a target IP from a source device. Returns success rate and latency.\",\n                \"input_schema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"source_device\": {\"type\": \"string\", \"description\": \"Device to ping from\"},\n                        \"target_ip\": {\"type\": \"string\", \"description\": \"IP address to ping\"}\n                    },\n                    \"required\": [\"source_device\", \"target_ip\"]\n                }\n            }\n        ]\n\n    def _execute_tool(self, tool_name, tool_input):\n        \"\"\"Execute a tool and return the result.\n\n        In production, these would call Netmiko/NAPALM to query real devices.\n        Here we return simulated but realistic output.\n        \"\"\"\n        if tool_name == \"get_interface_status\":\n            device = tool_input[\"device\"]\n            interface = tool_input[\"interface\"]\n            # Simulated interface data\n            interfaces = {\n                (\"router-core-01\", \"GigabitEthernet0/1\"): {\n                    \"status\": \"up/up\", \"ip\": \"10.0.1.1/24\",\n                    \"speed\": \"1Gbps\", \"input_errors\": 0, \"output_errors\": 0,\n                    \"input_rate\": \"450 Mbps\", \"output_rate\": \"380 Mbps\"\n                },\n                (\"router-core-01\", \"GigabitEthernet0/2\"): {\n                    \"status\": \"down/down\", \"ip\": \"10.0.2.1/24\",\n                    \"speed\": \"1Gbps\", \"input_errors\": 1523, \"output_errors\": 47,\n                    \"input_rate\": \"0 bps\", \"output_rate\": \"0 bps\"\n                },\n            }\n            key = (device, interface)\n            if key in interfaces:\n                return json.dumps(interfaces[key])\n            return json.dumps({\"status\": \"up/up\", \"ip\": \"N/A\", \"speed\": \"1Gbps\",\n                             \"input_errors\": 0, \"output_errors\": 0,\n                             \"input_rate\": \"100 Mbps\", \"output_rate\": \"50 Mbps\"})\n\n        elif tool_name == \"get_bgp_neighbors\":\n            return json.dumps({\n                \"device\": tool_input[\"device\"],\n                \"neighbors\": [\n                    {\"neighbor_ip\": \"203.0.113.2\", \"remote_as\": 65002,\n                     \"state\": \"Established\", \"prefixes_received\": 542000,\n                     \"uptime\": \"45d 12:30:00\", \"description\": \"ISP-A Lumen\"},\n                    {\"neighbor_ip\": \"198.51.100.1\", \"remote_as\": 65003,\n                     \"state\": \"Idle\", \"prefixes_received\": 0,\n                     \"uptime\": \"0:00:00\", \"description\": \"ISP-B Cogent\"},\n                ]\n            })\n\n        elif tool_name == \"ping_device\":\n            target = tool_input[\"target_ip\"]\n            if target == \"198.51.100.1\":\n                return json.dumps({\"target\": target, \"packets_sent\": 5,\n                                 \"packets_received\": 0, \"loss\": \"100%\",\n                                 \"min_rtt\": None, \"avg_rtt\": None, \"max_rtt\": None})\n            return json.dumps({\"target\": target, \"packets_sent\": 5,\n                             \"packets_received\": 5, \"loss\": \"0%\",\n                             \"min_rtt\": \"1.2ms\", \"avg_rtt\": \"2.1ms\", \"max_rtt\": \"3.5ms\"})\n\n        return json.dumps({\"error\": f\"Unknown tool: {tool_name}\"})\n\n    def run(self, user_input, max_iterations=10):\n        \"\"\"Run the agent loop.\n\n        The agent keeps calling tools until it has enough information\n        to answer the user's question, or hits the iteration limit.\n        \"\"\"\n        print(f\"User: {user_input}\\n\")\n\n        self.conversation = [{\"role\": \"user\", \"content\": user_input}]\n\n        for iteration in range(max_iterations):\n            # Ask Claude what to do next\n            response = client.messages.create(\n                model=MODEL,\n                max_tokens=1024,\n                system=\"You are a network operations assistant. Use the available tools \"\n                       \"to gather information from network devices and answer questions. \"\n                       \"Always check multiple data sources when troubleshooting.\",\n                tools=self.tools,\n                messages=self.conversation\n            )\n\n            # Add Claude's response to conversation history\n            self.conversation.append({\"role\": \"assistant\", \"content\": response.content})\n\n            # If Claude is done (no more tool calls), return the answer\n            if response.stop_reason == \"end_turn\":\n                for block in response.content:\n                    if hasattr(block, 'text'):\n                        print(f\"Agent: {block.text}\")\n                        return block.text\n\n            # If Claude wants to call tools, execute them\n            if response.stop_reason == \"tool_use\":\n                tool_results = []\n                for block in response.content:\n                    if block.type == \"tool_use\":\n                        print(f\"  [Tool Call] {block.name}({json.dumps(block.input)})\")\n                        result = self._execute_tool(block.name, block.input)\n                        print(f\"  [Result] {result[:120]}...\")\n                        tool_results.append({\n                            \"type\": \"tool_result\",\n                            \"tool_use_id\": block.id,\n                            \"content\": result\n                        })\n\n                # Feed tool results back to Claude\n                self.conversation.append({\"role\": \"user\", \"content\": tool_results})\n\n        return \"Max iterations reached\"\n\n\n# ----- Test the network agent -----\nagent = NetworkAgent()\nprint(\"=\" * 60)\nprint(\"DEMO: Network troubleshooting agent\")\nprint(\"=\" * 60)\nresponse = agent.run(\n    \"The ISP-B link on router-core-01 seems to be down. \"\n    \"Can you check the BGP neighbor status, the interface GigabitEthernet0/2, \"\n    \"and try to ping the ISP-B peer at 198.51.100.1?\"\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 3: Error Handling for Network Agents\n\nNetwork operations are inherently unreliable -- SSH sessions time out,\ndevices are unreachable, APIs rate-limit. Robust error handling is critical.\n\n**Networking parallel**: This is like implementing BFD (Bidirectional Forwarding\nDetection) for your agent -- classify failures quickly and react appropriately."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ErrorType(Enum):\n    \"\"\"Classify errors to determine the right recovery strategy.\n\n    Like how a routing protocol classifies link failures:\n    - Transient (flap) -> wait and retry\n    - Permanent (cable cut) -> reroute immediately\n    - Resource (table full) -> wait for resources to free up\n    \"\"\"\n    TRANSIENT = \"transient\"      # SSH timeout, API 503 -> retry\n    PERMANENT = \"permanent\"      # Wrong credentials, device decommissioned -> don't retry\n    RESOURCE = \"resource\"        # API rate limit, memory full -> wait longer, then retry\n    DEVICE = \"device\"            # Device unreachable, not in inventory -> skip\n    AUTH = \"auth\"                # Authentication failure -> alert operator\n\ndef classify_error(exception):\n    \"\"\"Classify an error to decide how to handle it.\"\"\"\n    error_msg = str(exception).lower()\n\n    if isinstance(exception, TimeoutError) or \"timeout\" in error_msg:\n        return ErrorType.TRANSIENT\n    elif isinstance(exception, PermissionError) or \"auth\" in error_msg:\n        return ErrorType.AUTH\n    elif \"rate limit\" in error_msg or isinstance(exception, MemoryError):\n        return ErrorType.RESOURCE\n    elif \"unreachable\" in error_msg or \"connection refused\" in error_msg:\n        return ErrorType.DEVICE\n    elif isinstance(exception, (KeyError, ValueError)):\n        return ErrorType.PERMANENT\n    else:\n        return ErrorType.TRANSIENT  # Default: assume transient and retry\n\n@dataclass\nclass ErrorContext:\n    \"\"\"Full context about an error -- useful for post-incident review.\"\"\"\n    error_type: ErrorType\n    message: str\n    device: str\n    step: str\n    attempt: int\n    timestamp: datetime\n\nclass ErrorLogger:\n    \"\"\"Track all errors during an agent run for debugging.\"\"\"\n\n    def __init__(self):\n        self.errors = []\n\n    def log(self, context: ErrorContext):\n        self.errors.append(context)\n        print(f\"  [{context.error_type.value.upper()}] {context.device}: {context.message}\")\n\n    def summary(self):\n        by_type = Counter(e.error_type.value for e in self.errors)\n        by_device = Counter(e.device for e in self.errors)\n        return {\"total\": len(self.errors), \"by_type\": dict(by_type), \"by_device\": dict(by_device)}\n\n# ----- Demo: Classify network errors -----\nlogger = ErrorLogger()\n\ntest_errors = [\n    (TimeoutError(\"SSH connection timed out\"), \"router-core-01\", \"get_interface\"),\n    (PermissionError(\"TACACS authentication failed\"), \"switch-dist-02\", \"login\"),\n    (ConnectionError(\"Device unreachable\"), \"switch-access-05\", \"ping\"),\n    (TimeoutError(\"SSH connection timed out\"), \"router-core-01\", \"get_bgp\"),\n    (RuntimeError(\"API rate limit exceeded\"), \"api-server\", \"query\"),\n]\n\nfor exc, device, step in test_errors:\n    ctx = ErrorContext(\n        error_type=classify_error(exc),\n        message=str(exc),\n        device=device,\n        step=step,\n        attempt=1,\n        timestamp=datetime.now()\n    )\n    logger.log(ctx)\n\nprint(\"\\nError Summary:\")\nprint(json.dumps(logger.summary(), indent=2))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 4: Retry with Exponential Backoff\n\nWhen SSH connections time out or APIs rate-limit, you need automatic retries.\nExponential backoff prevents thundering-herd problems (all retries hitting at once).\n\n**Networking parallel**: This is the same concept as Ethernet's binary exponential\nbackoff for collision handling in CSMA/CD -- wait longer after each failure."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class RetryPolicy:\n    \"\"\"Configure retry behavior for network operations.\"\"\"\n\n    def __init__(self, max_attempts=3, backoff_type=\"exponential\",\n                 initial_delay=1, max_delay=60):\n        self.max_attempts = max_attempts\n        self.backoff_type = backoff_type\n        self.initial_delay = initial_delay\n        self.max_delay = max_delay\n\n    def get_delay(self, attempt):\n        \"\"\"Calculate wait time before next retry.\"\"\"\n        if self.backoff_type == \"exponential\":\n            delay = self.initial_delay * (2 ** attempt)\n        elif self.backoff_type == \"linear\":\n            delay = self.initial_delay * (attempt + 1)\n        else:\n            delay = self.initial_delay\n        return min(delay, self.max_delay)\n\n\nclass RetryExecutor:\n    \"\"\"Execute functions with automatic retry on failure.\"\"\"\n\n    def __init__(self, policy=None):\n        self.policy = policy or RetryPolicy()\n\n    def execute(self, fn, args=None, kwargs=None):\n        args = args or ()\n        kwargs = kwargs or {}\n        last_error = None\n\n        for attempt in range(self.policy.max_attempts):\n            try:\n                print(f\"  Attempt {attempt + 1}/{self.policy.max_attempts}...\")\n                return fn(*args, **kwargs)\n            except Exception as e:\n                last_error = e\n                error_type = classify_error(e)\n                print(f\"    Failed: {e}\")\n\n                # Don't retry permanent or auth errors\n                if error_type in (ErrorType.PERMANENT, ErrorType.AUTH):\n                    print(f\"    Error type '{error_type.value}' is not retryable. Giving up.\")\n                    raise\n\n                if attempt < self.policy.max_attempts - 1:\n                    delay = self.policy.get_delay(attempt)\n                    print(f\"    Waiting {delay:.1f}s before retry...\")\n                    time.sleep(delay)\n\n        raise Exception(f\"Failed after {self.policy.max_attempts} attempts\") from last_error\n\n\n# ----- Demo: Simulate a flaky SSH connection -----\nssh_attempt_count = 0\n\ndef flaky_ssh_connection():\n    \"\"\"Simulates an SSH connection that fails twice, then succeeds.\"\"\"\n    global ssh_attempt_count\n    ssh_attempt_count += 1\n    if ssh_attempt_count < 3:\n        raise TimeoutError(\"SSH connection to router-core-01 timed out\")\n    return {\"device\": \"router-core-01\", \"output\": \"show version output here...\"}\n\nprint(\"Simulating flaky SSH with exponential backoff:\\n\")\nexecutor = RetryExecutor(RetryPolicy(max_attempts=4, initial_delay=0.5))\ntry:\n    result = executor.execute(flaky_ssh_connection)\n    print(f\"\\n  Success: Connected to {result['device']}\")\nexcept Exception as e:\n    print(f\"\\n  Failed permanently: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: State Management\n",
    "\n",
    "Implementing agent state tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AgentState:\n",
    "    \"\"\"Explicit state object for agent execution.\"\"\"\n",
    "    task_id: str\n",
    "    user_input: str\n",
    "    status: str  # \"planning\", \"executing\", \"complete\", \"error\"\n",
    "    current_step: int\n",
    "    plan: List[str]\n",
    "    results: Dict[int, str]\n",
    "    errors: List[str]\n",
    "    created_at: datetime\n",
    "    updated_at: datetime\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert to dictionary.\"\"\"\n",
    "        return {\n",
    "            \"task_id\": self.task_id,\n",
    "            \"user_input\": self.user_input,\n",
    "            \"status\": self.status,\n",
    "            \"current_step\": self.current_step,\n",
    "            \"plan\": self.plan,\n",
    "            \"results\": self.results,\n",
    "            \"errors\": self.errors,\n",
    "            \"created_at\": self.created_at.isoformat(),\n",
    "            \"updated_at\": self.updated_at.isoformat()\n",
    "        }\n",
    "    \n",
    "    def to_json(self):\n",
    "        \"\"\"Convert to JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2)\n",
    "\n# Demo: Create and manage state\nimport uuid\n\nstate = AgentState(\n",
    "    task_id=str(uuid.uuid4()),\n",
    "    user_input=\"Write a poem about Python\",\n",
    "    status=\"planning\",\n",
    "    current_step=0,\n",
    "    plan=[\n",
    "        \"Understand requirements\",\n",
    "        \"Generate poem structure\",\n",
    "        \"Write poem\",\n",
    "        \"Review and refine\"\n",
    "    ],\n",
    "    results={},\n",
    "    errors=[],\n",
    "    created_at=datetime.now(),\n",
    "    updated_at=datetime.now()\n",
    ")\n",
    "\nprint(\"Initial State:\")\nprint(state.to_json())\n",
    "\n# Update state as steps execute\nstate.status = \"executing\"\nstate.current_step = 1\nstate.results[0] = \"Requirements understood\"\nstate.updated_at = datetime.now()\n",
    "\nprint(\"\\nUpdated State:\")\nprint(state.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 6: DAG-Based Workflow for Network Operations\n\nComplex network tasks have dependencies. For example, you can't verify BGP\nuntil OSPF is converged, and you can't test end-to-end until both are up.\n\nA DAG (Directed Acyclic Graph) enforces execution order automatically.\n\n**Networking parallel**: This is like a maintenance change window checklist\nwhere steps have dependencies -- except the computer enforces the order."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import networkx as nx\n\nclass DAGWorkflow:\n    \"\"\"Execute network operations in dependency order using a DAG.\"\"\"\n\n    def __init__(self):\n        self.graph = nx.DiGraph()\n        self.results = {}\n\n    def add_step(self, step_id, action, dependencies=None):\n        \"\"\"Add a step. Dependencies must complete before this step runs.\"\"\"\n        self.graph.add_node(step_id, action=action)\n        if dependencies:\n            for dep in dependencies:\n                self.graph.add_edge(dep, step_id)\n\n    def validate(self):\n        \"\"\"Check that the workflow has no circular dependencies.\"\"\"\n        if not nx.is_directed_acyclic_graph(self.graph):\n            raise ValueError(\"Workflow contains circular dependencies!\")\n        return True\n\n    def execute(self):\n        \"\"\"Execute all steps in topological (dependency) order.\"\"\"\n        self.validate()\n        order = list(nx.topological_sort(self.graph))\n        print(f\"Execution order: {' -> '.join(order)}\\n\")\n\n        for step_id in order:\n            action = self.graph.nodes[step_id][\"action\"]\n            deps = list(self.graph.predecessors(step_id))\n            inputs = {dep: self.results[dep] for dep in deps}\n\n            print(f\"  [{step_id}] Running...\")\n            try:\n                result = action(**inputs) if inputs else action()\n                self.results[step_id] = result\n                print(f\"  [{step_id}] Done: {result}\")\n            except Exception as e:\n                print(f\"  [{step_id}] FAILED: {e}\")\n                raise\n        return self.results\n\n\n# ----- Demo: Network change window workflow -----\n# Simulated steps for a BGP peer addition change window\n\ndef backup_config():\n    return \"Config backed up: router-core-01_backup_20260207.cfg\"\n\ndef verify_ospf():\n    return \"OSPF: 12 neighbors, all Full/DR or Full/BDR\"\n\ndef add_bgp_peer(backup_config=None):\n    return \"BGP peer 198.51.100.5 (AS 65004) added\"\n\ndef verify_bgp(add_bgp_peer=None, verify_ospf=None):\n    return \"BGP peer 198.51.100.5 state: Established, 12000 prefixes received\"\n\ndef verify_end_to_end(verify_bgp=None):\n    return \"Traceroute to 8.8.8.8 via new path: 3 hops, 2.1ms avg RTT\"\n\nworkflow = DAGWorkflow()\nworkflow.add_step(\"backup_config\", backup_config)\nworkflow.add_step(\"verify_ospf\", verify_ospf)\nworkflow.add_step(\"add_bgp_peer\", add_bgp_peer, dependencies=[\"backup_config\"])\nworkflow.add_step(\"verify_bgp\", verify_bgp, dependencies=[\"add_bgp_peer\", \"verify_ospf\"])\nworkflow.add_step(\"verify_end_to_end\", verify_end_to_end, dependencies=[\"verify_bgp\"])\n\nprint(\"=\" * 60)\nprint(\"DEMO: Network change window workflow (DAG)\")\nprint(\"=\" * 60)\nresults = workflow.execute()\nprint(f\"\\nAll steps completed successfully.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Tool Executor with Validation\n",
    "\n",
    "Building a robust tool execution framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolExecutor:\n",
    "    \"\"\"Execute tools with validation and error handling.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tools = {}\n",
    "        self.execution_history = []\n",
    "    \n",
    "    def register_tool(self, definition, handler):\n",
    "        \"\"\"Register a tool with definition and handler.\"\"\"\n",
    "        self.tools[definition[\"name\"]] = {\n",
    "            \"definition\": definition,\n",
    "            \"handler\": handler\n",
    "        }\n",
    "    \n",
    "    def execute(self, tool_name, parameters):\n",
    "        \"\"\"Execute a tool with parameter validation.\"\"\"\n",
    "        if tool_name not in self.tools:\n",
    "            return {\"error\": f\"Unknown tool: {tool_name}\"}\n",
    "        \n",
    "        tool_info = self.tools[tool_name]\n",
    "        \n",
    "        # Validate parameters\n",
    "        validation_error = self._validate_parameters(\n",
    "            parameters, \n",
    "            tool_info[\"definition\"]\n",
    "        )\n",
    "        if validation_error:\n",
    "            return {\"error\": f\"Invalid parameters: {validation_error}\"}\n",
    "        \n",
    "        # Execute with error handling\n",
    "        try:\n",
    "            result = tool_info[\"handler\"](**parameters)\n",
    "            execution_record = {\n",
    "                \"tool\": tool_name,\n",
    "                \"parameters\": parameters,\n",
    "                \"result\": result,\n",
    "                \"status\": \"success\",\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            result = {\"error\": str(e)}\n",
    "            execution_record = {\n",
    "                \"tool\": tool_name,\n",
    "                \"parameters\": parameters,\n",
    "                \"result\": result,\n",
    "                \"status\": \"error\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"exception_type\": type(e).__name__\n",
    "            }\n",
    "        \n",
    "        self.execution_history.append(execution_record)\n",
    "        return result\n",
    "    \n",
    "    def _validate_parameters(self, params, tool_definition):\n",
    "        \"\"\"Validate parameters against tool definition.\"\"\"\n",
    "        required_params = tool_definition.get(\"input_schema\", {}).get(\"required\", [])\n",
    "        \n",
    "        for req_param in required_params:\n",
    "            if req_param not in params:\n",
    "                return f\"Missing required parameter: {req_param}\"\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_history(self):\n",
    "        \"\"\"Get execution history.\"\"\"\n",
    "        return self.execution_history\n",
    "\n# Demo: Register and execute tools\nexecutor = ToolExecutor()\n",
    "\n# Register tools\nexecutor.register_tool(\n",
    "    {\n",
    "        \"name\": \"multiply\",\n",
    "        \"description\": \"Multiply two numbers\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"a\": {\"type\": \"number\"},\n",
    "                \"b\": {\"type\": \"number\"}\n",
    "            },\n",
    "            \"required\": [\"a\", \"b\"]\n",
    "        }\n",
    "    },\n",
    "    lambda a, b: a * b\n",
    ")\n",
    "\nexecutor.register_tool(\n",
    "    {\n",
    "        \"name\": \"concatenate\",\n",
    "        \"description\": \"Concatenate strings\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"text1\": {\"type\": \"string\"},\n",
    "                \"text2\": {\"type\": \"string\"}\n",
    "            },\n",
    "            \"required\": [\"text1\", \"text2\"]\n",
    "        }\n",
    "    },\n",
    "    lambda text1, text2: f\"{text1} {text2}\"\n",
    ")\n",
    "\n# Execute tools\nprint(\"Executing multiply(5, 3):\")\nresult1 = executor.execute(\"multiply\", {\"a\": 5, \"b\": 3})\nprint(f\"Result: {result1}\\n\")\n",
    "\nprint(\"Executing concatenate('Hello', 'World'):\")\nresult2 = executor.execute(\"concatenate\", {\"text1\": \"Hello\", \"text2\": \"World\"})\nprint(f\"Result: {result2}\\n\")\n",
    "\nprint(\"Executing multiply with missing parameter:\")\nresult3 = executor.execute(\"multiply\", {\"a\": 5})\nprint(f\"Result: {result3}\\n\")\n",
    "\nprint(\"Execution History:\")\nfor record in executor.get_history():\n",
    "    print(f\"  {record['tool']}: {record['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Decision Making Framework\n",
    "\n",
    "Implementing multi-criteria decision analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiCriteriaDecisionAnalysis:\n",
    "    \"\"\"MCDA framework for evaluating options against multiple criteria.\"\"\"\n",
    "    \n",
    "    def __init__(self, criteria, weights):\n",
    "        \"\"\"\n",
    "        Initialize MCDA.\n",
    "        \n",
    "        Args:\n",
    "            criteria: List of evaluation functions\n",
    "            weights: List of weights for each criterion (must sum to 1)\n",
    "        \"\"\"\n",
    "        if len(criteria) != len(weights):\n",
    "            raise ValueError(\"Number of criteria must match number of weights\")\n",
    "        \n",
    "        if abs(sum(weights) - 1.0) > 0.01:\n",
    "            raise ValueError(\"Weights must sum to 1\")\n",
    "        \n",
    "        self.criteria = criteria\n",
    "        self.weights = weights\n",
    "    \n",
    "    def evaluate_option(self, option):\n",
    "        \"\"\"Evaluate a single option against all criteria.\"\"\"\n",
    "        scores = []\n",
    "        for criterion in self.criteria:\n",
    "            score = criterion(option)\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "    \n",
    "    def decide(self, options):\n",
    "        \"\"\"Choose the best option.\"\"\"\n",
    "        scores = {}\n",
    "        detailed_scores = {}\n",
    "        \n",
    "        for option in options:\n",
    "            criterion_scores = self.evaluate_option(option)\n",
    "            weighted_score = sum(s * w for s, w in zip(criterion_scores, self.weights))\n",
    "            scores[option] = weighted_score\n",
    "            detailed_scores[option] = criterion_scores\n",
    "        \n",
    "        best_option = max(options, key=lambda o: scores[o])\n",
    "        \n",
    "        return {\n",
    "            \"best_option\": best_option,\n",
    "            \"scores\": scores,\n",
    "            \"detailed_scores\": detailed_scores\n",
    "        }\n",
    "\n# Demo: Choose best LLM model\n@dataclass\nclass LLMModel:\n",
    "    name: str\n",
    "    accuracy: float  # 0-1\n",
    "    latency_ms: float\n",
    "    cost_per_1k: float\n",
    "    reliability: float  # 0-1\n",
    "\nmodels = [\n",
    "    LLMModel(\"GPT-4\", 0.95, 2000, 0.03, 0.98),\n",
    "    LLMModel(\"Claude 3.5 Sonnet\", 0.92, 800, 0.003, 0.99),\n",
    "    LLMModel(\"Gemini Pro\", 0.90, 1000, 0.0005, 0.95),\n",
    "]\n",
    "\n# Define criteria and weights\ncriteria = [\n",
    "    lambda m: m.accuracy,              # Accuracy (weight: 0.4)\n",
    "    lambda m: 1 / (m.latency_ms / 1000),  # Speed (weight: 0.3) \n",
    "    lambda m: 1 / m.cost_per_1k,       # Cost efficiency (weight: 0.2)\n",
    "    lambda m: m.reliability             # Reliability (weight: 0.1)\n",
    "]\nweights = [0.4, 0.3, 0.2, 0.1]\n",
    "\nmdca = MultiCriteriaDecisionAnalysis(criteria, weights)\nresult = mdca.decide(models)\n",
    "\nprint(\"Model Selection Results:\")\nprint(f\"Best Choice: {result['best_option'].name}\")\nprint(f\"\\nScores:\")\nfor model, score in sorted(result['scores'].items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {model.name}: {score:.3f}\")\n",
    "\nprint(f\"\\nDetailed Scores (Accuracy, Speed, Cost, Reliability):\")\nfor model, scores in result['detailed_scores'].items():\n",
    "    print(f\"  {model.name}: {[f'{s:.2f}' for s in scores]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 9: Circuit Breaker Pattern\n\nPrevents your agent from hammering a broken service. After too many failures,\nthe circuit \"opens\" and rejects requests immediately (fast-fail). After a\ncooldown period, it tries again.\n\n**Networking parallel**: This is exactly like interface dampening in BGP.\nWhen a BGP peer flaps too many times, the router suppresses it (penalty/suppress\nthreshold). After a decay period, it un-suppresses. Same concept, different domain."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitState(Enum):\n",
    "    \"\"\"Circuit breaker states.\"\"\"\n",
    "    CLOSED = \"closed\"          # Normal operation\n",
    "    OPEN = \"open\"              # Failing, reject requests\n",
    "    HALF_OPEN = \"half_open\"    # Testing recovery\n",
    "\nclass CircuitBreaker:\n",
    "    \"\"\"Prevent cascading failures using circuit breaker pattern.\"\"\"\n",
    "    \n",
    "    def __init__(self, failure_threshold=5, recovery_timeout=10):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.recovery_timeout = recovery_timeout\n",
    "        self.state = CircuitState.CLOSED\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.call_log = []\n",
    "    \n",
    "    def call(self, fn, *args, **kwargs):\n",
    "        \"\"\"Execute function with circuit breaker protection.\"\"\"\n",
    "        if self.state == CircuitState.OPEN:\n",
    "            if time.time() - self.last_failure_time > self.recovery_timeout:\n",
    "                print(f\"  [Circuit] Transitioning to HALF_OPEN\")\n",
    "                self.state = CircuitState.HALF_OPEN\n",
    "            else:\n",
    "                self.call_log.append((\"blocked\", \"circuit open\"))\n",
    "                raise Exception(\"Circuit breaker is OPEN\")\n",
    "        \n",
    "        try:\n",
    "            result = fn(*args, **kwargs)\n",
    "            self.on_success()\n",
    "            self.call_log.append((\"success\", None))\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self.on_failure()\n",
    "            self.call_log.append((\"failure\", str(e)))\n",
    "            raise\n",
    "    \n",
    "    def on_success(self):\n",
    "        \"\"\"Handle successful call.\"\"\"\n",
    "        self.failure_count = 0\n",
    "        if self.state == CircuitState.HALF_OPEN:\n",
    "            print(f\"  [Circuit] Recovered - transitioning to CLOSED\")\n",
    "            self.state = CircuitState.CLOSED\n",
    "    \n",
    "    def on_failure(self):\n",
    "        \"\"\"Handle failed call.\"\"\"\n",
    "        self.failure_count += 1\n",
    "        self.last_failure_time = time.time()\n",
    "        \n",
    "        if self.failure_count >= self.failure_threshold:\n",
    "            print(f\"  [Circuit] Failure threshold reached - opening circuit\")\n",
    "            self.state = CircuitState.OPEN\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.state.value\n",
    "\n# Demo: Simulate service with intermittent failures\nfailing_call_count = 0\n\ndef unreliable_service():\n",
    "    global failing_call_count\n",
    "    failing_call_count += 1\n",
    "    \n",
    "    # Fails first 5 times, then succeeds\n",
    "    if failing_call_count <= 5:\n",
    "        raise Exception(\"Service unavailable\")\n",
    "    return \"Success!\"\n",
    "\nbreaker = CircuitBreaker(failure_threshold=3, recovery_timeout=2)\n",
    "\nprint(\"Testing circuit breaker:\")\nfor i in range(12):\n",
    "    print(f\"\\nCall {i+1}:\")\n",
    "    try:\n",
    "        result = breaker.call(unreliable_service)\n",
    "        print(f\"  Result: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "    print(f\"  Circuit state: {breaker.get_state()}\")\n",
    "    \n",
    "    if i == 6:\n",
    "        print(\"\\n  [Waiting for recovery timeout...]\")\n",
    "        time.sleep(2.5)"
   ]
  },
  {
   "cell_type": {
    "cell_type": "markdown"
   },
   "metadata": {},
   "source": "## Cell 10: Planning Agent for Network Changes\n\nAn agent that creates an explicit plan before executing. Essential for\nnetwork changes where you need to think before you act.\n\n**Networking parallel**: This is like creating a Method of Procedure (MoP)\nbefore a maintenance window. The AI writes the MoP, then executes it step by step."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class PlanningAgent:\n    \"\"\"Agent that creates a plan (MoP) before executing network changes.\"\"\"\n\n    def __init__(self):\n        pass\n\n    def create_plan(self, task):\n        \"\"\"Ask Claude to create a step-by-step plan for a network task.\"\"\"\n        system_prompt = \"\"\"You are a senior network engineer creating a Method of Procedure (MoP)\nfor a network change. Given a task, create a detailed step-by-step plan.\n\nFormat your response as JSON:\n{\n  \"task\": \"description of the change\",\n  \"risk_level\": \"low/medium/high\",\n  \"estimated_impact\": \"description of potential impact\",\n  \"rollback_plan\": \"how to undo the change\",\n  \"steps\": [\n    {\"id\": 1, \"action\": \"specific action\", \"verification\": \"how to verify this step\", \"dependencies\": []},\n    {\"id\": 2, \"action\": \"next action\", \"verification\": \"verification command\", \"dependencies\": [1]}\n  ]\n}\"\"\"\n\n        response = client.messages.create(\n            model=MODEL,\n            max_tokens=1500,\n            system=system_prompt,\n            messages=[{\"role\": \"user\", \"content\": f\"Create a MoP for: {task}\"}]\n        )\n\n        response_text = response.content[0].text\n        try:\n            # Extract JSON from response\n            import re\n            json_match = re.search(r'\\{[\\s\\S]*\\}', response_text)\n            if json_match:\n                return json.loads(json_match.group())\n            return json.loads(response_text)\n        except (json.JSONDecodeError, AttributeError):\n            return {\"raw_plan\": response_text}\n\n    def execute_plan(self, plan):\n        \"\"\"Simulate executing the plan (in production, this would make real changes).\"\"\"\n        results = {}\n        if \"steps\" in plan:\n            for step in plan[\"steps\"]:\n                step_id = step.get(\"id\")\n                action = step.get(\"action\")\n                verification = step.get(\"verification\", \"N/A\")\n                print(f\"  Step {step_id}: {action}\")\n                print(f\"    Verify: {verification}\")\n                results[step_id] = f\"Completed: {action}\"\n        return results\n\n\n# ----- Demo: Plan a network change -----\nagent = PlanningAgent()\nprint(\"=\" * 60)\nprint(\"DEMO: Planning agent for network changes\")\nprint(\"=\" * 60)\nprint(\"\\nCreating MoP for: 'Add a new eBGP peer to ISP-C (AS 65004) on router-core-01'\\n\")\n\nplan = agent.create_plan(\n    \"Add a new eBGP peer to ISP-C (AS 65004, peer IP 192.0.2.1) on router-core-01. \"\n    \"The new peer should receive a default route and our /24 prefix 198.51.100.0/24. \"\n    \"Apply route-maps for inbound and outbound filtering.\"\n)\n\nprint(\"Generated Plan:\")\nprint(json.dumps(plan, indent=2))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Monitoring and Metrics\n",
    "\n",
    "Track agent performance and behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentMetrics:\n",
    "    \"\"\"Track metrics for agent execution.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.task_counts = Counter()\n",
    "        self.task_durations = []\n",
    "        self.tool_calls = Counter()\n",
    "        self.errors = Counter()\n",
    "        self.concurrent_tasks = 0\n",
    "    \n",
    "    def record_task(self, status):\n",
    "        \"\"\"Record task completion.\"\"\"\n",
    "        self.task_counts[status] += 1\n",
    "    \n",
    "    def record_duration(self, duration_seconds):\n",
    "        \"\"\"Record task duration.\"\"\"\n",
    "        self.task_durations.append(duration_seconds)\n",
    "    \n",
    "    def record_tool_call(self, tool_name, status):\n",
    "        \"\"\"Record tool call.\"\"\"\n",
    "        self.tool_calls[f\"{tool_name}:{status}\"] += 1\n",
    "    \n",
    "    def record_error(self, error_type):\n",
    "        \"\"\"Record error.\"\"\"\n",
    "        self.errors[error_type] += 1\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get metrics summary.\"\"\"\n",
    "        avg_duration = sum(self.task_durations) / len(self.task_durations) if self.task_durations else 0\n",
    "        \n",
    "        return {\n",
    "            \"task_counts\": dict(self.task_counts),\n",
    "            \"total_tasks\": sum(self.task_counts.values()),\n",
    "            \"success_rate\": self.task_counts[\"success\"] / sum(self.task_counts.values()) if sum(self.task_counts.values()) > 0 else 0,\n",
    "            \"avg_duration_seconds\": avg_duration,\n",
    "            \"min_duration_seconds\": min(self.task_durations) if self.task_durations else 0,\n",
    "            \"max_duration_seconds\": max(self.task_durations) if self.task_durations else 0,\n",
    "            \"tool_calls\": dict(self.tool_calls),\n",
    "            \"errors\": dict(self.errors),\n",
    "            \"total_errors\": sum(self.errors.values())\n",
    "        }\n",
    "\n# Demo: Simulate agent execution and metrics\nmetrics = AgentMetrics()\n",
    "\n# Simulate some task execution\nfor i in range(10):\n",
    "    if i < 8:\n",
    "        metrics.record_task(\"success\")\n",
    "        metrics.record_duration(2.3 + (i % 3) * 0.5)\n",
    "        metrics.record_tool_call(\"search\", \"success\")\n",
    "        metrics.record_tool_call(\"extract\", \"success\")\n",
    "    else:\n",
    "        metrics.record_task(\"error\")\n",
    "        metrics.record_duration(1.5)\n",
    "        metrics.record_error(\"timeout\")\n",
    "\nprint(\"Agent Metrics Summary:\")\nsummary = metrics.get_summary()\nfor key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Async/Parallel Workflow Execution\n",
    "\n",
    "Execute independent steps in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncWorkflow:\n",
    "    \"\"\"Execute workflows with async tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.steps = {}\n",
    "        self.results = {}\n",
    "    \n",
    "    def add_step(self, step_id, async_action, dependencies=None):\n",
    "        \"\"\"Add an async step.\"\"\"\n",
    "        self.steps[step_id] = {\n",
    "            \"action\": async_action,\n",
    "            \"dependencies\": dependencies or []\n",
    "        }\n",
    "    \n",
    "    async def execute(self):\n",
    "        \"\"\"Execute workflow asynchronously.\"\"\"\n",
    "        completed = set()\n",
    "        \n",
    "        while len(completed) < len(self.steps):\n",
    "            # Find executable steps\n",
    "            executable = [\n",
    "                (step_id, step) for step_id, step in self.steps.items()\n",
    "                if step_id not in completed \n",
    "                and all(dep in completed for dep in step[\"dependencies\"])\n",
    "            ]\n",
    "            \n",
    "            if not executable:\n",
    "                break\n",
    "            \n",
    "            # Execute all executable steps in parallel\n",
    "            tasks = []\n",
    "            task_map = {}\n",
    "            \n",
    "            for step_id, step in executable:\n",
    "                inputs = {dep: self.results[dep] for dep in step[\"dependencies\"]}\n",
    "                task = asyncio.create_task(step[\"action\"](**inputs) if inputs else step[\"action\"]())\n",
    "                tasks.append(task)\n",
    "                task_map[id(task)] = step_id\n",
    "            \n",
    "            # Wait for all tasks\n",
    "            for task in tasks:\n",
    "                result = await task\n",
    "                step_id = task_map[id(task)]\n",
    "                self.results[step_id] = result\n",
    "                completed.add(step_id)\n",
    "        \n",
    "        return self.results\n\n# Demo: Async workflow\nasync def task_a():\n",
    "    await asyncio.sleep(0.5)\n",
    "    return \"Result A\"\n",
    "\nasync def task_b():\n",
    "    await asyncio.sleep(0.3)\n",
    "    return \"Result B\"\n",
    "\nasync def task_c(a=None, b=None):\n",
    "    await asyncio.sleep(0.2)\n",
    "    return f\"Combined: {a} and {b}\"\n",
    "\nasync def run_async_demo():\n",
    "    workflow = AsyncWorkflow()\n",
    "    \n",
    "    # A and B execute in parallel, C depends on both\n",
    "    workflow.add_step(\"a\", task_a)\n",
    "    workflow.add_step(\"b\", task_b)\n",
    "    workflow.add_step(\"c\", task_c, dependencies=[\"a\", \"b\"])\n",
    "    \n",
    "    start = time.time()\n",
    "    results = await workflow.execute()\n",
    "    duration = time.time() - start\n",
    "    \n",
    "    print(f\"Async Workflow Results (completed in {duration:.2f}s):\")\n",
    "    for step_id, result in results.items():\n",
    "        print(f\"  {step_id}: {result}\")\n",
    "\n# Run async demo\nawait run_async_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 13: Human-in-the-Loop for Network Changes\n\nFor network changes, you almost always want human approval before\nthe agent makes modifications. This pattern pauses the agent and waits\nfor an operator to approve or reject the proposed change.\n\n**Networking parallel**: This is like a change approval board (CAB)\nworkflow, but automated. The agent proposes the change, a human\nreviews and approves, then the agent executes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class NetworkChangeApproval:\n    \"\"\"Require human approval before executing network changes.\n\n    Read-only operations (show commands) run automatically.\n    Write operations (config changes) require approval.\n    \"\"\"\n\n    def __init__(self):\n        self.pending = {}\n        self.history = []\n\n    def request_approval(self, change_description, change_details):\n        \"\"\"Request human approval for a network change.\"\"\"\n        change_id = f\"CHG-{len(self.history) + 1:04d}\"\n        self.pending[change_id] = {\n            \"id\": change_id,\n            \"description\": change_description,\n            \"details\": change_details,\n            \"status\": \"pending\",\n            \"requested_at\": datetime.now()\n        }\n\n        print(f\"\\n{'='*50}\")\n        print(f\"  APPROVAL REQUIRED: {change_id}\")\n        print(f\"{'='*50}\")\n        print(f\"  Change: {change_description}\")\n        print(f\"  Details: {json.dumps(change_details, indent=4)}\")\n        print(f\"{'='*50}\")\n        return change_id\n\n    def approve(self, change_id, approver=\"operator\"):\n        if change_id in self.pending:\n            self.pending[change_id][\"status\"] = \"approved\"\n            self.pending[change_id][\"approved_by\"] = approver\n            self.history.append(self.pending.pop(change_id))\n            print(f\"  APPROVED by {approver}: {change_id}\")\n            return True\n        return False\n\n    def reject(self, change_id, reason=\"\"):\n        if change_id in self.pending:\n            self.pending[change_id][\"status\"] = \"rejected\"\n            self.pending[change_id][\"rejection_reason\"] = reason\n            self.history.append(self.pending.pop(change_id))\n            print(f\"  REJECTED: {change_id} -- {reason}\")\n            return True\n        return False\n\n    def execute_with_approval(self, action, description, details, auto_approve=False):\n        \"\"\"Execute a change, requiring approval first.\"\"\"\n        change_id = self.request_approval(description, details)\n\n        if auto_approve:\n            print(\"\\n  (Auto-approving for demo purposes)\")\n            self.approve(change_id, approver=\"auto-demo\")\n            result = action()\n            print(f\"  Executed: {result}\")\n            return {\"status\": \"approved\", \"result\": result, \"change_id\": change_id}\n        else:\n            # In production, this would wait for a webhook/API/Slack callback\n            self.reject(change_id, reason=\"No operator available (demo mode)\")\n            return {\"status\": \"rejected\", \"change_id\": change_id}\n\n\n# ----- Demo: Change approval workflow -----\napprover = NetworkChangeApproval()\n\n# Change 1: Adding an ACL (auto-approved for demo)\nresult1 = approver.execute_with_approval(\n    action=lambda: \"ACL 'BLOCK-SCANNER' applied to Gi0/3 inbound\",\n    description=\"Add security ACL to block known scanner IPs\",\n    details={\n        \"device\": \"router-edge-01\",\n        \"acl_name\": \"BLOCK-SCANNER\",\n        \"interface\": \"GigabitEthernet0/3\",\n        \"direction\": \"inbound\",\n        \"entries\": [\"deny ip 185.220.0.0/16 any\", \"deny ip 45.148.10.0/24 any\"]\n    },\n    auto_approve=True\n)\n\n# Change 2: Shutting down an interface (rejected -- too risky without human review)\nresult2 = approver.execute_with_approval(\n    action=lambda: \"Interface GigabitEthernet0/1 shut down\",\n    description=\"Shut down uplink interface for maintenance\",\n    details={\n        \"device\": \"router-core-01\",\n        \"interface\": \"GigabitEthernet0/1\",\n        \"action\": \"shutdown\",\n        \"impact\": \"Loss of connectivity to datacenter spine\"\n    },\n    auto_approve=False\n)\n\nprint(f\"\\nChange History: {len(approver.history)} changes processed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 14: Putting It All Together -- Network Operations Agent\n\nCombining the patterns above into a comprehensive network operations agent\nwith state tracking, error handling, retry logic, and metrics."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class NetworkOpsAgent:\n    \"\"\"Complete network operations agent with error handling, retry, and metrics.\"\"\"\n\n    def __init__(self):\n        self.metrics = AgentMetrics()\n        self.logger = ErrorLogger()\n        self.retry = RetryExecutor(RetryPolicy(max_attempts=3, initial_delay=0.5))\n\n    def run_task(self, task_description, device=\"unknown\"):\n        \"\"\"Run a network operations task with full monitoring.\"\"\"\n        task_id = f\"TASK-{len(self.logger.errors) + 1:03d}\"\n        start_time = time.time()\n\n        try:\n            print(f\"[{task_id}] Starting: {task_description} on {device}\")\n\n            # Simulate the task (in production, this would call real tools)\n            result = f\"Completed: {task_description}\"\n\n            duration = time.time() - start_time\n            self.metrics.record_task(\"success\")\n            self.metrics.record_duration(duration)\n            self.metrics.record_tool_call(\"ssh\", \"success\")\n\n            print(f\"[{task_id}] Success ({duration:.2f}s)\")\n            return {\"status\": \"success\", \"result\": result}\n\n        except Exception as e:\n            duration = time.time() - start_time\n            self.metrics.record_task(\"error\")\n            self.metrics.record_error(type(e).__name__)\n\n            ctx = ErrorContext(\n                error_type=classify_error(e),\n                message=str(e), device=device,\n                step=task_id, attempt=1,\n                timestamp=datetime.now()\n            )\n            self.logger.log(ctx)\n\n            print(f\"[{task_id}] Error ({duration:.2f}s): {e}\")\n            return {\"status\": \"error\", \"error\": str(e)}\n\n    def get_report(self):\n        return {\n            \"metrics\": self.metrics.get_summary(),\n            \"errors\": self.logger.summary()\n        }\n\n\n# ----- Demo: Run network operations -----\nops_agent = NetworkOpsAgent()\n\ntasks = [\n    (\"Check BGP neighbor status\", \"router-core-01\"),\n    (\"Verify OSPF adjacencies\", \"router-core-01\"),\n    (\"Audit ACL configurations\", \"switch-dist-01\"),\n    (\"Generate interface utilization report\", \"switch-dist-02\"),\n    (\"Check NTP sync status\", \"router-edge-01\"),\n]\n\nfor task, device in tasks:\n    ops_agent.run_task(task, device)\n    time.sleep(0.2)\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Network Operations Agent Report:\")\nprint(\"=\" * 50)\nreport = ops_agent.get_report()\nprint(json.dumps(report, indent=2))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 15: Try It Yourself -- Network Agent Playground\n\nUse this cell to experiment with the network agent. Try different\ntroubleshooting scenarios and see how the agent uses tools to investigate."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# -----------------------------------------------------------------------\n# TRY IT: Ask the network agent a question\n# -----------------------------------------------------------------------\n# Modify the query below and run the cell. The agent has access to:\n# - get_interface_status(device, interface)\n# - get_bgp_neighbors(device)\n# - ping_device(source_device, target_ip)\n#\n# Try queries like:\n# - \"What's the status of all BGP peers on router-core-01?\"\n# - \"Is interface GigabitEthernet0/1 on router-core-01 healthy?\"\n# - \"Can router-core-01 reach 203.0.113.2?\"\n# -----------------------------------------------------------------------\n\nagent = NetworkAgent()\n\nyour_query = \"Check if there are any interface errors on router-core-01 GigabitEthernet0/1 and GigabitEthernet0/2\"\n\nprint(\"=\" * 60)\nprint(\"YOUR QUERY\")\nprint(\"=\" * 60)\nresponse = agent.run(your_query)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrated AI agent patterns applied to network operations:\n\n1. **Network Agent with Tool Calling** -- The core pattern: LLM decides which network tools to call (show commands, pings) and reasons about the results\n2. **Error Classification** -- Categorize failures (SSH timeout vs. auth failure vs. device unreachable) like routing protocols classify link failures\n3. **Retry with Exponential Backoff** -- Same concept as CSMA/CD backoff: wait longer after each failure\n4. **State Management** -- Track what the agent has done, what it knows, and what's left\n5. **DAG Workflows** -- Execute network changes in dependency order (like a MoP checklist)\n6. **Tool Validation** -- Validate parameters before calling network tools\n7. **Decision Making** -- Multi-criteria analysis for choosing between options\n8. **Circuit Breaker** -- Like BGP dampening: suppress a service after too many failures\n9. **Planning Agent** -- Generate a Method of Procedure before making changes\n10. **Monitoring & Metrics** -- Track agent performance (success rates, durations)\n11. **Async Workflows** -- Run independent checks in parallel (ping + traceroute + show commands)\n12. **Human-in-the-Loop** -- Require operator approval for config changes (like a CAB workflow)\n\n### Key Takeaway for Network Engineers\n\nAI agents are **event-driven automation with an LLM as the decision engine**. Instead of writing\nan if/else playbook for every possible scenario, you give the agent tools (SSH, APIs, databases)\nand let the LLM figure out which tools to use and in what order.\n\nThe patterns in this notebook (retry, circuit breaker, DAG workflows, human approval) are the\nsame reliability patterns you already use in networking -- just applied to AI-driven automation."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}