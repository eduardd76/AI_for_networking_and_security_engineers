{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volume 1, Chapter 2: Introduction to LLMs\n",
    "\n",
    "**Understanding Tokens, Context Windows, and Costs -- The Economics of AI for Networking**\n",
    "\n",
    "From: AI for Networking and Security Engineers - Volume 1, Chapter 2\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eduardd76/AI_for_networking_and_security_engineers/blob/master/Volume-1-Foundations/Colab-Notebooks/Vol1_Ch2_Tokenizer.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**What you'll learn:**\n",
    "- Visualize how network configs get tokenized (with surprises)\n",
    "- Calculate exact API costs for any config or log file\n",
    "- Check context window fit across models (the \"MTU\" of AI)\n",
    "- Compare model costs and pick the right one for each task\n",
    "- The cascade pattern: cheap model first, escalate when needed\n",
    "- Optimize prompts to cut costs without losing quality\n",
    "- Project monthly costs for your network at scale\n",
    "\n",
    "**Time:** ~15 minutes | **Cost:** ~$0.05 in API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q anthropic tiktoken\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from getpass import getpass\n",
    "\n",
    "# API key setup\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
    "    print('API key loaded from Colab Secrets')\n",
    "except Exception:\n",
    "    if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "        os.environ['ANTHROPIC_API_KEY'] = getpass('Enter Anthropic API key: ')\n",
    "    print('API key set manually')\n",
    "\n",
    "from anthropic import Anthropic\n",
    "import tiktoken\n",
    "\n",
    "client = Anthropic()\n",
    "MODEL = \"claude-sonnet-4-20250514\"\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")  # Good approximation for all models\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Estimate token count using tiktoken (close to Claude's actual count).\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: How Network Text Gets Tokenized\n",
    "\n",
    "Tokens are the \"packets\" of AI. Text gets split into chunks (tokens)\n",
    "before the model processes it. Common words = 1 token. Technical\n",
    "terms and IP addresses = many tokens.\n",
    "\n",
    "**Networking analogy**: Just like data gets fragmented into packets for\n",
    "transmission, text gets fragmented into tokens for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Visualize tokenization of networking text\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def show_tokens(text):\n",
    "    \"\"\"Visualize how text gets split into tokens.\"\"\"\n",
    "    tokens = encoding.encode(text)\n",
    "    print(f\"Text:   {text}\")\n",
    "    print(f\"Tokens: {len(tokens)}\")\n",
    "    decoded = [encoding.decode([t]) for t in tokens]\n",
    "    print(f\"Split:  {decoded}\")\n",
    "    print()\n",
    "\n",
    "print(\"HOW NETWORKING TEXT TOKENIZES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple words - efficient\n",
    "print(\"--- Common words (efficient) ---\")\n",
    "show_tokens(\"Hello world\")\n",
    "show_tokens(\"BGP\")\n",
    "show_tokens(\"router ospf 1\")\n",
    "\n",
    "# Technical terms - split into pieces\n",
    "print(\"--- Technical terms (less efficient) ---\")\n",
    "show_tokens(\"GigabitEthernet0/0\")\n",
    "show_tokens(\"TenGigabitEthernet1/0/1\")\n",
    "\n",
    "# IP addresses - surprisingly expensive\n",
    "print(\"--- IP addresses (token-expensive!) ---\")\n",
    "show_tokens(\"192.168.1.1\")\n",
    "show_tokens(\"255.255.255.0\")\n",
    "show_tokens(\"2001:db8::1\")\n",
    "\n",
    "# Full commands\n",
    "print(\"--- Full commands ---\")\n",
    "show_tokens(\"interface GigabitEthernet0/0\")\n",
    "show_tokens(\"ip address 192.168.1.1 255.255.255.0\")\n",
    "show_tokens(\"neighbor 203.0.113.2 remote-as 65002\")\n",
    "\n",
    "print(\"KEY INSIGHT: A single 'ip address' line costs ~14 tokens.\")\n",
    "print(\"A router with 100 interfaces = ~1,400 tokens just for IP lines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Token Counts for Real Configs\n",
    "\n",
    "Let's see how different config sizes translate to tokens and cost.\n",
    "This is equivalent to checking the packet size before choosing your MTU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Compare token counts across different config sizes\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Small: access switch\n",
    "small_config = \"\"\"\n",
    "hostname sw-access-01\n",
    "!\n",
    "vlan 10\n",
    " name USERS\n",
    "vlan 20\n",
    " name VOIP\n",
    "!\n",
    "interface GigabitEthernet1/0/1\n",
    " switchport access vlan 10\n",
    " switchport voice vlan 20\n",
    " spanning-tree portfast\n",
    "!\n",
    "interface GigabitEthernet1/0/48\n",
    " switchport mode trunk\n",
    " switchport trunk allowed vlan 10,20\n",
    "\"\"\"\n",
    "\n",
    "# Medium: branch router with OSPF + BGP\n",
    "medium_config = \"\"\"\n",
    "hostname branch-rtr-01\n",
    "!\n",
    "interface GigabitEthernet0/0\n",
    " description WAN_UPLINK_TO_HQ\n",
    " ip address 203.0.113.1 255.255.255.252\n",
    " ip ospf cost 10\n",
    " no shutdown\n",
    "!\n",
    "interface GigabitEthernet0/1\n",
    " description LAN_USERS\n",
    " ip address 192.168.1.1 255.255.255.0\n",
    " no shutdown\n",
    "!\n",
    "interface GigabitEthernet0/2\n",
    " description LAN_SERVERS\n",
    " ip address 172.16.10.1 255.255.255.0\n",
    " no shutdown\n",
    "!\n",
    "router ospf 1\n",
    " router-id 1.1.1.1\n",
    " network 192.168.1.0 0.0.0.255 area 0\n",
    " network 172.16.10.0 0.0.0.255 area 0\n",
    " network 203.0.113.0 0.0.0.3 area 0\n",
    " passive-interface GigabitEthernet0/1\n",
    " passive-interface GigabitEthernet0/2\n",
    "!\n",
    "router bgp 65001\n",
    " bgp router-id 1.1.1.1\n",
    " bgp log-neighbor-changes\n",
    " neighbor 203.0.113.2 remote-as 65002\n",
    " neighbor 203.0.113.2 description HQ_PEER\n",
    " network 192.168.0.0 mask 255.255.0.0\n",
    "!\n",
    "ip access-list extended MGMT_ACCESS\n",
    " permit tcp 10.0.0.0 0.0.255.255 any eq 22\n",
    " deny ip any any log\n",
    "!\n",
    "line vty 0 4\n",
    " access-class MGMT_ACCESS in\n",
    " transport input ssh\n",
    "\"\"\"\n",
    "\n",
    "# Large: simulated core router (~2000 lines)\n",
    "large_config = medium_config * 30  # Approx 2000 lines\n",
    "\n",
    "configs = {\n",
    "    \"Small (access switch)\": small_config,\n",
    "    \"Medium (branch router)\": medium_config,\n",
    "    \"Large (~2000 lines)\": large_config,\n",
    "}\n",
    "\n",
    "print(\"TOKEN COUNT COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Config Type':<25} {'Lines':<8} {'Chars':<10} {'Tokens':<10} {'Chars/Token'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, config in configs.items():\n",
    "    lines = len(config.strip().split('\\n'))\n",
    "    chars = len(config)\n",
    "    tokens = count_tokens(config)\n",
    "    ratio = chars / tokens if tokens > 0 else 0\n",
    "    print(f\"{name:<25} {lines:<8} {chars:<10,} {tokens:<10,} {ratio:.1f}\")\n",
    "\n",
    "print(\"\\nNetwork configs average ~3.5 chars/token (vs ~4 for English prose).\")\n",
    "print(\"That's because configs have many special characters (/, ., :) that\")\n",
    "print(\"each become their own token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3: Cost Calculator -- Know Before You Spend\n",
    "\n",
    "Before running any batch analysis, you should know the cost.\n",
    "This is like checking your bandwidth before starting a large transfer.\n",
    "\n",
    "**Key insight**: Output tokens cost 3-5x more than input tokens.\n",
    "When AI writes a detailed analysis, most of the cost is in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Cost calculator across all major models\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Current pricing (2025-2026)\n",
    "PRICING = {\n",
    "    \"Claude Haiku 4.5\":  {\"input\": 0.80,  \"output\": 4.00,  \"context\": 200_000},\n",
    "    \"Claude Sonnet 4.5\": {\"input\": 3.00,  \"output\": 15.00, \"context\": 200_000},\n",
    "    \"Claude Opus 4\":     {\"input\": 15.00, \"output\": 75.00, \"context\": 200_000},\n",
    "    \"GPT-4o-mini\":       {\"input\": 0.15,  \"output\": 0.60,  \"context\": 128_000},\n",
    "    \"GPT-4o\":            {\"input\": 2.50,  \"output\": 10.00, \"context\": 128_000},\n",
    "    \"Gemini 1.5 Pro\":    {\"input\": 1.25,  \"output\": 5.00,  \"context\": 2_000_000},\n",
    "}\n",
    "\n",
    "def calculate_cost(input_tokens, output_tokens, model):\n",
    "    \"\"\"Calculate cost for a single API call.\"\"\"\n",
    "    p = PRICING[model]\n",
    "    return (input_tokens * p[\"input\"] + output_tokens * p[\"output\"]) / 1_000_000\n",
    "\n",
    "\n",
    "# Scenario: Analyze 100 branch router configs\n",
    "num_configs = 100\n",
    "input_per_config = count_tokens(medium_config)\n",
    "output_estimate = 800  # Typical analysis response\n",
    "\n",
    "print(\"COST CALCULATOR: Analyze 100 Branch Router Configs\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"Input per config:  {input_per_config:,} tokens\")\n",
    "print(f\"Output estimate:   {output_estimate:,} tokens\")\n",
    "print(f\"Total configs:     {num_configs}\")\n",
    "print(f\"Total input:       {input_per_config * num_configs:,} tokens\")\n",
    "print(f\"Total output:      {output_estimate * num_configs:,} tokens\")\n",
    "print()\n",
    "print(f\"{'Model':<22} {'Per Config':<12} {'100 Configs':<12} {'1000/month'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for model in PRICING:\n",
    "    per_config = calculate_cost(input_per_config, output_estimate, model)\n",
    "    batch = per_config * num_configs\n",
    "    monthly = per_config * 1000\n",
    "    print(f\"{model:<22} ${per_config:<11.4f} ${batch:<11.2f} ${monthly:.2f}\")\n",
    "\n",
    "print(\"\\nNotice: Haiku 4.5 is ~4x cheaper than Sonnet 4.5, and GPT-4o-mini\")\n",
    "print(\"is the cheapest. But cheaper models may miss subtle issues.\")\n",
    "print(\"See Demo 6 for the cascade pattern that gives you best of both.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 4: Context Window Check -- The \"MTU\" of AI\n",
    "\n",
    "Context window = the max tokens a model can process in one request.\n",
    "Exceed it and your request fails -- just like exceeding MTU causes\n",
    "fragmentation or drops.\n",
    "\n",
    "Let's check: will your config fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Context window fit check -- will your config fit?\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Simulate configs of different sizes\n",
    "config_sizes = {\n",
    "    \"Small switch (200 lines)\":     medium_config * 3,\n",
    "    \"Branch router (500 lines)\":    medium_config * 8,\n",
    "    \"Core router (2,000 lines)\":    medium_config * 30,\n",
    "    \"Large core (5,000 lines)\":     medium_config * 75,\n",
    "    \"Massive ASR (20,000 lines)\":   medium_config * 300,\n",
    "    \"Full BGP table dump (80,000)\": medium_config * 1200,\n",
    "}\n",
    "\n",
    "OUTPUT_BUFFER = 4000  # Reserve space for the model's response\n",
    "\n",
    "print(\"CONTEXT WINDOW FIT CHECK\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Config':<35} {'Tokens':<10} \", end=\"\")\n",
    "for model in PRICING:\n",
    "    print(f\"{model[:10]:<12}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for name, config in config_sizes.items():\n",
    "    tokens = count_tokens(config)\n",
    "    print(f\"{name:<35} {tokens:<10,}\", end=\"\")\n",
    "    for model, info in PRICING.items():\n",
    "        fits = tokens + OUTPUT_BUFFER < info[\"context\"]\n",
    "        pct = (tokens + OUTPUT_BUFFER) / info[\"context\"] * 100\n",
    "        if fits:\n",
    "            print(f\"{'OK '+str(int(pct))+'%':<12}\", end=\"\")\n",
    "        else:\n",
    "            print(f\"{'NO '+str(int(pct))+'%':<12}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nLike choosing between standard MTU (1500) and jumbo frames (9000):\")\n",
    "print(\"  GPT-4o / Claude = standard MTU (128K-200K tokens)\")\n",
    "print(\"  Gemini 1.5 Pro  = jumbo frames (2M tokens)\")\n",
    "print(\"\\nWhen a config doesn't fit, you need chunking (fragmentation) or\")\n",
    "print(\"a bigger model (jumbo frames). See Chapter 7 for chunking strategies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 5: Exact Token Count via Claude API\n",
    "\n",
    "The `tiktoken` library gives estimates. Claude's API gives exact counts.\n",
    "Let's compare -- and see the actual cost from a real API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Compare tiktoken estimate vs Claude's exact count\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "test_configs = {\n",
    "    \"Interface block\": \"\"\"interface GigabitEthernet0/0\n",
    " description WAN_UPLINK\n",
    " ip address 203.0.113.1 255.255.255.252\n",
    " ip ospf cost 10\n",
    " no shutdown\"\"\",\n",
    "    \"BGP config\": \"\"\"router bgp 65001\n",
    " bgp router-id 10.0.0.1\n",
    " bgp log-neighbor-changes\n",
    " neighbor 203.0.113.2 remote-as 65002\n",
    " neighbor 203.0.113.2 route-map ISP-IN in\n",
    " neighbor 203.0.113.2 route-map ISP-OUT out\n",
    " network 198.51.100.0 mask 255.255.255.0\"\"\",\n",
    "    \"Full medium config\": medium_config,\n",
    "}\n",
    "\n",
    "print(\"TIKTOKEN ESTIMATE vs CLAUDE EXACT COUNT\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Config':<25} {'tiktoken':<12} {'Claude API':<12} {'Diff':<8} {'Error%'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for name, config in test_configs.items():\n",
    "    estimate = count_tokens(config)\n",
    "    # Claude's official count\n",
    "    exact = client.messages.count_tokens(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        messages=[{\"role\": \"user\", \"content\": config}]\n",
    "    )\n",
    "    diff = abs(exact.input_tokens - estimate)\n",
    "    error_pct = (diff / exact.input_tokens * 100) if exact.input_tokens > 0 else 0\n",
    "    print(f\"{name:<25} {estimate:<12} {exact.input_tokens:<12} {diff:<8} {error_pct:.1f}%\")\n",
    "\n",
    "print(\"\\nRule of thumb: tiktoken is within ~5-10% of Claude's actual count.\")\n",
    "print(\"Good enough for budgeting. Use the API count for exact billing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 6: The Cascade Pattern -- Smart Model Selection\n",
    "\n",
    "The 80/20 rule: 80% of tasks can use cheap models, 20% need expensive ones.\n",
    "The **cascade pattern** tries a cheap model first and escalates only when needed.\n",
    "\n",
    "**Networking analogy**: Like QoS classification -- not all traffic needs the\n",
    "priority queue. Route simple queries through best-effort (Haiku),\n",
    "escalate complex ones to the priority queue (Sonnet/Opus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Cascade pattern: cheap model first, escalate if needed\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "test_config = \"\"\"\n",
    "hostname core-rtr-01\n",
    "!\n",
    "snmp-server community public RO\n",
    "snmp-server community private RW\n",
    "!\n",
    "line vty 0 4\n",
    " transport input telnet ssh\n",
    " password cisco123\n",
    " login\n",
    "line vty 5 15\n",
    " no login\n",
    "!\n",
    "interface GigabitEthernet0/0\n",
    " ip address 10.0.1.1 255.255.255.0\n",
    "!\n",
    "router ospf 1\n",
    " network 10.0.0.0 0.0.255.255 area 0\n",
    " network 172.16.0.0 0.0.255.255 area 1\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Analyze this config for security issues. For each issue provide:\n",
    "- severity (critical/high/medium/low)\n",
    "- one-line description\n",
    "- fix command\n",
    "\n",
    "Config:\n",
    "{test_config}\"\"\"\n",
    "\n",
    "# Step 1: Try with Haiku (cheap)\n",
    "print(\"STEP 1: Quick scan with Haiku (cheap model)\")\n",
    "print(\"=\" * 60)\n",
    "start = time.time()\n",
    "haiku_resp = client.messages.create(\n",
    "    model=\"claude-haiku-4-5-20251001\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "haiku_time = time.time() - start\n",
    "haiku_cost = calculate_cost(\n",
    "    haiku_resp.usage.input_tokens,\n",
    "    haiku_resp.usage.output_tokens,\n",
    "    \"Claude Haiku 4.5\"\n",
    ")\n",
    "print(haiku_resp.content[0].text)\n",
    "print(f\"\\n[Haiku: {haiku_time:.1f}s, {haiku_resp.usage.input_tokens}+{haiku_resp.usage.output_tokens} tokens, ${haiku_cost:.4f}]\")\n",
    "\n",
    "# Step 2: Escalate to Sonnet (better reasoning)\n",
    "print(f\"\\n\\nSTEP 2: Deep analysis with Sonnet (escalated)\")\n",
    "print(\"=\" * 60)\n",
    "start = time.time()\n",
    "sonnet_resp = client.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=1500,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "sonnet_time = time.time() - start\n",
    "sonnet_cost = calculate_cost(\n",
    "    sonnet_resp.usage.input_tokens,\n",
    "    sonnet_resp.usage.output_tokens,\n",
    "    \"Claude Sonnet 4.5\"\n",
    ")\n",
    "print(sonnet_resp.content[0].text)\n",
    "print(f\"\\n[Sonnet: {sonnet_time:.1f}s, {sonnet_resp.usage.input_tokens}+{sonnet_resp.usage.output_tokens} tokens, ${sonnet_cost:.4f}]\")\n",
    "\n",
    "# Comparison\n",
    "print(f\"\\n\\nCOMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<20} {'Haiku':<20} {'Sonnet'}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Time':<20} {haiku_time:.1f}s{'':<16} {sonnet_time:.1f}s\")\n",
    "print(f\"{'Cost':<20} ${haiku_cost:.4f}{'':<14} ${sonnet_cost:.4f}\")\n",
    "print(f\"{'Cost ratio':<20} {'1x':<20} {sonnet_cost/haiku_cost:.1f}x\")\n",
    "print(f\"\\nCascade strategy: Use Haiku for the 80% of configs that are\")\n",
    "print(f\"straightforward. Escalate to Sonnet only for complex configs\")\n",
    "print(f\"or when Haiku flags critical issues that need deeper analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 7: Prompt Optimization -- Same Result, Lower Cost\n",
    "\n",
    "Your prompt wording directly affects token count and cost.\n",
    "Like optimizing packet headers -- reduce overhead without losing payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Compare verbose vs efficient prompts\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "verbose_prompt = \"\"\"Hello! I would really appreciate it if you could please help me out by\n",
    "analyzing the following network configuration. I need you to look for\n",
    "any security issues or problems that might be present in the configuration.\n",
    "Please be very thorough in your analysis and make sure to check everything\n",
    "carefully. Thank you so much for your help with this important task!\n",
    "\n",
    "Here is the configuration that I need you to analyze:\n",
    "\"\"\"\n",
    "\n",
    "efficient_prompt = \"\"\"Analyze for security issues. List severity, description, fix:\n",
    "\"\"\"\n",
    "\n",
    "# Even more optimized: use a system prompt for reusable instructions\n",
    "system_approach_system = \"You are a network security auditor. For each issue report: severity, description, fix command.\"\n",
    "system_approach_user = \"Analyze this config:\\n\"\n",
    "\n",
    "prompts = {\n",
    "    \"Verbose (wasteful)\": verbose_prompt,\n",
    "    \"Efficient (concise)\": efficient_prompt,\n",
    "    \"System prompt approach\": system_approach_user,\n",
    "}\n",
    "\n",
    "print(\"PROMPT OPTIMIZATION\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Approach':<25} {'Tokens':<10} {'At 1000 calls/mo*':<18} {'Savings'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "verbose_tokens = count_tokens(verbose_prompt)\n",
    "for name, prompt in prompts.items():\n",
    "    tokens = count_tokens(prompt)\n",
    "    # Cost for prompt tokens only (Sonnet input rate)\n",
    "    monthly_cost = tokens * 1000 * 3.00 / 1_000_000\n",
    "    savings = (1 - tokens / verbose_tokens) * 100 if tokens < verbose_tokens else 0\n",
    "    print(f\"{name:<25} {tokens:<10} ${monthly_cost:<17.2f} {savings:.0f}%\")\n",
    "\n",
    "print(\"\\n* Prompt token cost only, at Sonnet rate ($3/1M input tokens)\")\n",
    "print(\"\\nTips for efficient prompts:\")\n",
    "print(\"  1. Remove filler words ('please', 'thank you', 'I would like')\")\n",
    "print(\"  2. Move reusable instructions to the system prompt\")\n",
    "print(\"  3. Use structured format requests ('list:', 'table:')\")\n",
    "print(\"  4. Be specific about output format to avoid verbose responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 8: Full Network Cost Projection\n",
    "\n",
    "The question your manager will ask: \"How much will this cost per month?\"\n",
    "\n",
    "Let's build a projection for a real network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Monthly cost projection for a real network\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Define your network (adjust these numbers to match yours)\n",
    "network = {\n",
    "    \"Core routers\":       {\"count\": 4,   \"avg_lines\": 5000,  \"analyses_per_month\": 30},\n",
    "    \"Distribution\":       {\"count\": 12,  \"avg_lines\": 2000,  \"analyses_per_month\": 12},\n",
    "    \"Access switches\":    {\"count\": 200, \"avg_lines\": 300,   \"analyses_per_month\": 4},\n",
    "    \"Firewalls\":          {\"count\": 8,   \"avg_lines\": 3000,  \"analyses_per_month\": 8},\n",
    "    \"WAN edge\":           {\"count\": 6,   \"avg_lines\": 1500,  \"analyses_per_month\": 12},\n",
    "}\n",
    "\n",
    "# Estimate tokens: ~3.5 chars per token, ~50 chars per line\n",
    "CHARS_PER_LINE = 50\n",
    "CHARS_PER_TOKEN = 3.5\n",
    "OUTPUT_TOKENS = 1000  # Average analysis response\n",
    "\n",
    "print(\"MONTHLY COST PROJECTION\")\n",
    "print(\"=\" * 85)\n",
    "print(f\"{'Device Type':<20} {'Devices':<9} {'Analyses':<10} {'Input Tokens':<14} \", end=\"\")\n",
    "print(f\"{'Haiku 4.5':<12} {'Sonnet 4.5'}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "total_haiku = 0\n",
    "total_sonnet = 0\n",
    "\n",
    "for device_type, info in network.items():\n",
    "    monthly_analyses = info[\"count\"] * info[\"analyses_per_month\"]\n",
    "    input_tokens = int(info[\"avg_lines\"] * CHARS_PER_LINE / CHARS_PER_TOKEN)\n",
    "    total_input = input_tokens * monthly_analyses\n",
    "    total_output = OUTPUT_TOKENS * monthly_analyses\n",
    "    \n",
    "    haiku_cost = calculate_cost(total_input, total_output, \"Claude Haiku 4.5\")\n",
    "    sonnet_cost = calculate_cost(total_input, total_output, \"Claude Sonnet 4.5\")\n",
    "    \n",
    "    total_haiku += haiku_cost\n",
    "    total_sonnet += sonnet_cost\n",
    "    \n",
    "    print(f\"{device_type:<20} {info['count']:<9} {monthly_analyses:<10} {total_input:<14,} \", end=\"\")\n",
    "    print(f\"${haiku_cost:<11.2f} ${sonnet_cost:.2f}\")\n",
    "\n",
    "print(\"-\" * 85)\n",
    "print(f\"{'TOTAL':<20} {'':<9} {'':<10} {'':<14} ${total_haiku:<11.2f} ${total_sonnet:.2f}\")\n",
    "print(f\"{'ANNUAL':<20} {'':<9} {'':<10} {'':<14} ${total_haiku*12:<11.2f} ${total_sonnet*12:.2f}\")\n",
    "\n",
    "# Cascade savings\n",
    "cascade = total_haiku * 0.8 + total_sonnet * 0.2\n",
    "print(f\"\\nCascade strategy (80% Haiku / 20% Sonnet): ${cascade:.2f}/month (${cascade*12:.2f}/year)\")\n",
    "print(f\"Savings vs all-Sonnet: {(1 - cascade/total_sonnet)*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try It Yourself!\n",
    "\n",
    "Paste your own config below to see token count, cost, and context fit.\n",
    "Remember to sanitize sensitive data first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# YOUR TURN: Paste a config and get the full analysis\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "your_config = \"\"\"\n",
    "! Paste your config here (sanitize secrets first!)\n",
    "hostname your-router\n",
    "interface GigabitEthernet0/0\n",
    " ip address 10.0.0.1 255.255.255.0\n",
    " no shutdown\n",
    "\"\"\"\n",
    "\n",
    "tokens = count_tokens(your_config)\n",
    "lines = len(your_config.strip().split('\\n'))\n",
    "\n",
    "print(\"YOUR CONFIG ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Lines:      {lines}\")\n",
    "print(f\"Characters: {len(your_config):,}\")\n",
    "print(f\"Tokens:     {tokens:,}\")\n",
    "print()\n",
    "\n",
    "print(\"Cost per analysis:\")\n",
    "for model in PRICING:\n",
    "    cost = calculate_cost(tokens, 1000, model)\n",
    "    fits = \"OK\" if tokens + 4000 < PRICING[model][\"context\"] else \"TOO BIG\"\n",
    "    print(f\"  {model:<22} ${cost:.4f}  [{fits}]\")\n",
    "\n",
    "print(f\"\\nMonthly cost (daily analysis, Sonnet): ${calculate_cost(tokens, 1000, 'Claude Sonnet 4.5') * 30:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered the economics of AI for networking:\n",
    "\n",
    "1. **Tokenization** -- How network configs get split into tokens (IP addresses are expensive!)\n",
    "2. **Token Counting** -- Estimate with tiktoken, verify with Claude's API\n",
    "3. **Cost Calculation** -- Know exactly what any analysis will cost before running it\n",
    "4. **Context Windows** -- The \"MTU\" of AI: check fit before sending\n",
    "5. **Exact vs Estimated** -- tiktoken is within ~5-10% of Claude's actual count\n",
    "6. **Cascade Pattern** -- Cheap model first (Haiku), escalate to Sonnet when needed\n",
    "7. **Prompt Optimization** -- Remove filler words, use system prompts, be specific\n",
    "8. **Cost Projection** -- Build the spreadsheet your manager needs\n",
    "\n",
    "### Key Numbers to Remember\n",
    "\n",
    "| Concept | Value |\n",
    "|---------|-------|\n",
    "| 1 token | ~3.5 chars of config (~4 chars of English) |\n",
    "| Output tokens | Cost 3-5x more than input tokens |\n",
    "| 1000-line config | ~15,000 tokens |\n",
    "| Analyzing 1 config (Sonnet) | ~$0.06 |\n",
    "| Analyzing 1 config (Haiku) | ~$0.02 |\n",
    "| Cascade savings | ~60-70% vs all-Sonnet |\n",
    "\n",
    "### Next Steps\n",
    "- **Chapter 3**: Choose the right model for each networking task\n",
    "- **Chapter 4**: API authentication, error handling, production patterns\n",
    "- **Chapter 8**: Deep dive into cost optimization strategies\n",
    "\n",
    "-> [Continue to Chapter 3 Notebook](./Vol1_Ch3_Model_Selection.ipynb)"
   ]
  }
 ]
}
