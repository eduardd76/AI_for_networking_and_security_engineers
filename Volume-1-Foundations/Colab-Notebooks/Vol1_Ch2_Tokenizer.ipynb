{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volume 1, Chapter 2: Introduction to LLMs\n",
    "\n",
    "**Understanding Tokens - The Currency of AI**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eduardd76/AI_for_networking_and_security_engineers/blob/main/Volume-1-Foundations/Colab-Notebooks/Vol1_Ch2_Tokenizer.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**What you'll learn:**\n",
    "- üî¢ What tokens are and why they matter\n",
    "- üí∞ How to calculate API costs\n",
    "- üìä Token counting for network configs\n",
    "- ‚ö° Optimize prompts to reduce costs\n",
    "\n",
    "**Time:** ~10 minutes | **Cost:** ~$0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q anthropic tiktoken\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
    "    print(\"‚úì API key loaded from Colab Secrets\")\n",
    "except:\n",
    "    if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "        os.environ['ANTHROPIC_API_KEY'] = getpass('Enter Anthropic API key: ')\n",
    "    print(\"‚úì API key set\")\n",
    "\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "print(\"‚úì Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¢ Example 1: What Are Tokens?\n",
    "\n",
    "Tokens are pieces of text - roughly 4 characters or 0.75 words on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Use GPT tokenizer (similar to Claude's)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def show_tokens(text):\n",
    "    \"\"\"Visualize how text is tokenized.\"\"\"\n",
    "    tokens = encoding.encode(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Token count: {len(tokens)}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(\"Decoded tokens:\")\n",
    "    for i, token in enumerate(tokens):\n",
    "        decoded = encoding.decode([token])\n",
    "        print(f\"  [{i}] {token} ‚Üí '{decoded}'\")\n",
    "    print()\n",
    "\n",
    "# Simple examples\n",
    "print(\"=\" * 50)\n",
    "print(\"TOKENIZATION EXAMPLES\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "show_tokens(\"BGP\")\n",
    "show_tokens(\"GigabitEthernet0/0\")\n",
    "show_tokens(\"192.168.1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Insight\n",
    "\n",
    "Notice how:\n",
    "- Common words = fewer tokens\n",
    "- Technical terms = more tokens\n",
    "- IP addresses get split at dots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Example 2: Count Tokens in Network Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens in text.\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Sample configs of different sizes\n",
    "small_config = \"\"\"\n",
    "interface GigabitEthernet0/0\n",
    " ip address 192.168.1.1 255.255.255.0\n",
    " no shutdown\n",
    "\"\"\"\n",
    "\n",
    "medium_config = \"\"\"\n",
    "hostname CORE-RTR-01\n",
    "!\n",
    "interface GigabitEthernet0/0\n",
    " description WAN_UPLINK\n",
    " ip address 203.0.113.1 255.255.255.252\n",
    " ip ospf cost 10\n",
    "!\n",
    "interface GigabitEthernet0/1\n",
    " description LAN_SEGMENT\n",
    " ip address 192.168.1.1 255.255.255.0\n",
    "!\n",
    "router ospf 1\n",
    " router-id 1.1.1.1\n",
    " network 192.168.1.0 0.0.0.255 area 0\n",
    " network 203.0.113.0 0.0.0.3 area 0\n",
    "!\n",
    "router bgp 65001\n",
    " neighbor 203.0.113.2 remote-as 65002\n",
    " network 192.168.0.0 mask 255.255.0.0\n",
    "\"\"\"\n",
    "\n",
    "# Large config (simulated)\n",
    "large_config = medium_config * 20\n",
    "\n",
    "print(\"üìä TOKEN COUNT COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Small config:  {count_tokens(small_config):,} tokens ({len(small_config):,} chars)\")\n",
    "print(f\"Medium config: {count_tokens(medium_config):,} tokens ({len(medium_config):,} chars)\")\n",
    "print(f\"Large config:  {count_tokens(large_config):,} tokens ({len(large_config):,} chars)\")\n",
    "print()\n",
    "print(f\"Ratio (tokens/chars): ~{count_tokens(medium_config)/len(medium_config):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üí∞ Example 3: Calculate API Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current pricing (as of 2024)\n",
    "PRICING = {\n",
    "    \"claude-3-5-sonnet\": {\"input\": 3.00, \"output\": 15.00},  # per 1M tokens\n",
    "    \"claude-3-5-haiku\": {\"input\": 0.25, \"output\": 1.25},\n",
    "    \"claude-3-opus\": {\"input\": 15.00, \"output\": 75.00},\n",
    "    \"gpt-4o\": {\"input\": 2.50, \"output\": 10.00},\n",
    "    \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "}\n",
    "\n",
    "def calculate_cost(input_tokens, output_tokens, model=\"claude-3-5-sonnet\"):\n",
    "    \"\"\"Calculate API cost.\"\"\"\n",
    "    pricing = PRICING[model]\n",
    "    input_cost = (input_tokens / 1_000_000) * pricing[\"input\"]\n",
    "    output_cost = (output_tokens / 1_000_000) * pricing[\"output\"]\n",
    "    return input_cost + output_cost\n",
    "\n",
    "# Scenario: Analyze 100 router configs\n",
    "configs_count = 100\n",
    "tokens_per_config = count_tokens(medium_config)\n",
    "output_tokens_estimate = 500  # ~500 tokens for analysis output\n",
    "\n",
    "total_input = tokens_per_config * configs_count\n",
    "total_output = output_tokens_estimate * configs_count\n",
    "\n",
    "print(\"üí∞ COST CALCULATOR\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Scenario: Analyze {configs_count} router configs\")\n",
    "print(f\"Input tokens: {total_input:,}\")\n",
    "print(f\"Output tokens: {total_output:,}\")\n",
    "print()\n",
    "print(\"Cost by model:\")\n",
    "for model in PRICING:\n",
    "    cost = calculate_cost(total_input, total_output, model)\n",
    "    print(f\"  {model:20} ${cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚ö° Example 4: Optimize Prompts to Save Money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbose prompt (wasteful)\n",
    "verbose_prompt = \"\"\"\n",
    "Hello! I would really appreciate it if you could please help me out by\n",
    "analyzing the following network configuration. I need you to look for\n",
    "any security issues or problems that might be present. Please be very\n",
    "thorough in your analysis and make sure to check everything carefully.\n",
    "Thank you so much for your help with this task!\n",
    "\n",
    "Here is the configuration that I need you to analyze:\n",
    "\"\"\"\n",
    "\n",
    "# Efficient prompt (same result, fewer tokens)\n",
    "efficient_prompt = \"\"\"Analyze for security issues:\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚ö° PROMPT OPTIMIZATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Verbose prompt:   {count_tokens(verbose_prompt):,} tokens\")\n",
    "print(f\"Efficient prompt: {count_tokens(efficient_prompt):,} tokens\")\n",
    "print(f\"Savings:          {count_tokens(verbose_prompt) - count_tokens(efficient_prompt):,} tokens ({(1 - count_tokens(efficient_prompt)/count_tokens(verbose_prompt))*100:.0f}%)\")\n",
    "print()\n",
    "print(\"At 1,000 API calls/month:\")\n",
    "verbose_cost = calculate_cost(count_tokens(verbose_prompt) * 1000, 500 * 1000)\n",
    "efficient_cost = calculate_cost(count_tokens(efficient_prompt) * 1000, 500 * 1000)\n",
    "print(f\"  Verbose:   ${verbose_cost:.2f}\")\n",
    "print(f\"  Efficient: ${efficient_cost:.2f}\")\n",
    "print(f\"  Monthly savings: ${verbose_cost - efficient_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ Example 5: Use Claude's Token Counter (Exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude's official token counter\n",
    "test_text = medium_config\n",
    "\n",
    "# Count using Claude's API\n",
    "token_count = client.messages.count_tokens(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    messages=[{\"role\": \"user\", \"content\": test_text}]\n",
    ")\n",
    "\n",
    "print(\"üî¨ EXACT TOKEN COUNT (Claude API)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input tokens: {token_count.input_tokens}\")\n",
    "print(f\"tiktoken estimate: {count_tokens(test_text)}\")\n",
    "print(f\"Difference: {abs(token_count.input_tokens - count_tokens(test_text))} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìè Example 6: Context Window Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context window sizes\n",
    "CONTEXT_LIMITS = {\n",
    "    \"Claude 3.5 Sonnet\": 200_000,\n",
    "    \"Claude 3.5 Haiku\": 200_000,\n",
    "    \"GPT-4o\": 128_000,\n",
    "    \"GPT-4o-mini\": 128_000,\n",
    "    \"Gemini 1.5 Pro\": 2_000_000,\n",
    "}\n",
    "\n",
    "def will_fit(text, model=\"Claude 3.5 Sonnet\", output_buffer=2000):\n",
    "    \"\"\"Check if text fits in model's context window.\"\"\"\n",
    "    tokens = count_tokens(text)\n",
    "    limit = CONTEXT_LIMITS[model]\n",
    "    available = limit - output_buffer\n",
    "    fits = tokens <= available\n",
    "    return {\n",
    "        \"fits\": fits,\n",
    "        \"tokens\": tokens,\n",
    "        \"limit\": limit,\n",
    "        \"available\": available,\n",
    "        \"utilization\": (tokens / available) * 100\n",
    "    }\n",
    "\n",
    "# Test with different config sizes\n",
    "print(\"üìè CONTEXT WINDOW CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate a huge config (10,000 lines)\n",
    "huge_config = medium_config * 500\n",
    "\n",
    "for model in CONTEXT_LIMITS:\n",
    "    result = will_fit(huge_config, model)\n",
    "    status = \"‚úÖ FITS\" if result[\"fits\"] else \"‚ùå TOO BIG\"\n",
    "    print(f\"{model:20} {status} ({result['utilization']:.1f}% of limit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "| Concept | What It Means |\n",
    "|---------|---------------|\n",
    "| **Token** | ~4 chars or 0.75 words |\n",
    "| **Input tokens** | What you send (prompt + context) |\n",
    "| **Output tokens** | What AI returns (costs more!) |\n",
    "| **Context window** | Max total tokens (input + output) |\n",
    "\n",
    "**Cost optimization tips:**\n",
    "1. Remove unnecessary words from prompts\n",
    "2. Use Haiku for simple tasks (12x cheaper than Sonnet)\n",
    "3. Limit output tokens when possible\n",
    "4. Cache repeated prompts\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Next Steps\n",
    "\n",
    "‚û°Ô∏è [Chapter 3: Choosing the Right Model](./Vol1_Ch3_Model_Selection.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
