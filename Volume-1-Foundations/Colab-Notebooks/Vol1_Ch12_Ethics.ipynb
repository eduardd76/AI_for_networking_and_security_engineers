{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volume 1, Chapter 12: Ethics and Responsible AI\n",
    "\n",
    "**Deploy AI Safely in Network Operations**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eduardd76/AI_for_networking_and_security_engineers/blob/master/Volume-1-Foundations/Colab-Notebooks/Vol1_Ch12_Ethics.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**What you'll learn:**\n",
    "- üîí Sanitize sensitive data\n",
    "- üìù Audit AI decisions\n",
    "- ‚úã Human-in-the-loop approvals\n",
    "- ‚öñÔ∏è Detect and mitigate bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q anthropic\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
    "except:\n",
    "    if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "        os.environ['ANTHROPIC_API_KEY'] = getpass('Anthropic API key: ')\n",
    "\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "print(\"‚úì Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîí Example 1: Sanitize Sensitive Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSanitizer:\n",
    "    \"\"\"Remove sensitive info before sending to AI.\"\"\"\n",
    "    \n",
    "    PATTERNS = {\n",
    "        \"password\": r'password\\s+\\S+',\n",
    "        \"secret\": r'secret\\s+\\d+\\s+\\S+',\n",
    "        \"snmp_community\": r'snmp-server community \\S+',\n",
    "        \"api_key\": r'(api[_-]?key|token)[=:\\s]+[\\w-]+',\n",
    "        \"ip_address\": r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b',\n",
    "    }\n",
    "    \n",
    "    def __init__(self, mask_ips=False):\n",
    "        self.mask_ips = mask_ips\n",
    "        self.redactions = []\n",
    "    \n",
    "    def sanitize(self, text):\n",
    "        \"\"\"Remove sensitive data from text.\"\"\"\n",
    "        self.redactions = []\n",
    "        sanitized = text\n",
    "        \n",
    "        for name, pattern in self.PATTERNS.items():\n",
    "            if name == \"ip_address\" and not self.mask_ips:\n",
    "                continue\n",
    "            \n",
    "            matches = re.findall(pattern, sanitized, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                self.redactions.append({\"type\": name, \"value\": match})\n",
    "                sanitized = sanitized.replace(match, f\"[REDACTED_{name.upper()}]\")\n",
    "        \n",
    "        return sanitized\n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"Report what was redacted.\"\"\"\n",
    "        return self.redactions\n",
    "\n",
    "# Test\n",
    "config = \"\"\"\n",
    "hostname ROUTER-01\n",
    "enable secret 5 $1$xyz$hashedvalue\n",
    "username admin password cisco123\n",
    "snmp-server community public RO\n",
    "snmp-server community s3cr3tRW RW\n",
    "interface GigabitEthernet0/0\n",
    " ip address 192.168.1.1 255.255.255.0\n",
    "\"\"\"\n",
    "\n",
    "sanitizer = DataSanitizer()\n",
    "safe_config = sanitizer.sanitize(config)\n",
    "\n",
    "print(\"üîí DATA SANITIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüìã Original config has sensitive data\")\n",
    "print(\"\\nüìã Sanitized config (safe to send to AI):\")\n",
    "print(safe_config)\n",
    "print(f\"\\n‚ö†Ô∏è Redacted {len(sanitizer.report())} items:\")\n",
    "for r in sanitizer.report():\n",
    "    print(f\"   [{r['type']}] {r['value'][:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Example 2: Audit Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuditLogger:\n",
    "    \"\"\"Log all AI decisions for compliance.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "    \n",
    "    def log(self, action, input_data, output, user=\"system\", metadata=None):\n",
    "        \"\"\"Log an AI action.\"\"\"\n",
    "        entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"action\": action,\n",
    "            \"user\": user,\n",
    "            \"input_summary\": input_data[:100] + \"...\" if len(input_data) > 100 else input_data,\n",
    "            \"output_summary\": output[:100] + \"...\" if len(output) > 100 else output,\n",
    "            \"metadata\": metadata or {}\n",
    "        }\n",
    "        self.logs.append(entry)\n",
    "        return entry\n",
    "    \n",
    "    def get_logs(self, action=None, user=None):\n",
    "        \"\"\"Query logs.\"\"\"\n",
    "        results = self.logs\n",
    "        if action:\n",
    "            results = [l for l in results if l[\"action\"] == action]\n",
    "        if user:\n",
    "            results = [l for l in results if l[\"user\"] == user]\n",
    "        return results\n",
    "    \n",
    "    def export(self):\n",
    "        \"\"\"Export logs as JSON.\"\"\"\n",
    "        return json.dumps(self.logs, indent=2)\n",
    "\n",
    "# Use with AI calls\n",
    "audit = AuditLogger()\n",
    "\n",
    "# Make an AI call and log it\n",
    "prompt = \"Analyze this config for security issues: interface Gi0/0\"\n",
    "response = client.messages.create(\n",
    "    model=\"claude-haiku-4-5-20251001\",\n",
    "    max_tokens=200,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "audit.log(\n",
    "    action=\"config_analysis\",\n",
    "    input_data=prompt,\n",
    "    output=response.content[0].text,\n",
    "    user=\"network_admin\",\n",
    "    metadata={\"model\": \"claude-haiku-4-5-20251001\", \"tokens\": response.usage.input_tokens}\n",
    ")\n",
    "\n",
    "print(\"üìù AUDIT LOGGING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nLogged entry:\")\n",
    "print(json.dumps(audit.logs[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úã Example 3: Human-in-the-Loop Approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApprovalGate:\n",
    "    \"\"\"Require human approval for risky actions.\"\"\"\n",
    "    \n",
    "    RISKY_PATTERNS = [\n",
    "        \"shutdown\",\n",
    "        \"no interface\",\n",
    "        \"write erase\",\n",
    "        \"reload\",\n",
    "        \"delete\",\n",
    "        \"format\"\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pending = []\n",
    "    \n",
    "    def assess_risk(self, commands):\n",
    "        \"\"\"Assess risk level of commands.\"\"\"\n",
    "        risks = []\n",
    "        for cmd in commands if isinstance(commands, list) else [commands]:\n",
    "            for pattern in self.RISKY_PATTERNS:\n",
    "                if pattern in cmd.lower():\n",
    "                    risks.append({\"command\": cmd, \"reason\": f\"Contains '{pattern}'\"})\n",
    "        \n",
    "        if risks:\n",
    "            return {\"level\": \"HIGH\", \"risks\": risks, \"approval_required\": True}\n",
    "        return {\"level\": \"LOW\", \"risks\": [], \"approval_required\": False}\n",
    "    \n",
    "    def request_approval(self, action, commands, justification):\n",
    "        \"\"\"Create approval request.\"\"\"\n",
    "        assessment = self.assess_risk(commands)\n",
    "        \n",
    "        if not assessment[\"approval_required\"]:\n",
    "            return {\"status\": \"AUTO_APPROVED\", \"reason\": \"Low risk\"}\n",
    "        \n",
    "        request = {\n",
    "            \"id\": f\"REQ-{len(self.pending)+1:04d}\",\n",
    "            \"action\": action,\n",
    "            \"commands\": commands,\n",
    "            \"justification\": justification,\n",
    "            \"risk_assessment\": assessment,\n",
    "            \"status\": \"PENDING\",\n",
    "            \"created\": datetime.now().isoformat()\n",
    "        }\n",
    "        self.pending.append(request)\n",
    "        return {\"status\": \"PENDING_APPROVAL\", \"request_id\": request[\"id\"]}\n",
    "    \n",
    "    def approve(self, request_id, approver):\n",
    "        \"\"\"Approve a pending request.\"\"\"\n",
    "        for req in self.pending:\n",
    "            if req[\"id\"] == request_id:\n",
    "                req[\"status\"] = \"APPROVED\"\n",
    "                req[\"approver\"] = approver\n",
    "                req[\"approved_at\"] = datetime.now().isoformat()\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Test\n",
    "gate = ApprovalGate()\n",
    "\n",
    "# Safe command - auto-approved\n",
    "result1 = gate.request_approval(\n",
    "    action=\"show_config\",\n",
    "    commands=\"show running-config\",\n",
    "    justification=\"Routine audit\"\n",
    ")\n",
    "\n",
    "# Risky command - needs approval\n",
    "result2 = gate.request_approval(\n",
    "    action=\"interface_shutdown\",\n",
    "    commands=[\"interface Gi0/1\", \"shutdown\"],\n",
    "    justification=\"Decommissioning old link\"\n",
    ")\n",
    "\n",
    "print(\"‚úã HUMAN-IN-THE-LOOP APPROVAL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSafe command: {result1}\")\n",
    "print(f\"\\nRisky command: {result2}\")\n",
    "\n",
    "# Simulate approval\n",
    "if result2.get(\"request_id\"):\n",
    "    gate.approve(result2[\"request_id\"], \"senior_engineer\")\n",
    "    print(f\"\\n‚úÖ Request {result2['request_id']} approved by senior_engineer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚öñÔ∏è Example 4: Bias Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vendor_bias(prompt):\n",
    "    \"\"\"Check if AI response favors specific vendors.\"\"\"\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=500,\n",
    "        temperature=0,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    text = response.content[0].text.lower()\n",
    "    \n",
    "    vendors = {\n",
    "        \"cisco\": len(re.findall(r'\\bcisco\\b', text)),\n",
    "        \"juniper\": len(re.findall(r'\\bjuniper\\b', text)),\n",
    "        \"arista\": len(re.findall(r'\\barista\\b', text)),\n",
    "        \"palo alto\": len(re.findall(r'\\bpalo alto\\b', text)),\n",
    "    }\n",
    "    \n",
    "    total = sum(vendors.values())\n",
    "    if total == 0:\n",
    "        return {\"bias\": \"NONE\", \"vendors\": vendors, \"response\": response.content[0].text[:200]}\n",
    "    \n",
    "    dominant = max(vendors, key=vendors.get)\n",
    "    dominant_pct = vendors[dominant] / total * 100\n",
    "    \n",
    "    return {\n",
    "        \"bias\": \"HIGH\" if dominant_pct > 70 else \"MODERATE\" if dominant_pct > 50 else \"LOW\",\n",
    "        \"dominant_vendor\": dominant,\n",
    "        \"percentage\": dominant_pct,\n",
    "        \"vendors\": vendors,\n",
    "        \"response\": response.content[0].text[:300]\n",
    "    }\n",
    "\n",
    "# Test with potentially biased prompt\n",
    "result = check_vendor_bias(\"What's the best way to configure enterprise routing?\")\n",
    "\n",
    "print(\"‚öñÔ∏è BIAS DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBias level: {result['bias']}\")\n",
    "if 'dominant_vendor' in result:\n",
    "    print(f\"Dominant vendor: {result['dominant_vendor']} ({result['percentage']:.0f}%)\")\n",
    "print(f\"\\nVendor mentions: {result['vendors']}\")\n",
    "print(f\"\\nResponse preview: {result['response']}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üõ°Ô∏è Example 5: Complete Responsible AI System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponsibleAISystem:\n",
    "    \"\"\"Complete responsible AI implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sanitizer = DataSanitizer()\n",
    "        self.audit = AuditLogger()\n",
    "        self.approval = ApprovalGate()\n",
    "    \n",
    "    def analyze(self, config, user, action_type=\"analysis\"):\n",
    "        \"\"\"Analyze config with full safety checks.\"\"\"\n",
    "        \n",
    "        # Step 1: Sanitize\n",
    "        safe_config = self.sanitizer.sanitize(config)\n",
    "        \n",
    "        # Step 2: Call AI\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-haiku-4-5-20251001\",\n",
    "            max_tokens=300,\n",
    "            temperature=0,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Analyze for security issues:\\n{safe_config}\"\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        output = response.content[0].text\n",
    "        \n",
    "        # Step 3: Audit\n",
    "        self.audit.log(\n",
    "            action=action_type,\n",
    "            input_data=safe_config,\n",
    "            output=output,\n",
    "            user=user,\n",
    "            metadata={\"redactions\": len(self.sanitizer.report())}\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"analysis\": output,\n",
    "            \"redacted_items\": len(self.sanitizer.report()),\n",
    "            \"audit_logged\": True\n",
    "        }\n",
    "    \n",
    "    def apply_change(self, commands, user, justification):\n",
    "        \"\"\"Apply change with approval if needed.\"\"\"\n",
    "        \n",
    "        # Step 1: Check if approval needed\n",
    "        result = self.approval.request_approval(\n",
    "            action=\"config_change\",\n",
    "            commands=commands,\n",
    "            justification=justification\n",
    "        )\n",
    "        \n",
    "        # Step 2: Audit\n",
    "        self.audit.log(\n",
    "            action=\"change_request\",\n",
    "            input_data=str(commands),\n",
    "            output=str(result),\n",
    "            user=user\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Demo\n",
    "system = ResponsibleAISystem()\n",
    "\n",
    "config = \"\"\"\n",
    "hostname R1\n",
    "enable password test123\n",
    "interface Gi0/0\n",
    " ip address 10.1.1.1 255.255.255.0\n",
    "\"\"\"\n",
    "\n",
    "print(\"üõ°Ô∏è COMPLETE RESPONSIBLE AI SYSTEM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze\n",
    "result = system.analyze(config, user=\"admin\")\n",
    "print(f\"\\n‚úÖ Analysis complete\")\n",
    "print(f\"   Redacted items: {result['redacted_items']}\")\n",
    "print(f\"   Audit logged: {result['audit_logged']}\")\n",
    "print(f\"\\nüìã Analysis result:\")\n",
    "print(result['analysis'][:300])\n",
    "\n",
    "# Try to apply risky change\n",
    "change_result = system.apply_change(\n",
    "    commands=[\"interface Gi0/0\", \"shutdown\"],\n",
    "    user=\"admin\",\n",
    "    justification=\"Maintenance window\"\n",
    ")\n",
    "print(f\"\\n\\nüîí Change request: {change_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "| Practice | Why It Matters |\n",
    "|----------|----------------|\n",
    "| Sanitize data | Protect credentials from exposure |\n",
    "| Audit logging | Compliance and troubleshooting |\n",
    "| Approval gates | Prevent unintended damage |\n",
    "| Bias detection | Ensure fair recommendations |\n",
    "\n",
    "**The responsible AI checklist:**\n",
    "1. ‚úÖ Never send raw credentials to AI APIs\n",
    "2. ‚úÖ Log all AI decisions with context\n",
    "3. ‚úÖ Require approval for destructive actions\n",
    "4. ‚úÖ Monitor for vendor/solution bias\n",
    "5. ‚úÖ Keep humans in the loop for production changes\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed **Volume 1: Foundations**!\n",
    "\n",
    "You now understand:\n",
    "- How LLMs work (tokens, context, models)\n",
    "- How to call APIs effectively\n",
    "- How to write good prompts\n",
    "- How to get structured outputs\n",
    "- How to manage costs\n",
    "- How to deploy responsibly\n",
    "\n",
    "**Next:** [Volume 2: Practical Applications](../../../Volume-2-Practical-Applications/) - Build RAG systems, agents, and automation!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
