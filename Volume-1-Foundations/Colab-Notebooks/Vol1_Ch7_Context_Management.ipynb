{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volume 1, Chapter 7: Context Management\n",
    "\n",
    "**Handle Large Configs That Exceed Token Limits**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eduardd76/AI_for_networking_and_security_engineers/blob/main/Volume-1-Foundations/Colab-Notebooks/Vol1_Ch7_Context_Management.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**What you'll learn:**\n",
    "- üìè Check if your config fits in context\n",
    "- ‚úÇÔ∏è Chunk large configs intelligently\n",
    "- üîÑ Map-Reduce pattern for analysis\n",
    "- üíæ Cache prompts to save money\n",
    "\n",
    "**Time:** ~15 minutes | **Cost:** ~$0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q anthropic tiktoken\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
    "except:\n",
    "    if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "        os.environ['ANTHROPIC_API_KEY'] = getpass('Anthropic API key: ')\n",
    "\n",
    "from anthropic import Anthropic\n",
    "import tiktoken\n",
    "\n",
    "client = Anthropic()\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "print(\"‚úì Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìè Example 1: Check If Config Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context window limits\n",
    "LIMITS = {\n",
    "    \"claude-3-5-sonnet\": 200_000,\n",
    "    \"claude-3-5-haiku\": 200_000,\n",
    "    \"gpt-4o\": 128_000,\n",
    "}\n",
    "\n",
    "def will_fit(text, model=\"claude-3-5-sonnet\", output_reserve=4000, prompt_overhead=500):\n",
    "    \"\"\"Check if content fits in context window.\"\"\"\n",
    "    tokens = count_tokens(text)\n",
    "    limit = LIMITS[model]\n",
    "    available = limit - output_reserve - prompt_overhead\n",
    "    \n",
    "    return {\n",
    "        \"fits\": tokens <= available,\n",
    "        \"content_tokens\": tokens,\n",
    "        \"available_tokens\": available,\n",
    "        \"utilization\": (tokens / available) * 100,\n",
    "        \"overflow\": max(0, tokens - available)\n",
    "    }\n",
    "\n",
    "# Create sample configs of different sizes\n",
    "small_config = \"\"\"hostname R1\n",
    "interface Gi0/0\n",
    " ip address 10.1.1.1 255.255.255.0\n",
    "\"\"\"\n",
    "\n",
    "# Medium config (~500 lines)\n",
    "medium_config = (small_config + \"!\\n\") * 100\n",
    "\n",
    "# Large config (~5000 lines) \n",
    "large_config = medium_config * 10\n",
    "\n",
    "# Huge config (~50000 lines)\n",
    "huge_config = large_config * 10\n",
    "\n",
    "print(\"üìè CONTEXT WINDOW CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "configs = [\n",
    "    (\"Small (~50 lines)\", small_config),\n",
    "    (\"Medium (~500 lines)\", medium_config),\n",
    "    (\"Large (~5K lines)\", large_config),\n",
    "    (\"Huge (~50K lines)\", huge_config),\n",
    "]\n",
    "\n",
    "for name, config in configs:\n",
    "    result = will_fit(config)\n",
    "    status = \"‚úÖ FITS\" if result[\"fits\"] else f\"‚ùå OVERFLOW ({result['overflow']:,} tokens)\"\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Tokens: {result['content_tokens']:,}\")\n",
    "    print(f\"  Utilization: {result['utilization']:.1f}%\")\n",
    "    print(f\"  Status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÇÔ∏è Example 2: Intelligent Chunking by Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def chunk_by_interface(config):\n",
    "    \"\"\"Split config into interface blocks.\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Split on 'interface' keyword\n",
    "    parts = re.split(r'(^interface \\S+)', config, flags=re.MULTILINE)\n",
    "    \n",
    "    # Combine interface name with its config\n",
    "    current_chunk = \"\"\n",
    "    for part in parts:\n",
    "        if part.startswith('interface '):\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = part\n",
    "        else:\n",
    "            current_chunk += part\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test config\n",
    "test_config = \"\"\"hostname ROUTER-01\n",
    "!\n",
    "interface GigabitEthernet0/0\n",
    " description WAN\n",
    " ip address 203.0.113.1 255.255.255.252\n",
    " no shutdown\n",
    "!\n",
    "interface GigabitEthernet0/1\n",
    " description LAN\n",
    " ip address 192.168.1.1 255.255.255.0\n",
    " no shutdown\n",
    "!\n",
    "interface Loopback0\n",
    " ip address 1.1.1.1 255.255.255.255\n",
    "!\n",
    "router ospf 1\n",
    " network 192.168.1.0 0.0.0.255 area 0\n",
    "\"\"\"\n",
    "\n",
    "chunks = chunk_by_interface(test_config)\n",
    "\n",
    "print(\"‚úÇÔ∏è CHUNKED BY INTERFACE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n--- Chunk {i+1} ({count_tokens(chunk)} tokens) ---\")\n",
    "    print(chunk[:200] + (\"...\" if len(chunk) > 200 else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ Example 3: Map-Reduce Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chunk(chunk, chunk_name):\n",
    "    \"\"\"Analyze a single config chunk (MAP step).\"\"\"\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-haiku-20241022\",\n",
    "        max_tokens=300,\n",
    "        temperature=0,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Analyze this config section for security issues. Be brief.\n",
    "\n",
    "Section: {chunk_name}\n",
    "Config:\n",
    "{chunk}\n",
    "\n",
    "Return: List any issues found, or \"No issues\" if clean.\"\"\"\n",
    "        }]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def combine_results(results):\n",
    "    \"\"\"Combine chunk analyses (REDUCE step).\"\"\"\n",
    "    all_results = \"\\n\\n\".join([f\"**{name}:**\\n{result}\" for name, result in results])\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-haiku-20241022\",\n",
    "        max_tokens=500,\n",
    "        temperature=0,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Summarize these security findings into a final report:\n",
    "\n",
    "{all_results}\n",
    "\n",
    "Provide:\n",
    "1. Critical issues (if any)\n",
    "2. Warnings\n",
    "3. Overall risk level (Low/Medium/High)\"\"\"\n",
    "        }]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "# Full config to analyze\n",
    "full_config = \"\"\"hostname BRANCH-01\n",
    "!\n",
    "enable password cisco123\n",
    "!\n",
    "interface GigabitEthernet0/0\n",
    " description WAN\n",
    " ip address 203.0.113.1 255.255.255.252\n",
    "!\n",
    "interface GigabitEthernet0/1\n",
    " description LAN\n",
    " ip address 192.168.1.1 255.255.255.0\n",
    "!\n",
    "line vty 0 4\n",
    " transport input telnet\n",
    " password cisco\n",
    "!\n",
    "snmp-server community public RO\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ MAP-REDUCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# MAP: Analyze each chunk\n",
    "chunks = chunk_by_interface(full_config)\n",
    "chunk_results = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    name = f\"Section {i+1}\"\n",
    "    if \"interface\" in chunk:\n",
    "        match = re.search(r'interface (\\S+)', chunk)\n",
    "        if match:\n",
    "            name = match.group(1)\n",
    "    elif \"line vty\" in chunk:\n",
    "        name = \"VTY Lines\"\n",
    "    elif \"snmp\" in chunk:\n",
    "        name = \"SNMP Config\"\n",
    "    \n",
    "    print(f\"\\nüìç Analyzing: {name}...\")\n",
    "    result = analyze_chunk(chunk, name)\n",
    "    chunk_results.append((name, result))\n",
    "    print(f\"   Result: {result[:100]}...\")\n",
    "\n",
    "# REDUCE: Combine results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä COMBINED REPORT\")\n",
    "print(\"=\" * 60)\n",
    "final_report = combine_results(chunk_results)\n",
    "print(final_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ Example 4: Chunk by Token Limit with Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_tokens(text, max_tokens=1000, overlap_tokens=100):\n",
    "    \"\"\"Chunk text by token limit with overlap for context continuity.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        line_tokens = count_tokens(line)\n",
    "        \n",
    "        if current_tokens + line_tokens > max_tokens and current_chunk:\n",
    "            # Save current chunk\n",
    "            chunks.append('\\n'.join(current_chunk))\n",
    "            \n",
    "            # Start new chunk with overlap (last few lines)\n",
    "            overlap_lines = []\n",
    "            overlap_count = 0\n",
    "            for prev_line in reversed(current_chunk):\n",
    "                if overlap_count + count_tokens(prev_line) <= overlap_tokens:\n",
    "                    overlap_lines.insert(0, prev_line)\n",
    "                    overlap_count += count_tokens(prev_line)\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            current_chunk = overlap_lines + [line]\n",
    "            current_tokens = overlap_count + line_tokens\n",
    "        else:\n",
    "            current_chunk.append(line)\n",
    "            current_tokens += line_tokens\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test with medium config\n",
    "print(\"üì¶ TOKEN-BASED CHUNKING WITH OVERLAP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "chunks = chunk_by_tokens(medium_config, max_tokens=500, overlap_tokens=50)\n",
    "\n",
    "print(f\"Original: {count_tokens(medium_config):,} tokens\")\n",
    "print(f\"Chunks: {len(chunks)}\")\n",
    "print()\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):  # Show first 3\n",
    "    print(f\"Chunk {i+1}: {count_tokens(chunk)} tokens\")\n",
    "print(f\"... and {len(chunks)-3} more chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Example 5: Smart Chunking Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_analyze(config):\n",
    "    \"\"\"Automatically decide chunking strategy based on size.\"\"\"\n",
    "    tokens = count_tokens(config)\n",
    "    \n",
    "    print(f\"Config size: {tokens:,} tokens\")\n",
    "    \n",
    "    if tokens < 50000:  # Fits in context\n",
    "        print(\"Strategy: DIRECT (fits in context)\")\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=1000,\n",
    "            temperature=0,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Analyze for security issues:\\n{config}\"\n",
    "            }]\n",
    "        )\n",
    "        return response.content[0].text\n",
    "    \n",
    "    elif tokens < 200000:  # Needs chunking\n",
    "        print(\"Strategy: CHUNK + MAP-REDUCE\")\n",
    "        chunks = chunk_by_interface(config)\n",
    "        if len(chunks) < 3:\n",
    "            chunks = chunk_by_tokens(config, max_tokens=40000)\n",
    "        \n",
    "        results = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            response = client.messages.create(\n",
    "                model=\"claude-3-5-haiku-20241022\",\n",
    "                max_tokens=300,\n",
    "                temperature=0,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"List security issues in this config section (brief):\\n{chunk}\"\n",
    "                }]\n",
    "            )\n",
    "            results.append(response.content[0].text)\n",
    "        \n",
    "        return \"\\n\\n\".join(results)\n",
    "    \n",
    "    else:  # Too large\n",
    "        print(\"Strategy: SUMMARIZE FIRST\")\n",
    "        return \"Config too large. Consider splitting by device or section first.\"\n",
    "\n",
    "# Test\n",
    "print(\"üéØ SMART ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "result = smart_analyze(test_config)\n",
    "print(\"\\nResult:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "| Config Size | Strategy | Notes |\n",
    "|-------------|----------|-------|\n",
    "| < 50K tokens | Direct analysis | Single API call |\n",
    "| 50K - 200K | Chunk + Map-Reduce | Split, analyze, combine |\n",
    "| > 200K | Summarize first | Or use Gemini (2M context) |\n",
    "\n",
    "**Chunking best practices:**\n",
    "1. Chunk at natural boundaries (interfaces, sections)\n",
    "2. Use overlap to maintain context\n",
    "3. Use cheaper model (Haiku) for chunk analysis\n",
    "4. Combine results with final summary\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Next Steps\n",
    "\n",
    "‚û°Ô∏è [Chapter 8: Cost Optimization](./Vol1_Ch8_Cost_Optimization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
