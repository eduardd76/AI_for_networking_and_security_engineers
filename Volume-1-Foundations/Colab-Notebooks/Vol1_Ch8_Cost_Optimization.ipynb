{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volume 1, Chapter 8: Cost Optimization\n",
    "\n",
    "**Reduce Your AI Bill by 50-70%**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eduardd76/AI_for_networking_and_security_engineers/blob/main/Volume-1-Foundations/Colab-Notebooks/Vol1_Ch8_Cost_Optimization.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**What you'll learn:**\n",
    "- üí∞ Calculate and track API costs\n",
    "- üîÄ Route to cheaper models intelligently\n",
    "- ‚úÇÔ∏è Minimize tokens in prompts\n",
    "- üì¶ Batch requests to save money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q anthropic tiktoken\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
    "except:\n",
    "    if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "        os.environ['ANTHROPIC_API_KEY'] = getpass('Anthropic API key: ')\n",
    "\n",
    "from anthropic import Anthropic\n",
    "import tiktoken\n",
    "\n",
    "client = Anthropic()\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(\"‚úì Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üí∞ Example 1: Cost Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRICING = {\n",
    "    \"claude-3-5-haiku-20241022\": {\"input\": 0.25, \"output\": 1.25},\n",
    "    \"claude-3-5-sonnet-20241022\": {\"input\": 3.00, \"output\": 15.00},\n",
    "}\n",
    "\n",
    "class CostTracker:\n",
    "    def __init__(self):\n",
    "        self.total_cost = 0\n",
    "        self.calls = 0\n",
    "    \n",
    "    def track(self, model, input_tokens, output_tokens):\n",
    "        p = PRICING[model]\n",
    "        cost = (input_tokens/1e6 * p[\"input\"]) + (output_tokens/1e6 * p[\"output\"])\n",
    "        self.total_cost += cost\n",
    "        self.calls += 1\n",
    "        return cost\n",
    "    \n",
    "    def report(self):\n",
    "        print(f\"Total calls: {self.calls}\")\n",
    "        print(f\"Total cost: ${self.total_cost:.4f}\")\n",
    "        print(f\"Avg cost/call: ${self.total_cost/max(self.calls,1):.4f}\")\n",
    "\n",
    "tracker = CostTracker()\n",
    "\n",
    "# Make some calls\n",
    "for i in range(3):\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-haiku-20241022\",\n",
    "        max_tokens=100,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is OSPF? One sentence.\"}]\n",
    "    )\n",
    "    cost = tracker.track(\n",
    "        \"claude-3-5-haiku-20241022\",\n",
    "        response.usage.input_tokens,\n",
    "        response.usage.output_tokens\n",
    "    )\n",
    "    print(f\"Call {i+1}: ${cost:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "tracker.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÄ Example 2: Smart Model Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_task(prompt):\n",
    "    \"\"\"Classify task complexity.\"\"\"\n",
    "    prompt_lower = prompt.lower()\n",
    "    \n",
    "    # Complex tasks need Sonnet\n",
    "    complex_keywords = [\"troubleshoot\", \"diagnose\", \"why\", \"design\", \"compare\", \"analyze security\"]\n",
    "    if any(kw in prompt_lower for kw in complex_keywords):\n",
    "        return \"complex\"\n",
    "    \n",
    "    # Simple tasks can use Haiku\n",
    "    simple_keywords = [\"extract\", \"list\", \"classify\", \"convert\", \"what is\", \"count\"]\n",
    "    if any(kw in prompt_lower for kw in simple_keywords):\n",
    "        return \"simple\"\n",
    "    \n",
    "    return \"medium\"\n",
    "\n",
    "def smart_call(prompt, max_tokens=500):\n",
    "    \"\"\"Route to appropriate model based on task.\"\"\"\n",
    "    complexity = classify_task(prompt)\n",
    "    \n",
    "    model = {\n",
    "        \"simple\": \"claude-3-5-haiku-20241022\",\n",
    "        \"medium\": \"claude-3-5-haiku-20241022\",\n",
    "        \"complex\": \"claude-3-5-sonnet-20241022\"\n",
    "    }[complexity]\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"complexity\": complexity,\n",
    "        \"response\": response.content[0].text,\n",
    "        \"input_tokens\": response.usage.input_tokens,\n",
    "        \"output_tokens\": response.usage.output_tokens\n",
    "    }\n",
    "\n",
    "# Test different prompts\n",
    "prompts = [\n",
    "    \"What is BGP?\",  # Simple ‚Üí Haiku\n",
    "    \"Extract the IP from: ip address 10.1.1.1 255.255.255.0\",  # Simple ‚Üí Haiku\n",
    "    \"Why won't my OSPF adjacency form? Troubleshoot this config...\",  # Complex ‚Üí Sonnet\n",
    "]\n",
    "\n",
    "print(\"üîÄ SMART MODEL ROUTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    result = smart_call(prompt)\n",
    "    print(f\"\\nPrompt: {prompt[:50]}...\")\n",
    "    print(f\"  Complexity: {result['complexity']}\")\n",
    "    print(f\"  Model: {result['model'].split('-')[-2]}\")\n",
    "    print(f\"  Tokens: {result['input_tokens']} in / {result['output_tokens']} out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÇÔ∏è Example 3: Token Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def minimize_prompt(prompt):\n",
    "    \"\"\"Remove unnecessary tokens from prompt.\"\"\"\n",
    "    original = prompt\n",
    "    \n",
    "    # Remove politeness\n",
    "    politeness = [\"please\", \"thank you\", \"thanks\", \"kindly\", \"I would like\", \n",
    "                  \"could you\", \"would you\", \"I appreciate\", \"if you could\"]\n",
    "    for phrase in politeness:\n",
    "        prompt = re.sub(rf'\\b{phrase}\\b', '', prompt, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove filler words\n",
    "    fillers = [\"basically\", \"actually\", \"really\", \"very\", \"just\"]\n",
    "    for word in fillers:\n",
    "        prompt = re.sub(rf'\\b{word}\\b', '', prompt, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Clean up whitespace\n",
    "    prompt = re.sub(r'\\s+', ' ', prompt).strip()\n",
    "    \n",
    "    return {\n",
    "        \"original\": original,\n",
    "        \"optimized\": prompt,\n",
    "        \"original_tokens\": count_tokens(original),\n",
    "        \"optimized_tokens\": count_tokens(prompt),\n",
    "        \"saved\": count_tokens(original) - count_tokens(prompt)\n",
    "    }\n",
    "\n",
    "# Test\n",
    "verbose_prompt = \"\"\"\n",
    "Hello! I would really appreciate it if you could please help me out.\n",
    "Could you kindly analyze this network configuration for me?\n",
    "I just basically need to know if there are any security issues.\n",
    "Thank you so much for your help!\n",
    "\n",
    "Config:\n",
    "interface Gi0/0\n",
    " ip address 10.1.1.1 255.255.255.0\n",
    "\"\"\"\n",
    "\n",
    "result = minimize_prompt(verbose_prompt)\n",
    "\n",
    "print(\"‚úÇÔ∏è PROMPT MINIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original: {result['original_tokens']} tokens\")\n",
    "print(f\"Optimized: {result['optimized_tokens']} tokens\")\n",
    "print(f\"Saved: {result['saved']} tokens ({result['saved']/result['original_tokens']*100:.0f}%)\")\n",
    "print(f\"\\nOptimized prompt:\\n{result['optimized']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ Example 4: Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_analyze(items, batch_size=5):\n",
    "    \"\"\"Analyze multiple items in fewer API calls.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(items), batch_size):\n",
    "        batch = items[i:i+batch_size]\n",
    "        \n",
    "        # Combine into single prompt\n",
    "        batch_prompt = \"Classify each log entry (INFO/WARNING/ERROR/CRITICAL):\\n\\n\"\n",
    "        for j, item in enumerate(batch):\n",
    "            batch_prompt += f\"{j+1}. {item}\\n\"\n",
    "        batch_prompt += \"\\nReturn only: 1. LEVEL, 2. LEVEL, etc.\"\n",
    "        \n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-haiku-20241022\",\n",
    "            max_tokens=200,\n",
    "            messages=[{\"role\": \"user\", \"content\": batch_prompt}]\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"batch_size\": len(batch),\n",
    "            \"response\": response.content[0].text,\n",
    "            \"tokens\": response.usage.input_tokens + response.usage.output_tokens\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Sample logs\n",
    "logs = [\n",
    "    \"%LINK-3-UPDOWN: Interface Gi0/1, changed state to down\",\n",
    "    \"%SYS-5-CONFIG_I: Configured from console\",\n",
    "    \"%OSPF-5-ADJCHG: Nbr 10.1.1.1 DOWN\",\n",
    "    \"%LINEPROTO-5-UPDOWN: Line protocol on Gi0/0, changed state to up\",\n",
    "    \"%SYS-2-MALLOCFAIL: Memory allocation failed\",\n",
    "]\n",
    "\n",
    "print(\"üì¶ BATCH VS INDIVIDUAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Batch approach\n",
    "batch_results = batch_analyze(logs, batch_size=5)\n",
    "batch_tokens = sum(r[\"tokens\"] for r in batch_results)\n",
    "print(f\"\\nBatch (1 call for {len(logs)} items):\")\n",
    "print(f\"  Total tokens: {batch_tokens}\")\n",
    "print(f\"  Response: {batch_results[0]['response']}\")\n",
    "\n",
    "# Individual approach (for comparison)\n",
    "individual_tokens = 0\n",
    "for log in logs:\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-haiku-20241022\",\n",
    "        max_tokens=20,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Classify (INFO/WARNING/ERROR/CRITICAL): {log}\"}]\n",
    "    )\n",
    "    individual_tokens += response.usage.input_tokens + response.usage.output_tokens\n",
    "\n",
    "print(f\"\\nIndividual ({len(logs)} separate calls):\")\n",
    "print(f\"  Total tokens: {individual_tokens}\")\n",
    "print(f\"\\n‚úÖ Batch saved {individual_tokens - batch_tokens} tokens ({(1-batch_tokens/individual_tokens)*100:.0f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Example 5: Monthly Cost Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_monthly_cost(daily_calls, avg_input_tokens, avg_output_tokens, model):\n",
    "    p = PRICING[model]\n",
    "    daily_cost = (daily_calls * avg_input_tokens / 1e6 * p[\"input\"]) + \\\n",
    "                 (daily_calls * avg_output_tokens / 1e6 * p[\"output\"])\n",
    "    return daily_cost * 30\n",
    "\n",
    "print(\"üìä MONTHLY COST PROJECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scenarios = [\n",
    "    {\"name\": \"Light use\", \"daily_calls\": 100, \"input\": 500, \"output\": 200},\n",
    "    {\"name\": \"Medium use\", \"daily_calls\": 500, \"input\": 500, \"output\": 200},\n",
    "    {\"name\": \"Heavy use\", \"daily_calls\": 2000, \"input\": 500, \"output\": 200},\n",
    "]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    haiku_cost = project_monthly_cost(\n",
    "        scenario[\"daily_calls\"], scenario[\"input\"], scenario[\"output\"],\n",
    "        \"claude-3-5-haiku-20241022\"\n",
    "    )\n",
    "    sonnet_cost = project_monthly_cost(\n",
    "        scenario[\"daily_calls\"], scenario[\"input\"], scenario[\"output\"],\n",
    "        \"claude-3-5-sonnet-20241022\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{scenario['name']} ({scenario['daily_calls']} calls/day):\")\n",
    "    print(f\"  Haiku:  ${haiku_cost:>8.2f}/month\")\n",
    "    print(f\"  Sonnet: ${sonnet_cost:>8.2f}/month\")\n",
    "    print(f\"  Savings with Haiku: ${sonnet_cost - haiku_cost:.2f} ({(1-haiku_cost/sonnet_cost)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "| Strategy | Savings | Effort |\n",
    "|----------|---------|--------|\n",
    "| Use Haiku for simple tasks | 50-90% | Low |\n",
    "| Remove verbose language | 10-20% | Low |\n",
    "| Batch multiple items | 30-50% | Medium |\n",
    "| Cache repeated prompts | 50-80% | Medium |\n",
    "\n",
    "**Quick wins:**\n",
    "1. Default to Haiku, upgrade only when needed\n",
    "2. Remove \"please\", \"thank you\", filler words\n",
    "3. Batch similar tasks into single calls\n",
    "4. Track costs to find optimization opportunities\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Next Steps\n",
    "\n",
    "‚û°Ô∏è [Chapter 9: Working with Network Data](./Vol1_Ch9_Network_Data.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
