{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volume 1, Chapter 11: Testing and Validation\n",
    "\n",
    "**Test Your AI Systems Systematically**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eduardd76/AI_for_networking_and_security_engineers/blob/main/Volume-1-Foundations/Colab-Notebooks/Vol1_Ch11_Testing.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**What you'll learn:**\n",
    "- ‚úÖ Unit test prompts and outputs\n",
    "- üìä Measure accuracy and quality\n",
    "- üîÑ Detect regressions\n",
    "- üß™ Generate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q anthropic\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
    "except:\n",
    "    if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "        os.environ['ANTHROPIC_API_KEY'] = getpass('Anthropic API key: ')\n",
    "\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "print(\"‚úì Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Example 1: Unit Test for Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTester:\n",
    "    \"\"\"Test prompts with expected outputs.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "    \n",
    "    def test(self, name, prompt, expected_contains, model=\"claude-3-5-haiku-20241022\"):\n",
    "        \"\"\"Run a single test.\"\"\"\n",
    "        response = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=200,\n",
    "            temperature=0,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        output = response.content[0].text.lower()\n",
    "        passed = all(exp.lower() in output for exp in expected_contains)\n",
    "        \n",
    "        self.results.append({\n",
    "            \"name\": name,\n",
    "            \"passed\": passed,\n",
    "            \"output\": response.content[0].text[:100]\n",
    "        })\n",
    "        \n",
    "        return passed\n",
    "    \n",
    "    def report(self):\n",
    "        passed = sum(1 for r in self.results if r[\"passed\"])\n",
    "        total = len(self.results)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TEST RESULTS: {passed}/{total} passed ({passed/total*100:.0f}%)\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        for r in self.results:\n",
    "            status = \"‚úÖ\" if r[\"passed\"] else \"‚ùå\"\n",
    "            print(f\"{status} {r['name']}\")\n",
    "            if not r[\"passed\"]:\n",
    "                print(f\"   Output: {r['output']}\")\n",
    "\n",
    "# Run tests\n",
    "tester = PromptTester()\n",
    "\n",
    "print(\"‚úÖ PROMPT UNIT TESTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Log classification\n",
    "tester.test(\n",
    "    \"Log classification - OSPF down\",\n",
    "    \"Classify severity (INFO/WARNING/ERROR/CRITICAL): %OSPF-5-ADJCHG: Neighbor DOWN\",\n",
    "    [\"error\", \"critical\"]  # Should contain one of these\n",
    ")\n",
    "\n",
    "# Test 2: IP extraction\n",
    "tester.test(\n",
    "    \"IP extraction\",\n",
    "    \"Extract IP from: ip address 192.168.1.1 255.255.255.0. Return only IP.\",\n",
    "    [\"192.168.1.1\"]\n",
    ")\n",
    "\n",
    "# Test 3: Subnet calculation\n",
    "tester.test(\n",
    "    \"Subnet hosts\",\n",
    "    \"Usable hosts in /24? Return only the number.\",\n",
    "    [\"254\"]\n",
    ")\n",
    "\n",
    "# Test 4: Protocol identification\n",
    "tester.test(\n",
    "    \"Protocol ID\",\n",
    "    \"What protocol uses port 179? One word answer.\",\n",
    "    [\"bgp\"]\n",
    ")\n",
    "\n",
    "tester.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Example 2: Accuracy Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_accuracy(test_cases):\n",
    "    \"\"\"Measure accuracy on labeled test data.\"\"\"\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    for test in test_cases:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-haiku-20241022\",\n",
    "            max_tokens=50,\n",
    "            temperature=0,\n",
    "            messages=[{\"role\": \"user\", \"content\": test[\"prompt\"]}]\n",
    "        )\n",
    "        \n",
    "        predicted = response.content[0].text.strip().upper()\n",
    "        expected = test[\"expected\"].upper()\n",
    "        is_correct = expected in predicted\n",
    "        \n",
    "        correct += is_correct\n",
    "        results.append({\n",
    "            \"input\": test[\"prompt\"][:50],\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": predicted[:20],\n",
    "            \"correct\": is_correct\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": correct / len(test_cases),\n",
    "        \"correct\": correct,\n",
    "        \"total\": len(test_cases),\n",
    "        \"details\": results\n",
    "    }\n",
    "\n",
    "# Test cases for log severity classification\n",
    "test_cases = [\n",
    "    {\"prompt\": \"Classify (INFO/ERROR): %SYS-5-CONFIG_I: Configured from console\", \"expected\": \"INFO\"},\n",
    "    {\"prompt\": \"Classify (INFO/ERROR): %LINK-3-UPDOWN: Interface down\", \"expected\": \"ERROR\"},\n",
    "    {\"prompt\": \"Classify (INFO/ERROR): %OSPF-5-ADJCHG: Neighbor FULL\", \"expected\": \"INFO\"},\n",
    "    {\"prompt\": \"Classify (INFO/ERROR): %SYS-2-MALLOCFAIL: Memory allocation failed\", \"expected\": \"ERROR\"},\n",
    "]\n",
    "\n",
    "results = measure_accuracy(test_cases)\n",
    "\n",
    "print(\"üìä ACCURACY MEASUREMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {results['accuracy']*100:.0f}% ({results['correct']}/{results['total']})\")\n",
    "print(\"\\nDetails:\")\n",
    "for r in results['details']:\n",
    "    status = \"‚úÖ\" if r['correct'] else \"‚ùå\"\n",
    "    print(f\"  {status} Expected: {r['expected']}, Got: {r['predicted']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ Example 3: Regression Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "\n",
    "class RegressionTracker:\n",
    "    \"\"\"Track output changes across runs.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.baseline = {}\n",
    "    \n",
    "    def set_baseline(self, test_name, output):\n",
    "        \"\"\"Save baseline output.\"\"\"\n",
    "        self.baseline[test_name] = {\n",
    "            \"output\": output,\n",
    "            \"hash\": hashlib.md5(output.encode()).hexdigest()[:8]\n",
    "        }\n",
    "    \n",
    "    def check(self, test_name, current_output):\n",
    "        \"\"\"Check for regression.\"\"\"\n",
    "        if test_name not in self.baseline:\n",
    "            return {\"status\": \"NEW\", \"message\": \"No baseline\"}\n",
    "        \n",
    "        baseline = self.baseline[test_name]\n",
    "        current_hash = hashlib.md5(current_output.encode()).hexdigest()[:8]\n",
    "        \n",
    "        if current_hash == baseline[\"hash\"]:\n",
    "            return {\"status\": \"SAME\", \"message\": \"No change\"}\n",
    "        else:\n",
    "            return {\n",
    "                \"status\": \"CHANGED\",\n",
    "                \"message\": \"Output differs from baseline\",\n",
    "                \"baseline\": baseline[\"output\"][:100],\n",
    "                \"current\": current_output[:100]\n",
    "            }\n",
    "\n",
    "# Demo\n",
    "tracker = RegressionTracker()\n",
    "\n",
    "# Set baseline\n",
    "prompt = \"What does OSPF stand for? One line answer.\"\n",
    "response1 = client.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    "    max_tokens=50,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "tracker.set_baseline(\"ospf_definition\", response1.content[0].text)\n",
    "\n",
    "# Check again (should be same with temperature=0)\n",
    "response2 = client.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    "    max_tokens=50,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "result = tracker.check(\"ospf_definition\", response2.content[0].text)\n",
    "\n",
    "print(\"üîÑ REGRESSION CHECK\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Status: {result['status']}\")\n",
    "print(f\"Message: {result['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ Example 4: Generate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_data(category, count=5):\n",
    "    \"\"\"Use AI to generate test cases.\"\"\"\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7,  # Some creativity for variety\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Generate {count} realistic {category} test cases.\n",
    "\n",
    "Return JSON array:\n",
    "[\n",
    "  {{\"input\": \"...\", \"expected_output\": \"...\", \"description\": \"...\"}}\n",
    "]\n",
    "\n",
    "Make them diverse and realistic. ONLY JSON.\"\"\"\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    import re\n",
    "    text = response.content[0].text\n",
    "    json_match = re.search(r'\\[.*\\]', text, re.DOTALL)\n",
    "    return json.loads(json_match.group()) if json_match else []\n",
    "\n",
    "# Generate syslog test cases\n",
    "test_data = generate_test_data(\"Cisco syslog messages with severity levels\", 5)\n",
    "\n",
    "print(\"üß™ GENERATED TEST DATA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Generated {len(test_data)} test cases:\\n\")\n",
    "\n",
    "for i, tc in enumerate(test_data, 1):\n",
    "    print(f\"{i}. {tc.get('description', 'Test case')}\")\n",
    "    print(f\"   Input: {tc.get('input', '')[:60]}...\")\n",
    "    print(f\"   Expected: {tc.get('expected_output', '')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Example 5: Quality Metrics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_dashboard(test_suite):\n",
    "    \"\"\"Run full test suite and show metrics.\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"total\": len(test_suite),\n",
    "        \"passed\": 0,\n",
    "        \"failed\": 0,\n",
    "        \"by_category\": {}\n",
    "    }\n",
    "    \n",
    "    for test in test_suite:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-haiku-20241022\",\n",
    "            max_tokens=100,\n",
    "            temperature=0,\n",
    "            messages=[{\"role\": \"user\", \"content\": test[\"prompt\"]}]\n",
    "        )\n",
    "        \n",
    "        output = response.content[0].text.lower()\n",
    "        passed = test[\"expected\"].lower() in output\n",
    "        \n",
    "        if passed:\n",
    "            results[\"passed\"] += 1\n",
    "        else:\n",
    "            results[\"failed\"] += 1\n",
    "        \n",
    "        category = test.get(\"category\", \"other\")\n",
    "        if category not in results[\"by_category\"]:\n",
    "            results[\"by_category\"][category] = {\"passed\": 0, \"failed\": 0}\n",
    "        results[\"by_category\"][category][\"passed\" if passed else \"failed\"] += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test suite\n",
    "test_suite = [\n",
    "    {\"prompt\": \"Port for SSH?\", \"expected\": \"22\", \"category\": \"protocols\"},\n",
    "    {\"prompt\": \"Port for HTTPS?\", \"expected\": \"443\", \"category\": \"protocols\"},\n",
    "    {\"prompt\": \"Port for BGP?\", \"expected\": \"179\", \"category\": \"protocols\"},\n",
    "    {\"prompt\": \"Hosts in /30?\", \"expected\": \"2\", \"category\": \"subnetting\"},\n",
    "    {\"prompt\": \"Hosts in /24?\", \"expected\": \"254\", \"category\": \"subnetting\"},\n",
    "]\n",
    "\n",
    "results = quality_dashboard(test_suite)\n",
    "\n",
    "print(\"üìà QUALITY DASHBOARD\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOverall: {results['passed']}/{results['total']} ({results['passed']/results['total']*100:.0f}%)\")\n",
    "print(f\"\\nBy Category:\")\n",
    "for cat, stats in results[\"by_category\"].items():\n",
    "    total = stats[\"passed\"] + stats[\"failed\"]\n",
    "    pct = stats[\"passed\"] / total * 100\n",
    "    print(f\"  {cat}: {stats['passed']}/{total} ({pct:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "| Test Type | Purpose | When to Run |\n",
    "|-----------|---------|-------------|\n",
    "| Unit tests | Verify prompts work | Every commit |\n",
    "| Accuracy tests | Measure quality | Weekly |\n",
    "| Regression tests | Detect changes | After model updates |\n",
    "| Generated tests | Cover edge cases | Periodically |\n",
    "\n",
    "**Testing best practices:**\n",
    "1. Use temperature=0 for deterministic tests\n",
    "2. Save baselines for regression detection\n",
    "3. Test on diverse, realistic data\n",
    "4. Track metrics over time\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Next Steps\n",
    "\n",
    "‚û°Ô∏è [Chapter 12: Ethics and Responsible AI](./Vol1_Ch12_Ethics.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
