{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Chapter 36: Advanced RAG Techniques\n",
    "\n",
    "Run this notebook directly in Google Colab - no local Python needed!\n",
    "\n",
    "**Full code**: [GitHub](https://github.com/eduardd76/AI_for_networking_and_security_engineers/tree/main/CODE/Volume-3-Production-Systems/Chapter-36-Advanced-RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install dependencies and configure API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q chromadb sentence-transformers anthropic rank-bm25 python-dotenv\n",
    "\n",
    "# Import and configure API key\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Check for Colab secrets first\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
    "    print('✓ Using API keys from Colab secrets')\n",
    "except:\n",
    "    # Fall back to manual entry\n",
    "    if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "        os.environ['ANTHROPIC_API_KEY'] = getpass('Enter ANTHROPIC_API_KEY: ')\n",
    "    print('✓ API keys configured')\n",
    "\n",
    "print('\\n✅ Setup complete! Ready to run examples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Example 1: Hybrid Search (Vector + Keyword + Graph)\n",
    "\n",
    "Combine vector search, BM25 keyword search, and graph relationships for comprehensive retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "class HybridNetworkSearch:\n",
    "    \"\"\"Hybrid search for network documentation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Vector search with ChromaDB\n",
    "        self.client = chromadb.Client()\n",
    "        self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name=\"all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=\"network_docs_hybrid\",\n",
    "            embedding_function=self.embedding_fn\n",
    "        )\n",
    "        \n",
    "        # BM25 for keyword search\n",
    "        self.documents = []\n",
    "        self.doc_ids = []\n",
    "        self.bm25 = None\n",
    "        \n",
    "        # Graph for relationships\n",
    "        self.graph = {}\n",
    "\n",
    "    def add_documents(self, documents: List[Dict[str, str]]):\n",
    "        \"\"\"Add documents with metadata.\"\"\"\n",
    "        ids = [doc['id'] for doc in documents]\n",
    "        texts = [doc['text'] for doc in documents]\n",
    "        metadatas = [{k: v for k, v in doc.items() if k not in ['id', 'text']} for doc in documents]\n",
    "\n",
    "        # Add to vector store\n",
    "        self.collection.add(ids=ids, documents=texts, metadatas=metadatas)\n",
    "\n",
    "        # Build BM25 index\n",
    "        self.documents = texts\n",
    "        self.doc_ids = ids\n",
    "        tokenized_docs = [self._tokenize(doc) for doc in texts]\n",
    "        self.bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "        # Build relationship graph\n",
    "        for doc in documents:\n",
    "            device = doc.get('device', 'unknown')\n",
    "            if device not in self.graph:\n",
    "                self.graph[device] = []\n",
    "            self.graph[device].append(doc['id'])\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple tokenization for BM25.\"\"\"\n",
    "        tokens = re.findall(r'\\b\\w+\\b|\\d+\\.\\d+\\.\\d+\\.\\d+', text.lower())\n",
    "        return tokens\n",
    "\n",
    "    def hybrid_search(self, query: str, device: str = None, n_results: int = 5,\n",
    "                     vector_weight: float = 0.5, keyword_weight: float = 0.3, \n",
    "                     graph_weight: float = 0.2):\n",
    "        \"\"\"Combine all search methods with weighted fusion.\"\"\"\n",
    "        all_scores = {}\n",
    "\n",
    "        # Vector search\n",
    "        vector_results = self.collection.query(query_texts=[query], n_results=n_results * 2)\n",
    "        for i, doc_id in enumerate(vector_results['ids'][0]):\n",
    "            score = 1.0 - vector_results['distances'][0][i]\n",
    "            all_scores[doc_id] = all_scores.get(doc_id, 0) + (score * vector_weight)\n",
    "\n",
    "        # Keyword search\n",
    "        if self.bm25:\n",
    "            tokenized_query = self._tokenize(query)\n",
    "            scores = self.bm25.get_scores(tokenized_query)\n",
    "            max_score = max(scores) if max(scores) > 0 else 1.0\n",
    "            for i, score in enumerate(scores):\n",
    "                if score > 0:\n",
    "                    normalized = score / max_score\n",
    "                    all_scores[self.doc_ids[i]] = all_scores.get(self.doc_ids[i], 0) + (normalized * keyword_weight)\n",
    "\n",
    "        # Graph search (if device specified)\n",
    "        if device and device in self.graph:\n",
    "            for doc_id in self.graph[device][:n_results * 2]:\n",
    "                all_scores[doc_id] = all_scores.get(doc_id, 0) + graph_weight\n",
    "\n",
    "        # Sort by combined score\n",
    "        sorted_results = sorted(all_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_results = sorted_results[:n_results]\n",
    "\n",
    "        # Retrieve full documents\n",
    "        final_results = []\n",
    "        for doc_id, score in top_results:\n",
    "            doc_result = self.collection.get(ids=[doc_id])\n",
    "            if doc_result['documents']:\n",
    "                final_results.append({\n",
    "                    'id': doc_id,\n",
    "                    'text': doc_result['documents'][0],\n",
    "                    'metadata': doc_result['metadatas'][0],\n",
    "                    'score': score\n",
    "                })\n",
    "\n",
    "        return final_results\n",
    "\n",
    "# Example usage\n",
    "search = HybridNetworkSearch()\n",
    "\n",
    "# Add network documentation\n",
    "docs = [\n",
    "    {\n",
    "        'id': 'config_rtr01_bgp',\n",
    "        'text': 'router bgp 65000\\n neighbor 10.1.1.2 remote-as 65001',\n",
    "        'type': 'config',\n",
    "        'device': 'rtr01',\n",
    "        'timestamp': '2024-01-15'\n",
    "    },\n",
    "    {\n",
    "        'id': 'ticket_1234',\n",
    "        'text': 'BGP session down on rtr01. Root cause: MTU mismatch. Fixed by setting ip mtu 1500.',\n",
    "        'type': 'ticket',\n",
    "        'device': 'rtr01',\n",
    "        'timestamp': '2024-01-16'\n",
    "    },\n",
    "    {\n",
    "        'id': 'config_rtr02_bgp',\n",
    "        'text': 'router bgp 65001\\n neighbor 10.1.1.1 remote-as 65000',\n",
    "        'type': 'config',\n",
    "        'device': 'rtr02',\n",
    "        'timestamp': '2024-01-15'\n",
    "    }\n",
    "]\n",
    "\n",
    "search.add_documents(docs)\n",
    "\n",
    "# Test hybrid search\n",
    "print(\"=== Query: 'BGP AS 65000' ===\")\n",
    "results = search.hybrid_search(\"BGP AS 65000\", n_results=3)\n",
    "for result in results:\n",
    "    print(f\"\\n{result['id']} (score: {result['score']:.3f})\")\n",
    "    print(f\"Type: {result['metadata']['type']}, Device: {result['metadata']['device']}\")\n",
    "    print(f\"Text: {result['text'][:100]}...\")\n",
    "\n",
    "# Device-specific query\n",
    "print(\"\\n\\n=== Query: 'BGP config' for device 'rtr01' ===\")\n",
    "results = search.hybrid_search(\"BGP config\", device=\"rtr01\", n_results=3)\n",
    "for result in results:\n",
    "    print(f\"\\n{result['id']} (score: {result['score']:.3f})\")\n",
    "    print(f\"Type: {result['metadata']['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Example 2: Query Routing and Classification\n",
    "\n",
    "Route queries to appropriate search strategies based on query type and intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "import json\n",
    "\n",
    "class QueryRouter:\n",
    "    \"\"\"Route queries to appropriate search strategies.\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, hybrid_search):\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        self.search = hybrid_search\n",
    "\n",
    "    def classify_query(self, query: str):\n",
    "        \"\"\"Use Claude to classify query intent and extract entities.\"\"\"\n",
    "        prompt = f\"\"\"Analyze this network operations query and extract:\n",
    "\n",
    "1. Query type: device_specific, exact_match, incident_analysis, conceptual, or multi_hop\n",
    "2. Entities: device names, IP addresses, AS numbers, interface names, etc.\n",
    "3. Requires multi-hop reasoning: yes/no\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"query_type\": \"...\",\n",
    "  \"entities\": {{}},\n",
    "  \"multi_hop\": true/false,\n",
    "  \"search_strategy\": \"...\"\n",
    "}}\"\"\"\n",
    "\n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-5-20250929\",\n",
    "            max_tokens=500,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        content = response.content[0].text\n",
    "        if \"```json\" in content:\n",
    "            content = content.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in content:\n",
    "            content = content.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "\n",
    "        return json.loads(content)\n",
    "\n",
    "    def route_query(self, query: str, n_results: int = 5):\n",
    "        \"\"\"Route query to appropriate search strategy.\"\"\"\n",
    "        classification = self.classify_query(query)\n",
    "        query_type = classification.get('query_type', 'conceptual')\n",
    "        entities = classification.get('entities', {})\n",
    "\n",
    "        # Execute search based on classification\n",
    "        if query_type == 'device_specific' and entities.get('device'):\n",
    "            results = self.search.hybrid_search(\n",
    "                query, device=entities['device'], n_results=n_results,\n",
    "                vector_weight=0.2, keyword_weight=0.3, graph_weight=0.5\n",
    "            )\n",
    "            strategy = 'graph_focused'\n",
    "        elif query_type == 'exact_match':\n",
    "            results = self.search.hybrid_search(\n",
    "                query, n_results=n_results,\n",
    "                vector_weight=0.2, keyword_weight=0.7, graph_weight=0.1\n",
    "            )\n",
    "            strategy = 'keyword_focused'\n",
    "        else:\n",
    "            results = self.search.hybrid_search(\n",
    "                query, n_results=n_results,\n",
    "                vector_weight=0.7, keyword_weight=0.2, graph_weight=0.1\n",
    "            )\n",
    "            strategy = 'vector_focused'\n",
    "\n",
    "        return {\n",
    "            'query': query,\n",
    "            'classification': classification,\n",
    "            'strategy': strategy,\n",
    "            'results': results\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "router = QueryRouter(api_key=os.getenv('ANTHROPIC_API_KEY'), hybrid_search=search)\n",
    "\n",
    "queries = [\n",
    "    \"What is BGP?\",\n",
    "    \"Show BGP config for rtr01\",\n",
    "    \"Find AS 65000\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n=== Query: {query} ===\")\n",
    "    result = router.route_query(query, n_results=2)\n",
    "    print(f\"Classification: {result['classification']['query_type']}\")\n",
    "    print(f\"Strategy: {result['strategy']}\")\n",
    "    print(\"Results:\")\n",
    "    for r in result['results']:\n",
    "        print(f\"  - {r['id']} (score: {r['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Example 3: Context Compression\n",
    "\n",
    "Compress retrieved context to only relevant information using LLM extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "class ContextCompressor:\n",
    "    \"\"\"Compress retrieved context to only relevant information.\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "\n",
    "    def compress_document(self, query: str, document: str, doc_id: str) -> str:\n",
    "        \"\"\"Extract only query-relevant sentences from document.\"\"\"\n",
    "        prompt = f\"\"\"Extract only the sentences from this document that are relevant to answering the query.\n",
    "Preserve exact wording. Remove irrelevant content.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Document ID: {doc_id}\n",
    "Document:\n",
    "{document}\n",
    "\n",
    "Return only the relevant excerpts, maintaining original formatting where possible.\"\"\"\n",
    "\n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-5-20250929\",\n",
    "            max_tokens=2000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        return response.content[0].text.strip()\n",
    "\n",
    "# Example usage\n",
    "compressor = ContextCompressor(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "# Simulate retrieved document\n",
    "config_doc = \"\"\"hostname rtr01\n",
    "!\n",
    "interface Loopback0\n",
    " ip address 192.168.1.1 255.255.255.255\n",
    "!\n",
    "interface GigabitEthernet0/0\n",
    " description Link to DC2\n",
    " ip address 10.1.1.1 255.255.255.252\n",
    " ip mtu 1500\n",
    "!\n",
    "router bgp 65000\n",
    " neighbor 10.1.1.2 remote-as 65001\n",
    " neighbor 10.1.1.2 description PEER_TO_DC2\n",
    " network 192.168.1.0 mask 255.255.255.0\n",
    "!\n",
    "line vty 0 4\n",
    " login local\n",
    " transport input ssh\n",
    "\"\"\"\n",
    "\n",
    "query = \"What is the BGP configuration?\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Original length: {len(config_doc)} characters\\n\")\n",
    "print(\"Original document:\")\n",
    "print(config_doc)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "compressed = compressor.compress_document(query, config_doc, \"config_rtr01\")\n",
    "\n",
    "print(f\"Compressed length: {len(compressed)} characters\")\n",
    "print(f\"Compression ratio: {len(compressed)/len(config_doc):.1%}\\n\")\n",
    "print(\"Compressed document:\")\n",
    "print(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Example 4: RAG Evaluation Metrics\n",
    "\n",
    "Measure RAG system performance with retrieval precision, recall, and answer faithfulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "import json\n",
    "\n",
    "class RAGEvaluator:\n",
    "    \"\"\"Evaluate RAG system performance.\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "\n",
    "    def evaluate_retrieval(self, retrieved_doc_ids: list, relevant_doc_ids: list):\n",
    "        \"\"\"Evaluate retrieval quality.\"\"\"\n",
    "        retrieved_set = set(retrieved_doc_ids)\n",
    "        relevant_set = set(relevant_doc_ids)\n",
    "\n",
    "        tp = len(retrieved_set & relevant_set)  # True positives\n",
    "        fp = len(retrieved_set - relevant_set)  # False positives\n",
    "        fn = len(relevant_set - retrieved_set)  # False negatives\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn\n",
    "        }\n",
    "\n",
    "    def evaluate_answer_faithfulness(self, answer: str, retrieved_docs: list):\n",
    "        \"\"\"Check if answer is grounded in retrieved documents.\"\"\"\n",
    "        context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc}\" for i, doc in enumerate(retrieved_docs)])\n",
    "\n",
    "        prompt = f\"\"\"Evaluate if this answer is faithful to the provided documents.\n",
    "\n",
    "Answer to evaluate:\n",
    "{answer}\n",
    "\n",
    "Retrieved documents:\n",
    "{context}\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"faithful\": true/false,\n",
    "  \"faithfulness_score\": 0.0-1.0,\n",
    "  \"unsupported_claims\": [\"claim1\", \"claim2\"],\n",
    "  \"explanation\": \"brief explanation\"\n",
    "}}\"\"\"\n",
    "\n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-5-20250929\",\n",
    "            max_tokens=1000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        content = response.content[0].text\n",
    "        if \"```json\" in content:\n",
    "            content = content.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in content:\n",
    "            content = content.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "\n",
    "        return json.loads(content)\n",
    "\n",
    "# Example usage\n",
    "evaluator = RAGEvaluator(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "# Example 1: Evaluate retrieval\n",
    "print(\"=== Retrieval Evaluation ===\\n\")\n",
    "retrieval_result = evaluator.evaluate_retrieval(\n",
    "    retrieved_doc_ids=['config_rtr01_bgp', 'ticket_1234', 'doc_bgp_overview'],\n",
    "    relevant_doc_ids=['config_rtr01_bgp', 'ticket_1234']\n",
    ")\n",
    "\n",
    "print(f\"Precision: {retrieval_result['precision']:.2%}\")\n",
    "print(f\"Recall: {retrieval_result['recall']:.2%}\")\n",
    "print(f\"F1 Score: {retrieval_result['f1']:.2%}\")\n",
    "print(f\"True Positives: {retrieval_result['true_positives']}\")\n",
    "print(f\"False Positives: {retrieval_result['false_positives']}\")\n",
    "print(f\"False Negatives: {retrieval_result['false_negatives']}\")\n",
    "\n",
    "# Example 2: Evaluate faithfulness\n",
    "print(\"\\n=== Faithfulness Evaluation ===\\n\")\n",
    "answer = \"The BGP configuration on rtr01 uses AS 65000 and peers with 10.1.1.2 in AS 65001.\"\n",
    "docs = [\"router bgp 65000\\n neighbor 10.1.1.2 remote-as 65001\"]\n",
    "\n",
    "faithfulness = evaluator.evaluate_answer_faithfulness(answer, docs)\n",
    "print(f\"Faithful: {faithfulness['faithful']}\")\n",
    "print(f\"Score: {faithfulness['faithfulness_score']:.2%}\")\n",
    "print(f\"Explanation: {faithfulness['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Interactive Section\n",
    "\n",
    "Try your own RAG experiments here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Experiment with different RAG techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Full code: [Chapter 36 on GitHub](https://github.com/eduardd76/AI_for_networking_and_security_engineers/tree/main/CODE/Volume-3-Production-Systems/Chapter-36-Advanced-RAG)\n",
    "- Learn more: [vExpertAI.com](https://vexpertai.com)\n",
    "- Author: Eduard Dulharu ([@eduardd76](https://github.com/eduardd76))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
