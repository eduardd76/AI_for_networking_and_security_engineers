{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 40: Caching Strategies for AI Systems\n",
    "\n",
    "Run this notebook directly in Google Colab - no local Python needed!\n",
    "\n",
    "**Full code**: [GitHub](https://github.com/eduardd76/AI_for_networking_and_security_engineers/tree/main/CODE/Volume-3-Production-Systems/Chapter-40-Caching-Strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install dependencies and configure API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q redis anthropic numpy\n",
    "\n",
    "# Import and configure API key\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Check for Colab secrets first\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
    "    print('✓ Using API keys from Colab secrets')\n",
    "except:\n",
    "    # Fall back to manual entry\n",
    "    if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "        os.environ['ANTHROPIC_API_KEY'] = getpass('Enter ANTHROPIC_API_KEY: ')\n",
    "    print('✓ API keys configured')\n",
    "\n",
    "print('\\n✅ Setup complete! Ready to run examples.')\n",
    "print('\\n⚠️  Note: Redis examples require a running Redis instance.')\n",
    "print('   For testing without Redis, examples will use in-memory storage.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Semantic Cache\n",
    "\n",
    "Implement semantic caching for LLM responses using similarity matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class SimpleSemanticCache:\n",
    "    \"\"\"\n",
    "    In-memory semantic cache for LLM responses.\n",
    "    Uses simple hash-based embeddings for demonstration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, similarity_threshold: float = 0.95):\n",
    "        self.cache = {}  # Store: {cache_key: {embedding, response, timestamp}}\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.total_latency_saved = 0.0\n",
    "    \n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Generate simple embedding from text (for demo purposes)\"\"\"\n",
    "        # Normalize text\n",
    "        normalized = text.lower().strip()\n",
    "        \n",
    "        # Create hash-based embedding\n",
    "        hash_obj = hashlib.sha256(normalized.encode())\n",
    "        hash_bytes = hash_obj.digest()\n",
    "        \n",
    "        # Convert to vector\n",
    "        embedding = np.frombuffer(hash_bytes, dtype=np.uint8).astype(float)\n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"Calculate cosine similarity\"\"\"\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm1 = np.linalg.norm(vec1)\n",
    "        norm2 = np.linalg.norm(vec2)\n",
    "        \n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dot_product / (norm1 * norm2)\n",
    "    \n",
    "    def _search_cache(self, embedding: np.ndarray) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Search for similar cached entries\"\"\"\n",
    "        best_match = None\n",
    "        best_similarity = 0.0\n",
    "        \n",
    "        for cache_key, cached_data in self.cache.items():\n",
    "            cached_embedding = np.array(cached_data[\"embedding\"])\n",
    "            similarity = self._cosine_similarity(embedding, cached_embedding)\n",
    "            \n",
    "            if similarity > best_similarity and similarity >= self.similarity_threshold:\n",
    "                best_similarity = similarity\n",
    "                best_match = cached_data.copy()\n",
    "                best_match[\"similarity\"] = similarity\n",
    "        \n",
    "        return best_match\n",
    "    \n",
    "    def get(self, prompt: str) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"Get response from cache or simulate LLM call\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate embedding\n",
    "        query_embedding = self._get_embedding(prompt)\n",
    "        \n",
    "        # Search cache\n",
    "        cached = self._search_cache(query_embedding)\n",
    "        \n",
    "        if cached:\n",
    "            # Cache hit\n",
    "            latency = time.time() - start_time\n",
    "            self.hits += 1\n",
    "            \n",
    "            # Estimate latency saved (typical LLM call = 2-4s)\n",
    "            latency_saved = 3.0 - latency\n",
    "            self.total_latency_saved += latency_saved\n",
    "            \n",
    "            metadata = {\n",
    "                \"cache_hit\": True,\n",
    "                \"latency\": latency,\n",
    "                \"similarity\": cached[\"similarity\"],\n",
    "                \"cached_at\": cached[\"timestamp\"],\n",
    "                \"latency_saved\": latency_saved\n",
    "            }\n",
    "            \n",
    "            return cached[\"response\"], metadata\n",
    "        \n",
    "        # Cache miss - simulate LLM call\n",
    "        self.misses += 1\n",
    "        time.sleep(0.2)  # Simulate LLM latency\n",
    "        \n",
    "        # Generate response\n",
    "        response = f\"Simulated response for: {prompt[:50]}...\"\n",
    "        \n",
    "        # Store in cache\n",
    "        cache_key = hashlib.md5(prompt.encode()).hexdigest()\n",
    "        self.cache[cache_key] = {\n",
    "            \"prompt\": prompt,\n",
    "            \"embedding\": query_embedding.tolist(),\n",
    "            \"response\": response,\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        metadata = {\n",
    "            \"cache_hit\": False,\n",
    "            \"latency\": latency,\n",
    "            \"similarity\": 0.0,\n",
    "            \"latency_saved\": 0.0\n",
    "        }\n",
    "        \n",
    "        return response, metadata\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        total_requests = self.hits + self.misses\n",
    "        hit_rate = (self.hits / total_requests * 100) if total_requests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"hits\": self.hits,\n",
    "            \"misses\": self.misses,\n",
    "            \"total_requests\": total_requests,\n",
    "            \"hit_rate_percent\": round(hit_rate, 2),\n",
    "            \"total_latency_saved_seconds\": round(self.total_latency_saved, 2),\n",
    "            \"avg_latency_saved_per_hit\": round(\n",
    "                self.total_latency_saved / self.hits if self.hits > 0 else 0, 3\n",
    "            ),\n",
    "            \"cache_size\": len(self.cache)\n",
    "        }\n",
    "\n",
    "# Test semantic cache\n",
    "cache = SimpleSemanticCache(similarity_threshold=0.95)\n",
    "\n",
    "print(\"Testing Semantic Cache\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First query - cache miss\n",
    "print(\"\\n1. First query (cache miss):\")\n",
    "prompt1 = \"What are BGP best practices?\"\n",
    "response1, meta1 = cache.get(prompt1)\n",
    "print(f\"   Prompt: {prompt1}\")\n",
    "print(f\"   Cache Hit: {meta1['cache_hit']}\")\n",
    "print(f\"   Latency: {meta1['latency']:.3f}s\")\n",
    "\n",
    "# Similar query - should hit cache\n",
    "print(\"\\n2. Similar query (should hit cache):\")\n",
    "prompt2 = \"What are BGP best practices?\"\n",
    "response2, meta2 = cache.get(prompt2)\n",
    "print(f\"   Prompt: {prompt2}\")\n",
    "print(f\"   Cache Hit: {meta2['cache_hit']}\")\n",
    "print(f\"   Latency: {meta2['latency']:.3f}s\")\n",
    "print(f\"   Similarity: {meta2.get('similarity', 0):.3f}\")\n",
    "print(f\"   Latency Saved: {meta2.get('latency_saved', 0):.3f}s\")\n",
    "\n",
    "# Different query - cache miss\n",
    "print(\"\\n3. Different query (cache miss):\")\n",
    "prompt3 = \"How to troubleshoot OSPF adjacencies?\"\n",
    "response3, meta3 = cache.get(prompt3)\n",
    "print(f\"   Prompt: {prompt3}\")\n",
    "print(f\"   Cache Hit: {meta3['cache_hit']}\")\n",
    "print(f\"   Latency: {meta3['latency']:.3f}s\")\n",
    "\n",
    "# Statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Cache Statistics:\")\n",
    "stats = cache.get_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Multi-Tier Cache Architecture\n",
    "\n",
    "Implement hot/warm/cold tiers with automatic promotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Optional\n",
    "import time\n",
    "\n",
    "class CacheTier(Enum):\n",
    "    \"\"\"Cache tiers with different TTLs\"\"\"\n",
    "    HOT = \"hot\"      # 5 minutes - active troubleshooting\n",
    "    WARM = \"warm\"    # 1 hour - common queries\n",
    "    COLD = \"cold\"    # 24 hours - reference data\n",
    "\n",
    "@dataclass\n",
    "class CacheEntry:\n",
    "    \"\"\"Cache entry with metadata\"\"\"\n",
    "    key: str\n",
    "    value: Any\n",
    "    tier: CacheTier\n",
    "    access_count: int = 0\n",
    "    created_at: float = 0.0\n",
    "    expires_at: float = 0.0\n",
    "\n",
    "class MultiTierCache:\n",
    "    \"\"\"\n",
    "    Multi-tier in-memory cache with automatic promotion.\n",
    "    \"\"\"\n",
    "    \n",
    "    TTL_MAP = {\n",
    "        CacheTier.HOT: 300,      # 5 minutes\n",
    "        CacheTier.WARM: 3600,    # 1 hour\n",
    "        CacheTier.COLD: 86400    # 24 hours\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache: Dict[str, CacheEntry] = {}\n",
    "    \n",
    "    def get(self, key: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get value and promote if frequently accessed\"\"\"\n",
    "        entry = self.cache.get(key)\n",
    "        \n",
    "        if not entry:\n",
    "            return None\n",
    "        \n",
    "        # Check if expired\n",
    "        if time.time() > entry.expires_at:\n",
    "            del self.cache[key]\n",
    "            return None\n",
    "        \n",
    "        # Increment access count\n",
    "        entry.access_count += 1\n",
    "        \n",
    "        # Check for promotion\n",
    "        if entry.tier == CacheTier.COLD and entry.access_count >= 5:\n",
    "            self._promote(entry, CacheTier.WARM)\n",
    "            print(f\"  → Promoted {key} to WARM tier\")\n",
    "        elif entry.tier == CacheTier.WARM and entry.access_count >= 10:\n",
    "            self._promote(entry, CacheTier.HOT)\n",
    "            print(f\"  → Promoted {key} to HOT tier\")\n",
    "        \n",
    "        return {\n",
    "            \"value\": entry.value,\n",
    "            \"tier\": entry.tier.value,\n",
    "            \"access_count\": entry.access_count,\n",
    "            \"ttl_remaining\": entry.expires_at - time.time()\n",
    "        }\n",
    "    \n",
    "    def set(self, key: str, value: Any, tier: CacheTier = CacheTier.WARM):\n",
    "        \"\"\"Store value in specified tier\"\"\"\n",
    "        ttl = self.TTL_MAP[tier]\n",
    "        now = time.time()\n",
    "        \n",
    "        entry = CacheEntry(\n",
    "            key=key,\n",
    "            value=value,\n",
    "            tier=tier,\n",
    "            access_count=0,\n",
    "            created_at=now,\n",
    "            expires_at=now + ttl\n",
    "        )\n",
    "        \n",
    "        self.cache[key] = entry\n",
    "    \n",
    "    def _promote(self, entry: CacheEntry, new_tier: CacheTier):\n",
    "        \"\"\"Promote entry to higher tier\"\"\"\n",
    "        entry.tier = new_tier\n",
    "        ttl = self.TTL_MAP[new_tier]\n",
    "        entry.expires_at = time.time() + ttl\n",
    "    \n",
    "    def get_tier_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics for each tier\"\"\"\n",
    "        stats = {tier.value: {\"count\": 0, \"total_accesses\": 0} for tier in CacheTier}\n",
    "        \n",
    "        for entry in self.cache.values():\n",
    "            tier_stats = stats[entry.tier.value]\n",
    "            tier_stats[\"count\"] += 1\n",
    "            tier_stats[\"total_accesses\"] += entry.access_count\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Test multi-tier cache\n",
    "cache = MultiTierCache()\n",
    "\n",
    "print(\"Testing Multi-Tier Cache\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store entry in warm tier\n",
    "print(\"\\n1. Storing entry in WARM tier:\")\n",
    "cache.set(\"bgp_best_practices\", \"BGP best practices content...\", CacheTier.WARM)\n",
    "print(\"   Stored: bgp_best_practices\")\n",
    "\n",
    "# Access multiple times to trigger promotion\n",
    "print(\"\\n2. Accessing entry multiple times:\")\n",
    "for i in range(12):\n",
    "    result = cache.get(\"bgp_best_practices\")\n",
    "    if result:\n",
    "        print(f\"   Access {i+1}: tier={result['tier']}, count={result['access_count']}\")\n",
    "\n",
    "# Show tier statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Tier Statistics:\")\n",
    "stats = cache.get_tier_stats()\n",
    "for tier, tier_stats in stats.items():\n",
    "    if tier_stats['count'] > 0:\n",
    "        avg_accesses = tier_stats['total_accesses'] / tier_stats['count']\n",
    "        print(f\"  {tier.upper()}: {tier_stats['count']} entries, \"\n",
    "              f\"avg {avg_accesses:.1f} accesses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Cache Key Design with Normalization\n",
    "\n",
    "Design cache keys to maximize hit rate through parameter normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "class CacheKeyGenerator:\n",
    "    \"\"\"\n",
    "    Generate optimized cache keys for network queries.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, version: str = \"v1\"):\n",
    "        self.version = version\n",
    "    \n",
    "    def generate_key(\n",
    "        self,\n",
    "        query_type: str,\n",
    "        device_id: str,\n",
    "        parameters: Dict[str, Any],\n",
    "        time_sensitivity: str = \"static\"\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate cache key with normalization.\n",
    "        \n",
    "        Args:\n",
    "            query_type: Type of query (e.g., 'interface_status')\n",
    "            device_id: Device identifier\n",
    "            parameters: Query parameters\n",
    "            time_sensitivity: 'real-time', 'near-real-time', or 'static'\n",
    "        \"\"\"\n",
    "        # Normalize parameters\n",
    "        normalized_params = self._normalize_parameters(parameters)\n",
    "        \n",
    "        # Generate parameter hash\n",
    "        param_hash = self._hash_parameters(normalized_params)\n",
    "        \n",
    "        # Generate time bucket\n",
    "        time_bucket = self._get_time_bucket(time_sensitivity)\n",
    "        \n",
    "        # Build key\n",
    "        key = f\"{self.version}:{query_type}:{device_id}:{param_hash}:{time_bucket}\"\n",
    "        \n",
    "        return key\n",
    "    \n",
    "    def _normalize_parameters(self, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Normalize parameters to maximize cache hits\"\"\"\n",
    "        normalized = {}\n",
    "        \n",
    "        for key in sorted(params.keys()):\n",
    "            value = params[key]\n",
    "            \n",
    "            if isinstance(value, str):\n",
    "                normalized[key] = value.lower().strip()\n",
    "            elif isinstance(value, (int, float)):\n",
    "                normalized[key] = round(value, 2) if isinstance(value, float) else value\n",
    "            elif isinstance(value, list):\n",
    "                normalized[key] = sorted(value)\n",
    "            else:\n",
    "                normalized[key] = value\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def _hash_parameters(self, params: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate short hash from parameters\"\"\"\n",
    "        param_str = json.dumps(params, sort_keys=True)\n",
    "        hash_obj = hashlib.md5(param_str.encode())\n",
    "        return hash_obj.hexdigest()[:8]\n",
    "    \n",
    "    def _get_time_bucket(self, sensitivity: str) -> str:\n",
    "        \"\"\"Generate time bucket based on sensitivity\"\"\"\n",
    "        now = datetime.now()\n",
    "        \n",
    "        if sensitivity == \"real-time\":\n",
    "            return now.strftime(\"%Y-%m-%d-%H-%M\")  # Minute-level\n",
    "        elif sensitivity == \"near-real-time\":\n",
    "            return now.strftime(\"%Y-%m-%d-%H\")     # Hour-level\n",
    "        else:  # static\n",
    "            return \"static\"\n",
    "\n",
    "# Test cache key generation\n",
    "key_gen = CacheKeyGenerator(version=\"v1\")\n",
    "\n",
    "print(\"Testing Cache Key Generation\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example 1: Real-time interface status\n",
    "print(\"\\n1. Real-time interface status:\")\n",
    "key1 = key_gen.generate_key(\n",
    "    query_type=\"interface_status\",\n",
    "    device_id=\"rtr-001\",\n",
    "    parameters={\n",
    "        \"interfaces\": [\"GigabitEthernet0/0\", \"GigabitEthernet0/1\"],\n",
    "        \"include_stats\": True\n",
    "    },\n",
    "    time_sensitivity=\"real-time\"\n",
    ")\n",
    "print(f\"   Key: {key1}\")\n",
    "\n",
    "# Example 2: Near-real-time BGP summary\n",
    "print(\"\\n2. Near-real-time BGP summary:\")\n",
    "key2 = key_gen.generate_key(\n",
    "    query_type=\"bgp_summary\",\n",
    "    device_id=\"site-nyc\",\n",
    "    parameters={\n",
    "        \"peer_type\": \"ebgp\",\n",
    "        \"state\": \"established\"\n",
    "    },\n",
    "    time_sensitivity=\"near-real-time\"\n",
    ")\n",
    "print(f\"   Key: {key2}\")\n",
    "\n",
    "# Example 3: Static reference query\n",
    "print(\"\\n3. Static reference query:\")\n",
    "key3 = key_gen.generate_key(\n",
    "    query_type=\"best_practices\",\n",
    "    device_id=\"none\",\n",
    "    parameters={\n",
    "        \"topic\": \"OSPF Design\",\n",
    "        \"protocol\": \"ospf\"\n",
    "    },\n",
    "    time_sensitivity=\"static\"\n",
    ")\n",
    "print(f\"   Key: {key3}\")\n",
    "\n",
    "# Test parameter normalization\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Parameter Normalization Test:\")\n",
    "\n",
    "# These should generate the SAME key\n",
    "params_a = {\"Interface\": \"Gi0/0\", \"Status\": \"UP\"}\n",
    "params_b = {\"status\": \"up\", \"interface\": \"gi0/0\"}  # Different order/case\n",
    "\n",
    "key_a = key_gen.generate_key(\"query\", \"rtr-001\", params_a, \"static\")\n",
    "key_b = key_gen.generate_key(\"query\", \"rtr-001\", params_b, \"static\")\n",
    "\n",
    "print(f\"\\nKey A (original): {key_a}\")\n",
    "print(f\"Key B (normalized): {key_b}\")\n",
    "print(f\"\\nKeys match: {key_a == key_b} ✓\" if key_a == key_b else \"Keys don't match ✗\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Cache Performance Metrics\n",
    "\n",
    "Measure cache effectiveness with comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List\n",
    "import time\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class CacheMetrics:\n",
    "    \"\"\"Cache performance metrics\"\"\"\n",
    "    total_requests: int\n",
    "    cache_hits: int\n",
    "    cache_misses: int\n",
    "    hit_rate_percent: float\n",
    "    avg_latency_hit_ms: float\n",
    "    avg_latency_miss_ms: float\n",
    "    total_latency_saved_seconds: float\n",
    "    estimated_cost_saved_dollars: float\n",
    "\n",
    "class CacheMonitor:\n",
    "    \"\"\"\n",
    "    Monitor cache performance and calculate savings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        cost_per_llm_call: float = 0.10,\n",
    "        cost_per_cache_hit: float = 0.0001\n",
    "    ):\n",
    "        self.cost_per_llm_call = cost_per_llm_call\n",
    "        self.cost_per_cache_hit = cost_per_cache_hit\n",
    "        \n",
    "        # Tracking\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.hit_latencies = []\n",
    "        self.miss_latencies = []\n",
    "        self.latency_saved = 0.0\n",
    "    \n",
    "    def record_hit(self, latency_ms: float, latency_saved_ms: float):\n",
    "        \"\"\"Record cache hit\"\"\"\n",
    "        self.hits += 1\n",
    "        self.hit_latencies.append(latency_ms)\n",
    "        self.latency_saved += latency_saved_ms / 1000\n",
    "    \n",
    "    def record_miss(self, latency_ms: float):\n",
    "        \"\"\"Record cache miss\"\"\"\n",
    "        self.misses += 1\n",
    "        self.miss_latencies.append(latency_ms)\n",
    "    \n",
    "    def get_metrics(self) -> CacheMetrics:\n",
    "        \"\"\"Get current cache metrics\"\"\"\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = (self.hits / total * 100) if total > 0 else 0\n",
    "        \n",
    "        avg_hit_latency = sum(self.hit_latencies) / len(self.hit_latencies) if self.hit_latencies else 0\n",
    "        avg_miss_latency = sum(self.miss_latencies) / len(self.miss_latencies) if self.miss_latencies else 0\n",
    "        \n",
    "        # Calculate cost savings\n",
    "        cost_saved = self.hits * (self.cost_per_llm_call - self.cost_per_cache_hit)\n",
    "        \n",
    "        return CacheMetrics(\n",
    "            total_requests=total,\n",
    "            cache_hits=self.hits,\n",
    "            cache_misses=self.misses,\n",
    "            hit_rate_percent=round(hit_rate, 2),\n",
    "            avg_latency_hit_ms=round(avg_hit_latency, 2),\n",
    "            avg_latency_miss_ms=round(avg_miss_latency, 2),\n",
    "            total_latency_saved_seconds=round(self.latency_saved, 2),\n",
    "            estimated_cost_saved_dollars=round(cost_saved, 2)\n",
    "        )\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"Generate human-readable report\"\"\"\n",
    "        metrics = self.get_metrics()\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"=\" * 60)\n",
    "        report.append(\"CACHE PERFORMANCE REPORT\")\n",
    "        report.append(\"=\" * 60)\n",
    "        report.append(\"\")\n",
    "        report.append(\"Overall Metrics:\")\n",
    "        report.append(f\"  Total Requests: {metrics.total_requests:,}\")\n",
    "        report.append(f\"  Cache Hits: {metrics.cache_hits:,}\")\n",
    "        report.append(f\"  Cache Misses: {metrics.cache_misses:,}\")\n",
    "        report.append(f\"  Hit Rate: {metrics.hit_rate_percent}%\")\n",
    "        report.append(\"\")\n",
    "        report.append(\"Performance:\")\n",
    "        report.append(f\"  Avg Hit Latency: {metrics.avg_latency_hit_ms}ms\")\n",
    "        report.append(f\"  Avg Miss Latency: {metrics.avg_latency_miss_ms}ms\")\n",
    "        report.append(f\"  Latency Saved: {metrics.total_latency_saved_seconds}s\")\n",
    "        report.append(\"\")\n",
    "        report.append(\"Cost Savings:\")\n",
    "        report.append(f\"  Estimated Savings: ${metrics.estimated_cost_saved_dollars}\")\n",
    "        report.append(f\"  Cost per LLM call: ${self.cost_per_llm_call}\")\n",
    "        report.append(f\"  Cost per cache hit: ${self.cost_per_cache_hit}\")\n",
    "        report.append(\"\")\n",
    "        report.append(\"=\" * 60)\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# Simulate cache traffic\n",
    "monitor = CacheMonitor(cost_per_llm_call=0.10, cost_per_cache_hit=0.0001)\n",
    "\n",
    "print(\"Simulating cache traffic...\\n\")\n",
    "\n",
    "# Simulate 100 requests with 70% hit rate\n",
    "for i in range(100):\n",
    "    if random.random() < 0.70:  # 70% cache hit\n",
    "        monitor.record_hit(\n",
    "            latency_ms=random.uniform(20, 40),\n",
    "            latency_saved_ms=random.uniform(2900, 3100)\n",
    "        )\n",
    "    else:  # 30% cache miss\n",
    "        monitor.record_miss(\n",
    "            latency_ms=random.uniform(2800, 3200)\n",
    "        )\n",
    "\n",
    "# Generate report\n",
    "print(monitor.generate_report())\n",
    "\n",
    "# Calculate monthly projections\n",
    "metrics = monitor.get_metrics()\n",
    "monthly_requests = 100000\n",
    "projected_hit_rate = metrics.hit_rate_percent / 100\n",
    "\n",
    "cost_without_cache = monthly_requests * 0.10\n",
    "cost_with_cache = (\n",
    "    (monthly_requests * projected_hit_rate * 0.0001) +\n",
    "    (monthly_requests * (1 - projected_hit_rate) * 0.10)\n",
    ")\n",
    "monthly_savings = cost_without_cache - cost_with_cache\n",
    "\n",
    "print(\"\\nProjected Monthly Savings (100k requests):\")\n",
    "print(f\"  Without cache: ${cost_without_cache:,.2f}\")\n",
    "print(f\"  With cache: ${cost_with_cache:,.2f}\")\n",
    "print(f\"  Monthly savings: ${monthly_savings:,.2f}\")\n",
    "print(f\"  Annual savings: ${monthly_savings * 12:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Full code: [Chapter 40 on GitHub](https://github.com/eduardd76/AI_for_networking_and_security_engineers/tree/main/CODE/Volume-3-Production-Systems/Chapter-40-Caching-Strategies)\n",
    "- Learn more: [vExpertAI.com](https://vexpertai.com)\n",
    "- Author: Eduard Dulharu ([@eduardd76](https://github.com/eduardd76))\n",
    "\n",
    "**Production Implementation:**\n",
    "- Deploy Redis for distributed caching\n",
    "- Use pgvector for semantic similarity search\n",
    "- Implement cache warming strategies\n",
    "- Monitor hit rates and cost savings\n",
    "- Configure TTLs based on data volatility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
