{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Chapter 35: Vector Database Optimization & Log Analysis\n",
    "\n",
    "Run this notebook directly in Google Colab - no local Python needed!\n",
    "\n",
    "**Full code**: [GitHub](https://github.com/eduardd76/AI_for_networking_and_security_engineers/tree/main/CODE/Volume-3-Production-Systems/Chapter-35-Vector-Databases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install dependencies and configure API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q chromadb sentence-transformers anthropic python-dotenv\n",
    "\n",
    "# Import and configure API key\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Check for Colab secrets first\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
    "    print('✓ Using API keys from Colab secrets')\n",
    "except:\n",
    "    # Fall back to manual entry\n",
    "    if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "        os.environ['ANTHROPIC_API_KEY'] = getpass('Enter ANTHROPIC_API_KEY: ')\n",
    "    print('✓ API keys configured')\n",
    "\n",
    "print('\\n✅ Setup complete! Ready to run examples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Example 1: Embedding Model Comparison for Network Logs\n",
    "\n",
    "Test different embedding models to find the best balance of speed and accuracy for network logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Test different embedding models\n",
    "models = {\n",
    "    'all-MiniLM-L6-v2': SentenceTransformer('all-MiniLM-L6-v2'),  # 384 dims, fast\n",
    "    'all-mpnet-base-v2': SentenceTransformer('all-mpnet-base-v2'),  # 768 dims, accurate\n",
    "    'paraphrase-MiniLM-L3-v2': SentenceTransformer('paraphrase-MiniLM-L3-v2')  # 384 dims, very fast\n",
    "}\n",
    "\n",
    "# Sample network logs\n",
    "logs = [\n",
    "    \"BGP peer 10.1.1.1 down - connection timeout\",\n",
    "    \"Interface GigabitEthernet0/1 changed state to down\",\n",
    "    \"OSPF neighbor 10.2.2.2 state changed from FULL to DOWN\",\n",
    "    \"Authentication failed for user admin from 192.168.1.50\",\n",
    "    \"High CPU utilization detected: 95% for 5 minutes\"\n",
    "]\n",
    "\n",
    "# Test embedding speed and dimensions\n",
    "print(\"Embedding Model Performance for Network Logs:\\n\")\n",
    "for name, model in models.items():\n",
    "    start = time.time()\n",
    "    embeddings = model.encode(logs)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Dimensions: {embeddings.shape[1]}\")\n",
    "    print(f\"  Time for {len(logs)} logs: {elapsed*1000:.2f}ms\")\n",
    "    print(f\"  Per-log: {elapsed*1000/len(logs):.2f}ms\")\n",
    "    print(f\"  Throughput: {len(logs)/elapsed:.1f} logs/sec\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Example 2: ChromaDB Setup with Network Logs\n",
    "\n",
    "Build a vector database for semantic search across network security logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid\n",
    "\n",
    "# Initialize ChromaDB with persistent storage\n",
    "client = chromadb.Client(Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    is_persistent=False  # In-memory for Colab\n",
    "))\n",
    "\n",
    "# Create collection with custom embedding function\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"network_security_logs\",\n",
    "    metadata={\n",
    "        \"description\": \"Network and security logs with semantic search\",\n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sample network logs with metadata\n",
    "logs_data = [\n",
    "    {\n",
    "        \"log\": \"authentication failed for user admin from 192.168.1.50 after 3 attempts\",\n",
    "        \"metadata\": {\"severity\": 3, \"category\": \"security\", \"device\": \"firewall1\", \"timestamp\": 1705320000}\n",
    "    },\n",
    "    {\n",
    "        \"log\": \"bgp peer 10.1.1.1 down due to hold timer expired\",\n",
    "        \"metadata\": {\"severity\": 2, \"category\": \"routing\", \"device\": \"router1\", \"timestamp\": 1705320120}\n",
    "    },\n",
    "    {\n",
    "        \"log\": \"interface gigabitethernet0/1 excessive input errors detected\",\n",
    "        \"metadata\": {\"severity\": 4, \"category\": \"interface\", \"device\": \"switch1\", \"timestamp\": 1705320240}\n",
    "    },\n",
    "    {\n",
    "        \"log\": \"failed login attempt rejected invalid password from remote host 192.168.1.50\",\n",
    "        \"metadata\": {\"severity\": 3, \"category\": \"security\", \"device\": \"firewall1\", \"timestamp\": 1705320360}\n",
    "    },\n",
    "    {\n",
    "        \"log\": \"ospf neighbor 10.2.2.2 state change full to down adjacency lost\",\n",
    "        \"metadata\": {\"severity\": 2, \"category\": \"routing\", \"device\": \"router2\", \"timestamp\": 1705320480}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Embed and add to collection\n",
    "log_texts = [item[\"log\"] for item in logs_data]\n",
    "embeddings = embedding_model.encode(log_texts).tolist()\n",
    "\n",
    "collection.add(\n",
    "    embeddings=embeddings,\n",
    "    documents=log_texts,\n",
    "    metadatas=[item[\"metadata\"] for item in logs_data],\n",
    "    ids=[str(uuid.uuid4()) for _ in logs_data]\n",
    ")\n",
    "\n",
    "print(f\"Added {len(logs_data)} logs to ChromaDB\")\n",
    "print(f\"Collection size: {collection.count()} documents\\n\")\n",
    "\n",
    "# Query for similar security events\n",
    "query_text = \"access denied wrong credentials from 192.168.1.50\"\n",
    "query_embedding = embedding_model.encode([query_text]).tolist()\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding,\n",
    "    n_results=3,\n",
    "    where={\"category\": \"security\"}  # Filter to security logs only\n",
    ")\n",
    "\n",
    "print(f\"Query: '{query_text}'\")\n",
    "print(\"\\nTop 3 similar security events:\")\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    ")):\n",
    "    print(f\"\\n{i+1}. Distance: {distance:.4f}\")\n",
    "    print(f\"   Log: {doc}\")\n",
    "    print(f\"   Device: {metadata['device']}, Severity: {metadata['severity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Example 3: Batch Processing Large Log Volumes\n",
    "\n",
    "Efficiently ingest millions of network logs with batching and progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import uuid\n",
    "\n",
    "class NetworkLogIngestor:\n",
    "    \"\"\"Efficiently ingests millions of network logs into ChromaDB.\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str, batch_size: int = 1000):\n",
    "        self.batch_size = batch_size\n",
    "        self.client = chromadb.Client(Settings(is_persistent=False))\n",
    "        self.collection = self.client.get_or_create_collection(name=collection_name)\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.stats = {\n",
    "            'total_processed': 0,\n",
    "            'total_time': 0,\n",
    "            'batch_times': []\n",
    "        }\n",
    "\n",
    "    def ingest_batch(self, logs: List[Dict]) -> Dict:\n",
    "        \"\"\"Ingest a batch of logs with timing metrics.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Extract texts for embedding\n",
    "        texts = [log['text'] for log in logs]\n",
    "\n",
    "        # Generate embeddings\n",
    "        embed_start = time.time()\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            texts,\n",
    "            batch_size=32,\n",
    "            show_progress_bar=False\n",
    "        ).tolist()\n",
    "        embed_time = time.time() - embed_start\n",
    "\n",
    "        # Add to ChromaDB\n",
    "        db_start = time.time()\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings,\n",
    "            documents=texts,\n",
    "            metadatas=[log['metadata'] for log in logs],\n",
    "            ids=[str(uuid.uuid4()) for _ in logs]\n",
    "        )\n",
    "        db_time = time.time() - db_start\n",
    "\n",
    "        batch_time = time.time() - start_time\n",
    "\n",
    "        return {\n",
    "            'batch_size': len(logs),\n",
    "            'total_time': batch_time,\n",
    "            'embedding_time': embed_time,\n",
    "            'db_time': db_time,\n",
    "            'throughput': len(logs) / batch_time\n",
    "        }\n",
    "\n",
    "# Simulate large log stream\n",
    "def generate_sample_logs(count: int):\n",
    "    \"\"\"Generate sample network logs for testing.\"\"\"\n",
    "    log_templates = [\n",
    "        \"interface {iface} changed state to {state}\",\n",
    "        \"bgp peer {ip} connection {status}\",\n",
    "        \"authentication {result} for user {user} from {ip}\",\n",
    "        \"cpu utilization {percent}% threshold exceeded\"\n",
    "    ]\n",
    "\n",
    "    import random\n",
    "\n",
    "    for i in range(count):\n",
    "        template = random.choice(log_templates)\n",
    "        log_text = template.format(\n",
    "            iface=f\"GigabitEthernet0/{random.randint(1,48)}\",\n",
    "            state=random.choice(['up', 'down']),\n",
    "            ip=f\"10.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}\",\n",
    "            status=random.choice(['established', 'timeout', 'reset']),\n",
    "            result=random.choice(['succeeded', 'failed', 'rejected']),\n",
    "            user=random.choice(['admin', 'operator', 'guest']),\n",
    "            percent=random.randint(50, 99)\n",
    "        )\n",
    "\n",
    "        yield {\n",
    "            'text': log_text,\n",
    "            'metadata': {\n",
    "                'severity': random.randint(1, 7),\n",
    "                'timestamp': 1705320000 + i,\n",
    "                'device': f\"device{random.randint(1,50)}\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Test ingestion with 5,000 logs (reduced for Colab)\n",
    "print(\"Testing batch ingestion with 5,000 logs...\\n\")\n",
    "\n",
    "ingestor = NetworkLogIngestor(\n",
    "    collection_name=\"large_log_collection\",\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "# Process in batches\n",
    "batch = []\n",
    "for log in generate_sample_logs(5000):\n",
    "    batch.append(log)\n",
    "    if len(batch) >= 1000:\n",
    "        stats = ingestor.ingest_batch(batch)\n",
    "        ingestor.stats['total_processed'] += stats['batch_size']\n",
    "        ingestor.stats['total_time'] += stats['total_time']\n",
    "        \n",
    "        print(f\"Processed {ingestor.stats['total_processed']} logs | \"\n",
    "              f\"Batch throughput: {stats['throughput']:.1f} logs/sec | \"\n",
    "              f\"Embedding: {stats['embedding_time']:.2f}s | \"\n",
    "              f\"DB: {stats['db_time']:.2f}s\")\n",
    "        batch = []\n",
    "\n",
    "# Process remaining\n",
    "if batch:\n",
    "    stats = ingestor.ingest_batch(batch)\n",
    "    ingestor.stats['total_processed'] += stats['batch_size']\n",
    "\n",
    "print(f\"\\n=== Ingestion Complete ===\")\n",
    "print(f\"Total logs: {ingestor.stats['total_processed']}\")\n",
    "print(f\"Total time: {ingestor.stats['total_time']:.2f}s\")\n",
    "print(f\"Average throughput: {ingestor.stats['total_processed']/ingestor.stats['total_time']:.1f} logs/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Example 4: Security Event Correlation\n",
    "\n",
    "Use semantic search to find related security events and reconstruct attack chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime\n",
    "\n",
    "class SecurityEventCorrelator:\n",
    "    \"\"\"Semantic search for security event correlation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = chromadb.Client(Settings(is_persistent=False))\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=\"security_events\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def ingest_event(self, log_text: str, metadata: dict):\n",
    "        \"\"\"Add a single security event.\"\"\"\n",
    "        embedding = self.model.encode([log_text]).tolist()\n",
    "        self.collection.add(\n",
    "            embeddings=embedding,\n",
    "            documents=[log_text],\n",
    "            metadatas=[metadata],\n",
    "            ids=[f\"event_{metadata['timestamp']}_{hash(log_text) % 1000000}\"]\n",
    "        )\n",
    "\n",
    "    def find_related_events(self, incident_description: str, min_severity: int = 3, top_k: int = 10):\n",
    "        \"\"\"Find security events related to an incident.\"\"\"\n",
    "        query_embedding = self.model.encode([incident_description]).tolist()\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding,\n",
    "            n_results=top_k,\n",
    "            where={\"severity\": {\"$lte\": min_severity}}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "# Example usage\n",
    "correlator = SecurityEventCorrelator()\n",
    "\n",
    "# Ingest sample security events\n",
    "sample_events = [\n",
    "    {\n",
    "        \"log\": \"failed ssh login attempt from 203.0.113.45 username admin\",\n",
    "        \"metadata\": {\"severity\": 3, \"device\": \"firewall1\", \"timestamp\": 1705320000, \"event_type\": \"auth_failure\"}\n",
    "    },\n",
    "    {\n",
    "        \"log\": \"port scan detected from 203.0.113.45 targeting ports 22,23,80,443\",\n",
    "        \"metadata\": {\"severity\": 2, \"device\": \"ids1\", \"timestamp\": 1705320120, \"event_type\": \"scan\"}\n",
    "    },\n",
    "    {\n",
    "        \"log\": \"multiple authentication failures from 203.0.113.45 threshold exceeded\",\n",
    "        \"metadata\": {\"severity\": 2, \"device\": \"firewall1\", \"timestamp\": 1705320240, \"event_type\": \"brute_force\"}\n",
    "    },\n",
    "    {\n",
    "        \"log\": \"successful ssh login from 203.0.113.45 username admin after failed attempts\",\n",
    "        \"metadata\": {\"severity\": 1, \"device\": \"server1\", \"timestamp\": 1705320360, \"event_type\": \"auth_success\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Ingesting security events...\")\n",
    "for event in sample_events:\n",
    "    correlator.ingest_event(event['log'], event['metadata'])\n",
    "\n",
    "print(f\"Total events: {correlator.collection.count()}\\n\")\n",
    "\n",
    "# Investigate a reported compromise\n",
    "print(\"=== Investigating Suspected Server Compromise ===\\n\")\n",
    "incident = \"server was compromised after successful authentication from suspicious IP\"\n",
    "\n",
    "related = correlator.find_related_events(\n",
    "    incident_description=incident,\n",
    "    min_severity=3,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(f\"Query: '{incident}'\\n\")\n",
    "print(\"Related events (ordered by relevance):\\n\")\n",
    "\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    related['documents'][0],\n",
    "    related['metadatas'][0],\n",
    "    related['distances'][0]\n",
    ")):\n",
    "    relevance = (1 - distance) * 100\n",
    "    timestamp = datetime.fromtimestamp(metadata['timestamp'])\n",
    "\n",
    "    print(f\"{i+1}. [{timestamp.strftime('%H:%M:%S')}] Relevance: {relevance:.1f}%\")\n",
    "    print(f\"   Event: {doc}\")\n",
    "    print(f\"   Device: {metadata['device']}, Type: {metadata['event_type']}, Severity: {metadata['severity']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Interactive Section\n",
    "\n",
    "Try your own vector database experiments here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Experiment with different queries and log patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Full code: [Chapter 35 on GitHub](https://github.com/eduardd76/AI_for_networking_and_security_engineers/tree/main/CODE/Volume-3-Production-Systems/Chapter-35-Vector-Databases)\n",
    "- Learn more: [vExpertAI.com](https://vexpertai.com)\n",
    "- Author: Eduard Dulharu ([@eduardd76](https://github.com/eduardd76))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
