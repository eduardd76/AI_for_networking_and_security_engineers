{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Volume 1, Chapter 4: API Basics\n",
    "\n",
    "**Simple, practical examples of using Claude API with LangChain**\n",
    "\n",
    "From: AI for Networking Engineers - Volume 1, Chapter 4\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eduardd76/AI_for_networking_and_security_engineers/blob/main/CODE/Colab-Notebooks/Vol1_Ch4_API_Basics.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## ðŸ”§ Setup (Run These First)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-anthropic anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "api_key"
   },
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "# Option 1: Use Colab Secrets (recommended)\n",
    "# Go to the key icon on the left, add \"ANTHROPIC_API_KEY\"\n",
    "\n",
    "# Option 2: Enter directly (less secure)\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "    os.environ['ANTHROPIC_API_KEY'] = getpass('Enter your Anthropic API key: ')\n",
    "\n",
    "print(\"âœ“ API key set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examples"
   },
   "source": [
    "## ðŸ“– Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Simple API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example1"
   },
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Create LLM instance\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Make a simple call\n",
    "question = \"What is BGP in one sentence?\"\n",
    "response = llm.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: API Call with Network Config Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example2"
   },
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "router bgp 65001\n",
    " neighbor 203.0.113.1 remote-as 174\n",
    " neighbor 203.0.113.1 shutdown\n",
    " network 10.0.0.0 mask 255.0.0.0\n",
    "\"\"\"\n",
    "\n",
    "question = f\"\"\"Here's a BGP configuration:\n",
    "\n",
    "{config}\n",
    "\n",
    "Is there a problem with this config?\"\"\"\n",
    "\n",
    "response = llm.invoke(question)\n",
    "\n",
    "print(\"Config:\")\n",
    "print(config)\n",
    "print(\"\\nAnalysis:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example3"
   },
   "outputs": [],
   "source": [
    "question = \"Explain the 7 layers of the OSI model in simple terms.\"\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"Answer (streaming):\")\n",
    "\n",
    "# Stream the response\n",
    "for chunk in llm.stream(question):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Temperature Settings (Creativity Control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example4"
   },
   "outputs": [],
   "source": [
    "question = \"Suggest a creative name for a network automation tool\"\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "# Temperature 0 (deterministic)\n",
    "llm_deterministic = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    temperature=0\n",
    ")\n",
    "response1 = llm_deterministic.invoke(question)\n",
    "print(f\"Temperature 0 (deterministic):\\n{response1.content}\\n\")\n",
    "\n",
    "# Temperature 1 (creative)\n",
    "llm_creative = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    temperature=1\n",
    ")\n",
    "response2 = llm_creative.invoke(question)\n",
    "print(f\"Temperature 1 (creative):\\n{response2.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Model Comparison (Haiku vs Sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "question = \"Explain the difference between TCP and UDP in networking.\"\n",
    "\n",
    "models = [\n",
    "    (\"claude-3-5-haiku-20241022\", \"Haiku (fast, cheap)\"),\n",
    "    (\"claude-3-5-sonnet-20241022\", \"Sonnet (balanced)\")\n",
    "]\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "for model_id, model_name in models:\n",
    "    llm = ChatAnthropic(model=model_id, temperature=0)\n",
    "    \n",
    "    start = time.time()\n",
    "    response = llm.invoke(question)\n",
    "    duration = time.time() - start\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Time: {duration:.2f}s\")\n",
    "    print(f\"  Answer: {response.content[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "1. **LangChain makes API calls simple** - Just a few lines of code\n",
    "2. **Always pass context** - Include configs, logs, or outputs\n",
    "3. **Use streaming** - For long responses\n",
    "4. **Temperature controls creativity** - 0 = deterministic, 1 = creative\n",
    "5. **Choose models wisely** - Haiku for simple tasks, Sonnet for complex ones\n",
    "\n",
    "## ðŸ“š Next Steps\n",
    "\n",
    "- Try other notebooks: Prompt Engineering, Structured Outputs\n",
    "- Modify examples with your own network data\n",
    "- Explore Volume 2 for RAG and agents"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
