{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Volume 2, Chapter 14: RAG Fundamentals\n",
    "\n",
    "**Build a simple RAG system for network documentation**\n",
    "\n",
    "From: AI for Networking Engineers - Volume 2, Chapter 14\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eduardd76/AI_for_networking_and_security_engineers/blob/main/CODE/Colab-Notebooks/Vol2_Ch14_RAG_Basics.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-anthropic langchain-openai chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API keys\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "    os.environ['ANTHROPIC_API_KEY'] = getpass('Enter Anthropic API key: ')\n",
    "\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass('Enter OpenAI API key (for embeddings): ')\n",
    "\n",
    "print(\"âœ“ API keys set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“– RAG Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Sample Network Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample network documentation\n",
    "network_docs = [\n",
    "    \"\"\"\n",
    "    VLAN Configuration Standards\n",
    "    \n",
    "    VLAN Ranges:\n",
    "    - VLAN 1-99: Infrastructure\n",
    "    - VLAN 100-199: User networks\n",
    "    - VLAN 200-299: Voice\n",
    "    - VLAN 999: Quarantine\n",
    "    \n",
    "    Best Practices:\n",
    "    - Never use VLAN 1 for production traffic\n",
    "    - Document all VLANs in IPAM\n",
    "    - Use meaningful VLAN names\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    BGP Configuration Standards\n",
    "    \n",
    "    AS Numbers:\n",
    "    - Primary AS: 65001\n",
    "    - Backup AS: 65002\n",
    "    \n",
    "    Peering Requirements:\n",
    "    - MD5 authentication required\n",
    "    - Prefix-list filtering mandatory\n",
    "    - Maximum prefix limits: 1000 for peers\n",
    "    - BFD for fast convergence\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    OSPF Design Guidelines\n",
    "    \n",
    "    Area Design:\n",
    "    - Area 0: Backbone (core switches/routers)\n",
    "    - Area 1: Data centers\n",
    "    - Area 2: Branch offices\n",
    "    \n",
    "    Best Practices:\n",
    "    - Limit areas to 50-100 routers\n",
    "    - Use stub areas at network edges\n",
    "    - Reference bandwidth: 100000 (for 100G links)\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Created {len(network_docs)} sample documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "print(\"Step 1: Creating embeddings...\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "print(\"Step 2: Splitting documents into chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "for doc in network_docs:\n",
    "    chunks.extend(text_splitter.split_text(doc))\n",
    "\n",
    "print(f\"  â†’ Split into {len(chunks)} chunks\")\n",
    "\n",
    "print(\"Step 3: Creating vector database...\")\n",
    "vectorstore = Chroma.from_texts(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    persist_directory=\"./colab_chroma_db\"\n",
    ")\n",
    "\n",
    "print(\"Step 4: Setting up LLM...\")\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"Step 5: Creating RAG chain...\")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ RAG system ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Ask Questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different questions\n",
    "questions = [\n",
    "    \"What VLAN should I use for voice traffic?\",\n",
    "    \"What is our primary BGP AS number?\",\n",
    "    \"Which OSPF area should data centers be in?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    result = qa_chain({\"query\": question})\n",
    "    \n",
    "    print(f\"\\nAnswer:\\n{result['result']}\")\n",
    "    \n",
    "    print(f\"\\nSources used:\")\n",
    "    for j, doc in enumerate(result['source_documents'], 1):\n",
    "        print(f\"  {j}. {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Your Own Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask your own question\n",
    "my_question = input(\"Ask a question about network standards: \")\n",
    "\n",
    "result = qa_chain({\"query\": my_question})\n",
    "\n",
    "print(f\"\\nAnswer:\\n{result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ How RAG Works\n",
    "\n",
    "1. **Index** - Documents are split into chunks and converted to embeddings\n",
    "2. **Store** - Embeddings are stored in a vector database (ChromaDB)\n",
    "3. **Retrieve** - When you ask a question, it finds relevant chunks\n",
    "4. **Generate** - Claude uses the retrieved chunks to answer your question\n",
    "\n",
    "## ðŸ’¡ Key Benefits\n",
    "\n",
    "- Works with your own documentation\n",
    "- More accurate than asking Claude directly\n",
    "- Shows sources (traceability)\n",
    "- Can handle large knowledge bases\n",
    "\n",
    "## ðŸ“š Next Steps\n",
    "\n",
    "- Add your own network documentation\n",
    "- Try other RAG chapters (advanced retrieval, semantic search)\n",
    "- Build a production RAG system"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
