{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search for Network Documentation\n",
    "Explore how semantic search works and apply it to network documentation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install langchain chromadb -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Vectorstore and LLM\n",
    "Use pre-trained models for embeddings."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "vectorstore = Chroma(persist_directory='./chroma_db', embedding_function=embeddings)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Multi-Query Retrieval\n",
    "Generate multiple query variations for improved document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class MultiQueryRetriever:\n",
    "    \"\"\"Generate multiple query variations for better retrieval.\"\"\"\n",
    "\n",
    "    def __init__(self, vectorstore, llm):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        \n",
    "        self.query_generation_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant helping generate search queries.\n",
    "        Given a user question about network engineering, generate 3 alternative\n",
    "        versions of the question to retrieve relevant documents.\n",
    "        \n",
    "        Original question: {question}\n",
    "        Generate 3 alternative questions (one per line):\n",
    "        \"\"\")\n",
    "\n",
    "    def generate_queries(self, original_query):\n",
    "        chain = self.query_generation_prompt | self.llm | StrOutputParser()\n",
    "        response = chain.invoke({'question': original_query})\n",
    "        alternatives = [q.strip() for q in response.split('\\n') if q.strip()]\n",
    "        return [original_query] + alternatives[:3]\n",
    "\n",
    "    def retrieve(self, question, k_per_query=3):\n",
    "        queries = self.generate_queries(question)\n",
    "        print(f'Generated {len(queries)} queries:')\n",
    "        for q in queries:\n",
    "            print(f'  - {q}')\n",
    "        all_docs = []\n",
    "        seen_content = set()\n",
    "        for query in queries:\n",
    "            docs = self.vectorstore.similarity_search(query, k=k_per_query)\n",
    "            for doc in docs:\n",
    "                content_hash = hash(doc.page_content)\n",
    "                if content_hash not in seen_content:\n",
    "                    seen_content.add(content_hash)\n",
    "                    all_docs.append(doc)\n",
    "        print(f'Retrieved {len(all_docs)} unique documents')\n",
    "        return all_docs\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model='claude-sonnet-4-20250514',\n",
    "    api_key='your-api-key'\n",
    ")\n",
    "retriever = MultiQueryRetriever(vectorstore, llm)\n",
    "docs = retriever.retrieve('How do I peer with AWS?', k_per_query=2)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Implement Hybrid Search\n",
    "Combine keyword and semantic search for comprehensive results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "class HybridNetworkRetriever:\n",
    "    \"\"\"Combine semantic and keyword search.\"\"\"\n",
    "\n",
    "    def __init__(self, vectorstore, documents):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.documents = documents\n",
    "\n",
    "        self.semantic_retriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n",
    "        self.keyword_retriever = BM25Retriever.from_documents(documents)\n",
    "        self.keyword_retriever.k = 5\n",
    "\n",
    "        self.ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[self.semantic_retriever, self.keyword_retriever],\n",
    "            weights=[0.6, 0.4]\n",
    "        )\n",
    "\n",
    "    def search(self, query, k=5):\n",
    "        results = self.ensemble_retriever.get_relevant_documents(query)\n",
    "        return results[:k]\n",
    "\n",
    "document_list = []  # Your documents would be loaded here\n",
    "hybrid_retriever = HybridNetworkRetriever(vectorstore, document_list)\n",
    "hybrid_results = hybrid_retriever.search('BGP peering techniques')\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}