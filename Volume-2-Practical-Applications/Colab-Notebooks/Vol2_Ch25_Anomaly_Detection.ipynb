{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Volume 2, Chapter 25: Anomaly Detection with AI\n",
        "\n",
        "**Build production-ready anomaly detection for network operations**\n",
        "\n",
        "From: AI for Networking Engineers - Volume 2, Chapter 25\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eduardd76/AI_for_networking_and_security_engineers/blob/main/CODE/Colab-Notebooks/Vol2_Ch25_Anomaly_Detection.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. **Statistical Anomaly Detection** - Z-score, IQR methods for quick anomaly checks\n",
        "2. **ML-Based Detection** - Isolation Forest for unsupervised anomaly detection\n",
        "3. **Time-Series Detection** - Prophet for seasonality-aware anomaly detection\n",
        "4. **LLM Explanation** - Use Claude to explain what anomalies mean\n",
        "\n",
        "**Real Impact**: Reduce false positive alerts by 90%, detect attacks in seconds instead of days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q anthropic langchain-anthropic scikit-learn prophet pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "api_key"
      },
      "outputs": [],
      "source": [
        "# Set API key\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get API key from Colab secrets\n",
        "os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example1"
      },
      "source": [
        "## Example 1: Statistical Anomaly Detection (Z-Score)\n",
        "\n",
        "Fast, simple method for detecting outliers in bandwidth/latency data.\n",
        "\n",
        "**When to use**: Quick checks, real-time monitoring, simple patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zscore_detector"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Anomaly:\n",
        "    \"\"\"Detected anomaly.\"\"\"\n",
        "    timestamp: datetime\n",
        "    metric: str\n",
        "    value: float\n",
        "    expected_range: Tuple[float, float]\n",
        "    severity: str\n",
        "    method: str\n",
        "\n",
        "    def __str__(self):\n",
        "        return (f\"Anomaly: {self.metric}={self.value:.2f} \"\n",
        "                f\"(expected {self.expected_range[0]:.2f}-{self.expected_range[1]:.2f}) \"\n",
        "                f\"at {self.timestamp} [{self.severity}]\")\n",
        "\n",
        "class StatisticalAnomalyDetector:\n",
        "    \"\"\"Statistical anomaly detection using Z-score and IQR methods.\"\"\"\n",
        "\n",
        "    def __init__(self, z_threshold: float = 3.0, iqr_multiplier: float = 1.5):\n",
        "        self.z_threshold = z_threshold\n",
        "        self.iqr_multiplier = iqr_multiplier\n",
        "\n",
        "    def detect_zscore(self, data: np.ndarray, values: np.ndarray) -> List[Anomaly]:\n",
        "        \"\"\"\n",
        "        Detect anomalies using Z-score method.\n",
        "        Z-score = (value - mean) / std_dev\n",
        "        Anomaly if |Z-score| > threshold\n",
        "        \"\"\"\n",
        "        mean = np.mean(data)\n",
        "        std = np.std(data)\n",
        "\n",
        "        if std == 0:\n",
        "            return []  # No variation in data\n",
        "\n",
        "        anomalies = []\n",
        "        for idx, value in enumerate(values):\n",
        "            z_score = abs((value - mean) / std)\n",
        "\n",
        "            if z_score > self.z_threshold:\n",
        "                lower = mean - (self.z_threshold * std)\n",
        "                upper = mean + (self.z_threshold * std)\n",
        "\n",
        "                if z_score > self.z_threshold * 2:\n",
        "                    severity = \"critical\"\n",
        "                elif z_score > self.z_threshold * 1.5:\n",
        "                    severity = \"high\"\n",
        "                else:\n",
        "                    severity = \"medium\"\n",
        "\n",
        "                anomalies.append(Anomaly(\n",
        "                    timestamp=datetime.now(),\n",
        "                    metric=\"bandwidth\",\n",
        "                    value=value,\n",
        "                    expected_range=(max(0, lower), upper),\n",
        "                    severity=severity,\n",
        "                    method=\"z-score\"\n",
        "                ))\n",
        "\n",
        "        return anomalies\n",
        "\n",
        "# Test with synthetic network data\n",
        "np.random.seed(42)\n",
        "\n",
        "# Normal traffic: 100 Mbps Â± 20 Mbps\n",
        "normal_traffic = np.random.normal(100, 20, 1000)\n",
        "\n",
        "# Test data with anomalies\n",
        "test_data = np.array([\n",
        "    95, 105, 98, 102, 110,  # Normal\n",
        "    250,  # Anomaly: spike (DDoS?)\n",
        "    105, 98, 95, 102,  # Normal\n",
        "    15,   # Anomaly: drop (link down?)\n",
        "    100, 105, 98  # Normal\n",
        "])\n",
        "\n",
        "detector = StatisticalAnomalyDetector(z_threshold=3.0)\n",
        "\n",
        "print(\"=== Z-Score Detection ===\")\n",
        "anomalies = detector.detect_zscore(normal_traffic, test_data)\n",
        "for anomaly in anomalies:\n",
        "    print(anomaly)\n",
        "\n",
        "print(f\"\\nDetected {len(anomalies)} anomalies out of {len(test_data)} data points\")\n",
        "print(f\"Anomaly rate: {len(anomalies)/len(test_data)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example2"
      },
      "source": [
        "## Example 2: ML-Based Anomaly Detection (Isolation Forest)\n",
        "\n",
        "Unsupervised ML that detects complex patterns without labeled data.\n",
        "\n",
        "**When to use**: Multi-dimensional data (bandwidth + latency + CPU + memory), complex patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isolation_forest"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "@dataclass\n",
        "class MLAnomaly:\n",
        "    \"\"\"ML-detected anomaly with context.\"\"\"\n",
        "    timestamp: datetime\n",
        "    features: Dict[str, float]\n",
        "    anomaly_score: float\n",
        "    severity: str\n",
        "    method: str\n",
        "\n",
        "class MLAnomalyDetector:\n",
        "    \"\"\"ML-based anomaly detection using Isolation Forest.\"\"\"\n",
        "\n",
        "    def __init__(self, contamination: float = 0.1):\n",
        "        self.contamination = contamination\n",
        "        self.scaler = StandardScaler()\n",
        "        self.isolation_forest = None\n",
        "        self.feature_names = None\n",
        "\n",
        "    def train_isolation_forest(self, X: np.ndarray, feature_names: List[str] = None):\n",
        "        \"\"\"Train Isolation Forest model.\"\"\"\n",
        "        self.feature_names = feature_names or [f\"feature_{i}\" for i in range(X.shape[1])]\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        self.isolation_forest = IsolationForest(\n",
        "            contamination=self.contamination,\n",
        "            random_state=42,\n",
        "            n_estimators=100\n",
        "        )\n",
        "        self.isolation_forest.fit(X_scaled)\n",
        "\n",
        "    def detect_isolation_forest(self, X: np.ndarray, timestamps: List[datetime] = None) -> List[MLAnomaly]:\n",
        "        \"\"\"Detect anomalies using trained Isolation Forest.\"\"\"\n",
        "        if self.isolation_forest is None:\n",
        "            raise ValueError(\"Model not trained. Call train_isolation_forest first.\")\n",
        "\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "        predictions = self.isolation_forest.predict(X_scaled)\n",
        "        scores = self.isolation_forest.score_samples(X_scaled)\n",
        "\n",
        "        anomalies = []\n",
        "        for idx, (pred, score) in enumerate(zip(predictions, scores)):\n",
        "            if pred == -1:  # Anomaly detected\n",
        "                if score < -0.5:\n",
        "                    severity = \"critical\"\n",
        "                elif score < -0.3:\n",
        "                    severity = \"high\"\n",
        "                else:\n",
        "                    severity = \"medium\"\n",
        "\n",
        "                features = {\n",
        "                    name: float(X[idx, i])\n",
        "                    for i, name in enumerate(self.feature_names)\n",
        "                }\n",
        "\n",
        "                timestamp = timestamps[idx] if timestamps else datetime.now()\n",
        "\n",
        "                anomalies.append(MLAnomaly(\n",
        "                    timestamp=timestamp,\n",
        "                    features=features,\n",
        "                    anomaly_score=float(score),\n",
        "                    severity=severity,\n",
        "                    method=\"isolation-forest\"\n",
        "                ))\n",
        "\n",
        "        return anomalies\n",
        "\n",
        "# Simulate network metrics (multi-dimensional)\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Normal behavior\n",
        "normal_data = np.column_stack([\n",
        "    np.random.normal(100, 15, n_samples),      # Bandwidth (Mbps)\n",
        "    np.random.normal(5, 1, n_samples),         # Latency (ms)\n",
        "    np.random.normal(0.01, 0.005, n_samples),  # Packet loss (%)\n",
        "    np.random.normal(50, 10, n_samples),       # CPU (%)\n",
        "    np.random.normal(60, 5, n_samples)         # Memory (%)\n",
        "])\n",
        "\n",
        "# Test data with anomalies\n",
        "test_data = np.array([\n",
        "    [105, 5.2, 0.012, 52, 61],  # Normal\n",
        "    [98, 4.8, 0.008, 48, 59],   # Normal\n",
        "    [350, 45, 8.5, 95, 92],     # Anomaly: DDoS attack\n",
        "    [102, 5.1, 0.011, 51, 60],  # Normal\n",
        "    [15, 2.1, 0.005, 10, 25],   # Anomaly: Device reboot\n",
        "    [100, 5.0, 0.010, 50, 61],  # Normal\n",
        "])\n",
        "\n",
        "feature_names = ['bandwidth_mbps', 'latency_ms', 'packet_loss_pct', 'cpu_pct', 'memory_pct']\n",
        "\n",
        "# Train and detect\n",
        "detector = MLAnomalyDetector(contamination=0.1)\n",
        "detector.train_isolation_forest(normal_data, feature_names)\n",
        "\n",
        "print(\"=== Isolation Forest Detection ===\")\n",
        "anomalies_if = detector.detect_isolation_forest(test_data)\n",
        "\n",
        "for anomaly in anomalies_if:\n",
        "    print(f\"\\n{anomaly.severity.upper()} Anomaly (score: {anomaly.anomaly_score:.3f})\")\n",
        "    print(f\"Features:\")\n",
        "    for k, v in anomaly.features.items():\n",
        "        print(f\"  {k}: {v:.2f}\")\n",
        "\n",
        "print(f\"\\nDetected {len(anomalies_if)} anomalies in {len(test_data)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example3"
      },
      "source": [
        "## Example 3: Time-Series Anomaly Detection (Prophet)\n",
        "\n",
        "Handles daily/weekly/seasonal patterns automatically.\n",
        "\n",
        "**When to use**: Time-series data with patterns (traffic spikes during business hours, low on weekends)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prophet_detector"
      },
      "outputs": [],
      "source": [
        "from prophet import Prophet\n",
        "\n",
        "@dataclass\n",
        "class TimeSeriesAnomaly:\n",
        "    \"\"\"Time-series anomaly with forecast context.\"\"\"\n",
        "    timestamp: datetime\n",
        "    actual_value: float\n",
        "    predicted_value: float\n",
        "    lower_bound: float\n",
        "    upper_bound: float\n",
        "    deviation_pct: float\n",
        "    severity: str\n",
        "\n",
        "class ProphetAnomalyDetector:\n",
        "    \"\"\"Time-series anomaly detection using Facebook Prophet.\"\"\"\n",
        "\n",
        "    def __init__(self, interval_width: float = 0.95):\n",
        "        self.interval_width = interval_width\n",
        "        self.model = Prophet(\n",
        "            interval_width=interval_width,\n",
        "            daily_seasonality=True,\n",
        "            weekly_seasonality=True,\n",
        "            yearly_seasonality=False  # Not enough data for yearly\n",
        "        )\n",
        "        self.trained = False\n",
        "\n",
        "    def train(self, timestamps: List[datetime], values: List[float]):\n",
        "        \"\"\"Train Prophet model on historical data.\"\"\"\n",
        "        df = pd.DataFrame({'ds': timestamps, 'y': values})\n",
        "        self.model.fit(df)\n",
        "        self.trained = True\n",
        "\n",
        "    def detect_anomalies(self, timestamps: List[datetime], values: List[float]) -> List[TimeSeriesAnomaly]:\n",
        "        \"\"\"Detect anomalies in time-series data.\"\"\"\n",
        "        if not self.trained:\n",
        "            raise ValueError(\"Model not trained. Call train() first.\")\n",
        "\n",
        "        df = pd.DataFrame({'ds': timestamps})\n",
        "        forecast = self.model.predict(df)\n",
        "\n",
        "        anomalies = []\n",
        "        for idx, (ts, actual) in enumerate(zip(timestamps, values)):\n",
        "            predicted = forecast.iloc[idx]['yhat']\n",
        "            lower = forecast.iloc[idx]['yhat_lower']\n",
        "            upper = forecast.iloc[idx]['yhat_upper']\n",
        "\n",
        "            # Check if outside prediction interval\n",
        "            if actual < lower or actual > upper:\n",
        "                if actual > upper:\n",
        "                    deviation_pct = ((actual - upper) / upper) * 100\n",
        "                else:\n",
        "                    deviation_pct = ((lower - actual) / lower) * 100\n",
        "\n",
        "                if deviation_pct > 50:\n",
        "                    severity = \"critical\"\n",
        "                elif deviation_pct > 25:\n",
        "                    severity = \"high\"\n",
        "                else:\n",
        "                    severity = \"medium\"\n",
        "\n",
        "                anomalies.append(TimeSeriesAnomaly(\n",
        "                    timestamp=ts,\n",
        "                    actual_value=actual,\n",
        "                    predicted_value=predicted,\n",
        "                    lower_bound=lower,\n",
        "                    upper_bound=upper,\n",
        "                    deviation_pct=deviation_pct,\n",
        "                    severity=severity\n",
        "                ))\n",
        "\n",
        "        return anomalies\n",
        "\n",
        "# Generate synthetic bandwidth data with patterns\n",
        "np.random.seed(42)\n",
        "\n",
        "# 30 days of hourly data\n",
        "hours = 24 * 30\n",
        "start_time = datetime(2024, 1, 1, 0, 0, 0)\n",
        "timestamps = [start_time + timedelta(hours=i) for i in range(hours)]\n",
        "\n",
        "# Generate realistic traffic pattern\n",
        "values = []\n",
        "for ts in timestamps:\n",
        "    hour = ts.hour\n",
        "    day_of_week = ts.weekday()\n",
        "\n",
        "    base = 100\n",
        "\n",
        "    # Daily pattern (business hours)\n",
        "    if 8 <= hour <= 18:\n",
        "        daily_boost = 50\n",
        "    else:\n",
        "        daily_boost = 0\n",
        "\n",
        "    # Weekly pattern (weekends lower)\n",
        "    if day_of_week >= 5:\n",
        "        weekly_factor = 0.6\n",
        "    else:\n",
        "        weekly_factor = 1.0\n",
        "\n",
        "    noise = np.random.normal(0, 10)\n",
        "    value = (base + daily_boost) * weekly_factor + noise\n",
        "    values.append(max(0, value))\n",
        "\n",
        "# Add anomalies\n",
        "values[100] = 300  # Spike\n",
        "values[200] = 20   # Drop\n",
        "values[500] = 280  # Another spike\n",
        "\n",
        "# Split train/test\n",
        "train_size = int(0.8 * len(values))\n",
        "train_timestamps = timestamps[:train_size]\n",
        "train_values = values[:train_size]\n",
        "test_timestamps = timestamps[train_size:]\n",
        "test_values = values[train_size:]\n",
        "\n",
        "# Train and detect\n",
        "print(\"Training Prophet model (this may take a minute)...\")\n",
        "detector = ProphetAnomalyDetector(interval_width=0.95)\n",
        "detector.train(train_timestamps, train_values)\n",
        "\n",
        "print(\"\\n=== Prophet Anomaly Detection ===\")\n",
        "anomalies = detector.detect_anomalies(test_timestamps, test_values)\n",
        "\n",
        "print(f\"Found {len(anomalies)} anomalies in {len(test_timestamps)} data points\")\n",
        "print(f\"Anomaly rate: {len(anomalies)/len(test_timestamps)*100:.1f}%\\n\")\n",
        "\n",
        "for anomaly in anomalies[:5]:  # Show first 5\n",
        "    print(f\"{anomaly.severity.upper()} at {anomaly.timestamp}\")\n",
        "    print(f\"  Actual: {anomaly.actual_value:.1f} Mbps\")\n",
        "    print(f\"  Expected: {anomaly.predicted_value:.1f} Mbps ({anomaly.lower_bound:.1f} - {anomaly.upper_bound:.1f})\")\n",
        "    print(f\"  Deviation: {anomaly.deviation_pct:.1f}%\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example4"
      },
      "source": [
        "## Example 4: LLM-Powered Anomaly Explanation\n",
        "\n",
        "Combine ML detection with Claude explanation for actionable insights.\n",
        "\n",
        "**The Power Move**: ML detects fast, LLM explains why it matters and what to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llm_explainer"
      },
      "outputs": [],
      "source": [
        "from anthropic import Anthropic\n",
        "\n",
        "class AnomalyExplainer:\n",
        "    \"\"\"Use LLM to explain detected anomalies.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.client = Anthropic(api_key=api_key)\n",
        "\n",
        "    def explain_anomaly(self, features: Dict[str, float], baseline: Dict[str, float], context: str = \"\") -> Dict:\n",
        "        \"\"\"\n",
        "        Generate explanation for an anomaly.\n",
        "        \"\"\"\n",
        "        current_str = \"\\n\".join([f\"- {k}: {v:.2f}\" for k, v in features.items()])\n",
        "        baseline_str = \"\\n\".join([f\"- {k}: {v:.2f}\" for k, v in baseline.items()])\n",
        "\n",
        "        prompt = f\"\"\"You are a network operations expert. Analyze this anomaly and provide actionable insights.\n",
        "\n",
        "Current Values (ANOMALOUS):\n",
        "{current_str}\n",
        "\n",
        "Expected Normal Values:\n",
        "{baseline_str}\n",
        "\n",
        "Additional Context:\n",
        "{context or 'No additional context provided'}\n",
        "\n",
        "Provide:\n",
        "1. EXPLANATION: What is abnormal and why it matters\n",
        "2. ROOT CAUSE: Most likely cause(s) of this anomaly\n",
        "3. RECOMMENDED ACTIONS: Specific steps to investigate or remediate (numbered list)\n",
        "\n",
        "Be specific to networking. Reference actual metrics, protocols, and troubleshooting commands.\"\"\"\n",
        "\n",
        "        response = self.client.messages.create(\n",
        "            model=\"claude-3-5-sonnet-20241022\",\n",
        "            max_tokens=2000,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "\n",
        "        return response.content[0].text\n",
        "\n",
        "# Example: Explain a DDoS attack\n",
        "explainer = AnomalyExplainer(api_key=os.environ['ANTHROPIC_API_KEY'])\n",
        "\n",
        "# Anomaly detected\n",
        "anomaly_features = {\n",
        "    'bandwidth_mbps': 450.0,\n",
        "    'latency_ms': 85.0,\n",
        "    'packet_loss_pct': 12.5,\n",
        "    'cpu_pct': 98.0,\n",
        "    'memory_pct': 92.0,\n",
        "    'connections_per_sec': 15000\n",
        "}\n",
        "\n",
        "baseline_features = {\n",
        "    'bandwidth_mbps': 100.0,\n",
        "    'latency_ms': 5.0,\n",
        "    'packet_loss_pct': 0.01,\n",
        "    'cpu_pct': 45.0,\n",
        "    'memory_pct': 60.0,\n",
        "    'connections_per_sec': 500\n",
        "}\n",
        "\n",
        "context = \"Time: 03:45 AM (maintenance window). Device: core-router-01\"\n",
        "\n",
        "print(\"=== Anomaly Explanation from Claude ===\")\n",
        "explanation = explainer.explain_anomaly(anomaly_features, baseline_features, context)\n",
        "print(explanation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## Summary\n",
        "\n",
        "You now have 4 anomaly detection methods:\n",
        "\n",
        "1. **Statistical (Z-Score)** - Fast, simple, good for quick checks\n",
        "2. **ML (Isolation Forest)** - Handles multi-dimensional data, unsupervised\n",
        "3. **Time-Series (Prophet)** - Handles seasonality, best for time-series data\n",
        "4. **LLM Explanation** - Makes anomalies actionable with context\n",
        "\n",
        "**Production Strategy**:\n",
        "- Start with Statistical for real-time monitoring\n",
        "- Use Isolation Forest for complex multi-metric analysis\n",
        "- Use Prophet for capacity planning and trending\n",
        "- Always explain critical anomalies with LLM\n",
        "\n",
        "**Real Impact**: Reduce false positives by 90%, detect attacks in seconds instead of days."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
